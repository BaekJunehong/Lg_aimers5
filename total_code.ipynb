{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017e9265",
   "metadata": {},
   "source": [
    "# 제품 이상여부 판별 프로젝트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab431",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8341e8",
   "metadata": {},
   "source": [
    "### 필수 라이브러리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a315cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d054e30",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "43bcdbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedfb65e",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc013be",
   "metadata": {},
   "source": [
    "### 2-1. 데이터 결측값 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51c06102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 성공적으로 수정되고 저장되었습니다: ./data/clean_train_data.csv\n",
      "데이터가 성공적으로 수정되고 저장되었습니다: ./data/clean_test_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def shift_row_values(row, start_col_index, move_limit, total_columns):\n",
    "    move_count = 0  # 이동 카운터 초기화\n",
    "    for col_index in range(start_col_index, total_columns):  # 모든 열을 대상으로\n",
    "        if pd.isna(row[col_index]) or row[col_index] == \"OK\":  # 빈값 또는 \"OK\" 확인\n",
    "            # 빈값 또는 \"OK\"가 발견되면 현재 위치부터 이후 3칸 간격의 변수 값을 앞으로 이동\n",
    "            for shift_index in range(col_index, total_columns - 3, 3):  # 3칸씩 이동\n",
    "                # 값을 이동\n",
    "                row[shift_index] = row[shift_index + 3]\n",
    "                row[shift_index + 3] = None  # 원래 자리 비우기\n",
    "                move_count += 1  # 이동 카운트 증가\n",
    "\n",
    "                if move_count >= move_limit:  # 설정된 횟수에 도달하면 중지\n",
    "                    break\n",
    "        if move_count >= move_limit:  # 외부 루프에서도 체크\n",
    "            break\n",
    "    return row\n",
    "\n",
    "def shift_values(data, start_col_index, move_limit):\n",
    "    total_columns = data.shape[1]\n",
    "    data = data.apply(shift_row_values, axis=1, args=(start_col_index, move_limit, total_columns))\n",
    "    return data\n",
    "\n",
    "# 변수 이름 설정 및 시작 열 인덱스 및 이동 횟수 설정\n",
    "variables_with_limits = [\n",
    "    ('HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 52),\n",
    "    ('HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 22),\n",
    "    ('HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2', 22)\n",
    "]\n",
    "\n",
    "# 각 변수에 대해 함수 호출\n",
    "def process_data(data, variables_with_limits, output_file):\n",
    "    for start_var, move_limit in variables_with_limits:\n",
    "        start_col_index = data.columns.get_loc(start_var)  # 각 변수의 시작 열 인덱스 찾기\n",
    "        data = shift_values(data, start_col_index, move_limit)\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f'데이터가 성공적으로 수정되고 저장되었습니다: {output_file}')\n",
    "\n",
    "# 데이터 처리\n",
    "process_data(train_data, variables_with_limits, './data/clean_train_data.csv')\n",
    "process_data(test_data, variables_with_limits, './data/clean_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b207c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 불러오기\n",
    "train_data = pd.read_csv('./data/clean_train_data.csv')\n",
    "test_data = pd.read_csv('./data/clean_test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f08fd",
   "metadata": {},
   "source": [
    "### 2-2. 기본 전처리  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e50863ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data와 test_data에서 '?'를 포함하는 열 이름 필터링\n",
    "train_Process_Desc_col = train_data.filter(like='?').columns\n",
    "test_Process_Desc_col = test_data.filter(like='?').columns\n",
    "\n",
    "# ? -> Θ로 변경할 열 이름과 새 열 이름 생성\n",
    "train_new_columns = {col: col.replace('?', 'Θ') for col in train_Process_Desc_col}\n",
    "test_new_columns = {col: col.replace('?', 'Θ') for col in test_Process_Desc_col}\n",
    "\n",
    "# 열 이름 변경\n",
    "train_data.rename(columns=train_new_columns, inplace=True)\n",
    "test_data.rename(columns=test_new_columns, inplace=True)\n",
    "\n",
    "# 'Θ'를 포함하는 열 이름 필터링\n",
    "train_Process_Desc_col = train_data.filter(like='Θ').columns\n",
    "test_Process_Desc_col = test_data.filter(like='Θ').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f44f36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 열을 임시로 분리\n",
    "target_train = train_data['target']\n",
    "target_test = test_data['target']\n",
    "\n",
    "# 모든 값이 NaN인 열 제거\n",
    "train_data = train_data.dropna(axis=1, how='all')\n",
    "test_data = test_data.dropna(axis=1, how='all')\n",
    "\n",
    "# target 열을 다시 결합\n",
    "train_data['target'] = target_train\n",
    "test_data['target'] = target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a7a694a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wip Line 열 제거\n",
    "wip_line_columns = train_data.filter(like='Wip Line').columns\n",
    "\n",
    "train_data.drop(columns=wip_line_columns, inplace=True)\n",
    "test_data.drop(columns=wip_line_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5c56bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Desc 열 제거\n",
    "Process_Desc_col = train_data.filter(like='Process Desc').columns\n",
    "\n",
    "train_data.drop(columns=Process_Desc_col, inplace=True)\n",
    "test_data.drop(columns=Process_Desc_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a48a2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insp. Seq No 열 제거\n",
    "Insp_Seq_No_col = train_data.filter(like='Insp. Seq No').columns\n",
    "\n",
    "train_data.drop(columns=Insp_Seq_No_col, inplace=True)\n",
    "test_data.drop(columns=Insp_Seq_No_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "547dea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insp Judge Code 열 제거\n",
    "Insp_Judge_Code_col = train_data.filter(like='Insp Judge Code').columns\n",
    "\n",
    "train_data.drop(columns=Insp_Judge_Code_col, inplace=True)\n",
    "test_data.drop(columns=Insp_Judge_Code_col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d1d5a",
   "metadata": {},
   "source": [
    "### 2. 제품 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc8675",
   "metadata": {},
   "source": [
    "receip no, workorder, model.suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "30eea99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Receip_No\n",
    "# 파생변수 생성: Receip_No 3개의 컬럼 값이 모두 동일하면 해당 값을 저장, 아니면 diff\n",
    "train_data['Receip_No'] = train_data.apply(\n",
    "    lambda row: row['Receip No Collect Result_Dam'] if (row['Receip No Collect Result_Dam'] == row['Receip No Collect Result_Fill1'] == row['Receip No Collect Result_Fill2']) else 'diff',\n",
    "    axis=1\n",
    ")\n",
    "test_data['Receip_No'] = test_data.apply(\n",
    "    lambda row: row['Receip No Collect Result_Dam'] if (row['Receip No Collect Result_Dam'] == row['Receip No Collect Result_Fill1'] == row['Receip No Collect Result_Fill2']) else 'diff',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 필요없는 변수 삭제\n",
    "train_data = train_data.drop(columns=['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'])\n",
    "test_data = test_data.drop(columns=['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "56e8f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "### model_receip\n",
    "# 파생변수 생성: Receip No와 Model.Suffix의 조합\n",
    "train_data['model_receip'] = train_data['Model.Suffix_Dam'] + '_' + train_data['Receip_No'].astype(str)\n",
    "test_data['model_receip'] = test_data['Model.Suffix_Dam'] + '_' + test_data['Receip_No'].astype(str)\n",
    "\n",
    "# 필요없는 변수 삭제\n",
    "train_data = train_data.drop(columns=['Model.Suffix_Dam', 'Model.Suffix_AutoClave', 'Model.Suffix_Fill1', 'Model.Suffix_Fill2'])\n",
    "test_data = test_data.drop(columns=['Model.Suffix_Dam', 'Model.Suffix_AutoClave', 'Model.Suffix_Fill1', 'Model.Suffix_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "271ab2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### workorder_receip\n",
    "# # Workorder -뒤의 번호 구분을 제거\n",
    "# train_data['cleaned_workorder'] = train_data['Workorder_Dam'].str.split('-').str[0]\n",
    "# test_data['cleaned_workorder'] = test_data['Workorder_Dam'].str.split('-').str[0]\n",
    "\n",
    "# # 파생변수 생성: Receip No와 workorder의 조합\n",
    "# train_data['workorder_receip'] = train_data['cleaned_workorder'] + '_' + train_data['Receip_No'].astype(str)\n",
    "# test_data['workorder_receip'] = test_data['cleaned_workorder'] + '_' + test_data['Receip_No'].astype(str)\n",
    "\n",
    "# # 필요없는 변수 삭제\n",
    "# train_data = train_data.drop(columns=['Receip_No', 'cleaned_workorder', 'Workorder_Dam', 'Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2'])\n",
    "# test_data = test_data.drop(columns=['Receip_No', 'cleaned_workorder', 'Workorder_Dam', 'Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "855b308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### workorder_receip\n",
    "# Workorder 앞의 4자리만 저장\n",
    "train_data['cleaned_workorder'] = train_data['Workorder_Dam'].str[:4]\n",
    "test_data['cleaned_workorder'] = test_data['Workorder_Dam'].str[:4]\n",
    "\n",
    "# 필요없는 변수 삭제\n",
    "train_data = train_data.drop(columns=['Receip_No', 'Workorder_Dam', 'Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2'])\n",
    "test_data = test_data.drop(columns=['Receip_No', 'Workorder_Dam', 'Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a419e85",
   "metadata": {},
   "source": [
    "### 3. 공통 변수 (dam, fill1, fill2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8116722",
   "metadata": {},
   "source": [
    "- workmode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e6496012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorkMode Collect Result_Dam의 이름을 WorkMode Collect Result로 변경\n",
    "train_data = train_data.rename(columns={'WorkMode Collect Result_Dam': 'WorkMode Collect Result'})\n",
    "test_data = test_data.rename(columns={'WorkMode Collect Result_Dam': 'WorkMode Collect Result'})\n",
    "\n",
    "# WorkMode Collect Result_Fill1, WorkMode Collect Result_Fill2 열 드롭\n",
    "train_data = train_data.drop(columns=['WorkMode Collect Result_Fill1', 'WorkMode Collect Result_Fill2'])\n",
    "test_data = test_data.drop(columns=['WorkMode Collect Result_Fill1', 'WorkMode Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e1ed1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorkMode Collect Result 열의 값이 7인 행을 1로 변경\n",
    "train_data['WorkMode Collect Result'] = train_data['WorkMode Collect Result'].replace(7, 1)\n",
    "test_data['WorkMode Collect Result'] = test_data['WorkMode Collect Result'].replace(7, 1)\n",
    "\n",
    "# WorkMode Collect Result 열의 결측값을 0으로 채움\n",
    "train_data['WorkMode Collect Result'] = train_data['WorkMode Collect Result'].fillna(0)\n",
    "test_data['WorkMode Collect Result'] = test_data['WorkMode Collect Result'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49b347",
   "metadata": {},
   "source": [
    "- equipment  \n",
    "(dispenser1 & dispenser2 변수를 만들 경우 다른 변수들에 의해  \n",
    "이미 설명이 되는 변수라 상관계수가 너무 높아서 제거하게 됨.  \n",
    "따라서 equipment가 같은지만 판단하는 파생변수 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7d277e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment로 시작하는 열 필터링\n",
    "Equipment_col = train_data.filter(like='Equipment').columns\n",
    "Equipment_col2 = test_data.filter(like='Equipment').columns\n",
    "\n",
    "new_train = train_data.filter(items=Equipment_col)\n",
    "new_test = test_data.filter(items=Equipment_col2)\n",
    "\n",
    "# Equipment_same_num 파생변수 생성\n",
    "def determine_equipment_same_num(row):\n",
    "    if (row['Equipment_Dam'] == 'Dam dispenser #1' and row['Equipment_AutoClave'] == 'Auto Clave Out' and \n",
    "        row['Equipment_Fill1'] == 'Fill1 dispenser #1' and row['Equipment_Fill2'] == 'Fill2 dispenser #1') or \\\n",
    "       (row['Equipment_Dam'] == 'Dam dispenser #2' and row['Equipment_AutoClave'] == 'Auto Clave Out' and \n",
    "        row['Equipment_Fill1'] == 'Fill1 dispenser #2' and row['Equipment_Fill2'] == 'Fill2 dispenser #2'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train_data['Equipment_same_num'] = new_train.apply(determine_equipment_same_num, axis=1)\n",
    "test_data['Equipment_same_num'] = new_test.apply(determine_equipment_same_num, axis=1)\n",
    "\n",
    "train_data = train_data.drop(columns=['Equipment_Dam', 'Equipment_AutoClave', 'Equipment_Fill1', 'Equipment_Fill2'])\n",
    "test_data = test_data.drop(columns=['Equipment_Dam', 'Equipment_AutoClave', 'Equipment_Fill1', 'Equipment_Fill2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c794d2a",
   "metadata": {},
   "source": [
    "- palletID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3a35ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 변수의 값이 동일하면 해당 값을 가져가고, 하나라도 일치하지 않으면 diff의 값을 가지는 파생 변수 생성 함수\n",
    "def create_palletid_collect_result(df):\n",
    "    df['PalletID_Collect_Result'] = df.apply(\n",
    "        lambda row: row['PalletID Collect Result_Dam'] \n",
    "                    if (row['PalletID Collect Result_Dam'] == row['PalletID Collect Result_Fill1'] == row['PalletID Collect Result_Fill2']) \n",
    "                    else 'diff', \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# 함수 적용\n",
    "create_palletid_collect_result(train_data)\n",
    "create_palletid_collect_result(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b2743648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'PalletID Collect Result_Dam',\n",
    "    'PalletID Collect Result_Fill1',\n",
    "    'PalletID Collect Result_Fill2'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eaddc2",
   "metadata": {},
   "source": [
    "- production Qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "41b89ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 변수의 값이 동일하면 해당 값을 가져가고, 하나라도 일치하지 않으면 0의 값을 가지는 파생 변수 생성 함수\n",
    "def create_palletid_collect_result(df):\n",
    "    df['Production_Qty_Collect_Result'] = df.apply(\n",
    "        lambda row: row['Production Qty Collect Result_Dam'] \n",
    "                    if (row['Production Qty Collect Result_Dam'] == row['Production Qty Collect Result_Fill1'] == row['Production Qty Collect Result_Fill2']) \n",
    "                    else 0, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# 함수 적용\n",
    "create_palletid_collect_result(train_data)\n",
    "create_palletid_collect_result(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "57f73fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'Production Qty Collect Result_Dam',\n",
    "    'Production Qty Collect Result_Fill1',\n",
    "    'Production Qty Collect Result_Fill2'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181a327",
   "metadata": {},
   "source": [
    "### 4. CURE 변수\n",
    "- dam -> distance 파생변수 (standby는 단일값, start와 end는 값은 여러개지만 distance 파생변수를 만들었을 때 더 의미있었음)\n",
    "- fill2 -> 변수값 범주화 (start, end, standby를 각각 범주화했을 때가 합쳐서 distance 만들었을 때보다 더 의미있었음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "da559ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dam\n",
    "# 시작 위치와 끝 위치 열 이름\n",
    "start_x_col = 'CURE START POSITION X Collect Result_Dam'\n",
    "start_z_col = 33.5\n",
    "end_x_col = 'CURE END POSITION X Collect Result_Dam'\n",
    "end_z_col = 'CURE END POSITION Z Collect Result_Dam'\n",
    "\n",
    "# 시작 위치와 끝 위치 사이의 거리 계산\n",
    "train_data['CURE_DISTANCE_Dam'] = np.sqrt(\n",
    "    (train_data[end_x_col] - train_data[start_x_col]) ** 2 +\n",
    "    (train_data[end_z_col] - start_z_col) ** 2\n",
    ")\n",
    "\n",
    "test_data['CURE_DISTANCE_Dam'] = np.sqrt(\n",
    "    (train_data[end_x_col] - train_data[start_x_col]) ** 2 +\n",
    "    (train_data[end_z_col] - start_z_col) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1077a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fill2\n",
    "# UV 경화 좌표 합치기\n",
    "def create_coordinate_columns(data):\n",
    "    # Fill2\n",
    "    # cure end\n",
    "    data['cure_end_position_XZ_Fill2'] = (\n",
    "        data['CURE END POSITION X Collect Result_Fill2'].astype(str) + ',' +\n",
    "        data['CURE END POSITION Z Collect Result_Fill2'].astype(str) \n",
    "    )\n",
    "\n",
    "    # cure start\n",
    "    data['cure_start_position_XZ_Fill2'] = (\n",
    "        data['CURE START POSITION X Collect Result_Fill2'].astype(str) + ',' +\n",
    "        data['CURE START POSITION Z Collect Result_Fill2'].astype(str) \n",
    "    )\n",
    "\n",
    "    # cure standby\n",
    "    data['cure_standby_position_XZ_Fill2'] = (\n",
    "        data['CURE STANDBY POSITION X Collect Result_Fill2'].astype(str) + ',' +\n",
    "        data['CURE STANDBY POSITION Z Collect Result_Fill2'].astype(str) \n",
    "    )\n",
    "\n",
    "# train_data와 test_data에 대해 함수 호출\n",
    "create_coordinate_columns(train_data)\n",
    "create_coordinate_columns(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8d64e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'CURE END POSITION X Collect Result_Dam',\n",
    "    'CURE END POSITION Z Collect Result_Dam',\n",
    "    'CURE END POSITION Θ Collect Result_Dam',\n",
    "    'CURE START POSITION X Collect Result_Dam',\n",
    "    'CURE START POSITION Z Collect Result_Dam',\n",
    "    'CURE START POSITION Θ Collect Result_Dam',\n",
    "\n",
    "    'CURE END POSITION X Collect Result_Fill2',\n",
    "    'CURE END POSITION Z Collect Result_Fill2',\n",
    "    'CURE END POSITION Θ Collect Result_Fill2',\n",
    "    'CURE START POSITION X Collect Result_Fill2',\n",
    "    'CURE START POSITION Z Collect Result_Fill2',\n",
    "    'CURE START POSITION Θ Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION X Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION Θ Collect Result_Fill2'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04369a1d",
   "metadata": {},
   "source": [
    "### 5. HEAD 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc627b9d",
   "metadata": {},
   "source": [
    "- dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "10cd88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 스테이지의 좌표 열 정의\n",
    "stage1_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam']\n",
    "\n",
    "stage2_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam']\n",
    "\n",
    "stage3_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "# 거리 계산 함수\n",
    "def calculate_distances(data):\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE2_Dam'] = np.sqrt(\n",
    "        (data[stage2_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage2_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage2_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE2_STAGE3_Dam'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage2_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage2_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage2_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_distances(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_distances(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "890b25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 열 이름\n",
    "stage1_stage2_col = 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Dam'\n",
    "stage2_stage3_col = 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Dam'\n",
    "stage1_stage3_col = 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam'\n",
    "\n",
    "# 삼각형의 넓이와 높이를 계산하는 함수\n",
    "def calculate_triangle_features(data):\n",
    "    a = data[stage1_stage2_col]\n",
    "    b = data[stage2_stage3_col]\n",
    "    c = data[stage1_stage3_col]\n",
    "\n",
    "    # 헤론의 공식에 따른 삼각형의 넓이 계산\n",
    "    s = (a + b + c) / 2\n",
    "    area = np.sqrt(s * (s - a) * (s - b) * (s - c))\n",
    "\n",
    "    # 높이 계산 (밑변을 c로 가정)\n",
    "    height = (2 * area) / c\n",
    "\n",
    "    # 결과를 새로운 열에 저장\n",
    "    data['HEAD NORMAL DISTANCE_TRIANGLE_area_Dam'] = area\n",
    "    data['HEAD NORMAL DISTANCE_TRIANGLE_height_Dam'] = height\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_triangle_features(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_triangle_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f5720282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'\n",
    "\n",
    "    , 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Dam'\n",
    "    , 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Dam'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "96c5073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dam 노즐 zero 위치 Z좌표 드롭\n",
    "train_data.drop(columns='Head Zero Position Z Collect Result_Dam', inplace=True)\n",
    "test_data.drop(columns='Head Zero Position Z Collect Result_Dam', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6caeb3",
   "metadata": {},
   "source": [
    "- fill1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "77b02357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 스테이지의 좌표 열 정의\n",
    "stage1_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1']\n",
    "\n",
    "stage2_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1']\n",
    "\n",
    "stage3_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "# 거리 계산 함수\n",
    "def calculate_distances(data):\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill1'] = np.sqrt(\n",
    "        (data[stage2_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage2_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage2_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill1'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage2_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage2_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage2_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_distances(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_distances(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ebede468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 열 이름\n",
    "stage1_stage2_col = 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill1'\n",
    "stage2_stage3_col = 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill1'\n",
    "stage1_stage3_col = 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1'\n",
    "\n",
    "# 삼각형의 넓이와 높이를 계산하는 함수\n",
    "def calculate_triangle_features(data):\n",
    "    a = data[stage1_stage2_col]\n",
    "    b = data[stage2_stage3_col]\n",
    "    c = data[stage1_stage3_col]\n",
    "\n",
    "    # 헤론의 공식에 따른 삼각형의 넓이 계산\n",
    "    s = (a + b + c) / 2\n",
    "    area = np.sqrt(s * (s - a) * (s - b) * (s - c))\n",
    "\n",
    "    # 높이 계산 (밑변을 c로 가정)\n",
    "    height = (2 * area) / c\n",
    "\n",
    "    # 결과를 새로운 열에 저장\n",
    "    data['HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1'] = area\n",
    "    data['HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1'] = height\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_triangle_features(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_triangle_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dd586d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'\n",
    "\n",
    "    , 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill1'\n",
    "    , 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill1'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec92df",
   "metadata": {},
   "source": [
    "- fill2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a594c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 스테이지의 좌표 열 정의\n",
    "stage1_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2']\n",
    "\n",
    "stage2_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2']\n",
    "\n",
    "stage3_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2']\n",
    "\n",
    "# 거리 계산 함수\n",
    "def calculate_distances(data):\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2'] = np.sqrt(\n",
    "        (data[stage2_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage2_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage2_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage2_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage2_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage2_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_distances(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_distances(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "372882b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b4b59",
   "metadata": {},
   "source": [
    "### 6. Resin 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e60e4",
   "metadata": {},
   "source": [
    "- dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "513346ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume*time 파생변수 - Dam\n",
    "train_data['volume_time_multip_stage1_Dam'] = train_data['Dispense Volume(Stage1) Collect Result_Dam'] * train_data['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "train_data['volume_time_multip_stage2_Dam'] = train_data['Dispense Volume(Stage2) Collect Result_Dam'] * train_data['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "train_data['volume_time_multip_stage3_Dam'] = train_data['Dispense Volume(Stage3) Collect Result_Dam'] * train_data['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "train_data['volume_time_multip_avg_Dam'] = (train_data['volume_time_multip_stage1_Dam'] + \n",
    "                                            train_data['volume_time_multip_stage2_Dam'] + \n",
    "                                            train_data['volume_time_multip_stage3_Dam']) / 3\n",
    "\n",
    "# volume*time 파생변수 - Dam\n",
    "test_data['volume_time_multip_stage1_Dam'] = test_data['Dispense Volume(Stage1) Collect Result_Dam'] * test_data['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "test_data['volume_time_multip_stage2_Dam'] = test_data['Dispense Volume(Stage2) Collect Result_Dam'] * test_data['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "test_data['volume_time_multip_stage3_Dam'] = test_data['Dispense Volume(Stage3) Collect Result_Dam'] * test_data['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "test_data['volume_time_multip_avg_Dam'] = (test_data['volume_time_multip_stage1_Dam'] + \n",
    "                                            test_data['volume_time_multip_stage2_Dam'] + \n",
    "                                            test_data['volume_time_multip_stage3_Dam']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9728ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 열 목록 추가\n",
    "columns_to_drop = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage1) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage2) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage3) Collect Result_Dam',\n",
    "    'volume_time_multip_stage1_Dam',\n",
    "    'volume_time_multip_stage2_Dam',\n",
    "    'volume_time_multip_stage3_Dam'\n",
    "]\n",
    "\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad147b9",
   "metadata": {},
   "source": [
    "- fill1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a79650ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume*time 파생변수 - Fill1\n",
    "train_data['volume_time_multip_stage1_Fill1'] = train_data['Dispense Volume(Stage1) Collect Result_Fill1'] * train_data['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "train_data['volume_time_multip_stage2_Fill1'] = train_data['Dispense Volume(Stage2) Collect Result_Fill1'] * train_data['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "train_data['volume_time_multip_stage3_Fill1'] = train_data['Dispense Volume(Stage3) Collect Result_Fill1'] * train_data['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "\n",
    "train_data['volume_time_multip_avg_Fill1'] = (train_data['volume_time_multip_stage1_Fill1'] + \n",
    "                                            train_data['volume_time_multip_stage2_Fill1'] + \n",
    "                                            train_data['volume_time_multip_stage3_Fill1']) / 3\n",
    "\n",
    "# volume*time 파생변수 - Fill1\n",
    "test_data['volume_time_multip_stage1_Fill1'] = test_data['Dispense Volume(Stage1) Collect Result_Fill1'] * test_data['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "test_data['volume_time_multip_stage2_Fill1'] = test_data['Dispense Volume(Stage2) Collect Result_Fill1'] * test_data['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "test_data['volume_time_multip_stage3_Fill1'] = test_data['Dispense Volume(Stage3) Collect Result_Fill1'] * test_data['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "\n",
    "test_data['volume_time_multip_avg_Fill1'] = (test_data['volume_time_multip_stage1_Fill1'] + \n",
    "                                            test_data['volume_time_multip_stage2_Fill1'] + \n",
    "                                            test_data['volume_time_multip_stage3_Fill1']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "867afd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 열 목록 추가\n",
    "columns_to_drop = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage2) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage3) Collect Result_Fill1',\n",
    "    'volume_time_multip_stage1_Fill1',\n",
    "    'volume_time_multip_stage2_Fill1',\n",
    "    'volume_time_multip_stage3_Fill1'\n",
    "]\n",
    "\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238cdb3",
   "metadata": {},
   "source": [
    "### 7. Circle, Line 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a087c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "### circle\n",
    "# 열 이름 변경\n",
    "train_data.rename(columns={\n",
    "    'Stage1 Circle1 Distance Speed Collect Result_Dam': 'Stage1_Circle_Distance_Speed_Dam',\n",
    "    'Stage2 Circle1 Distance Speed Collect Result_Dam': 'Stage2_Circle_Distance_Speed_Dam',\n",
    "    'Stage3 Circle1 Distance Speed Collect Result_Dam': 'Stage3_Circle_Distance_Speed_Dam'\n",
    "}, inplace=True)\n",
    "\n",
    "test_data.rename(columns={\n",
    "    'Stage1 Circle1 Distance Speed Collect Result_Dam': 'Stage1_Circle_Distance_Speed_Dam',\n",
    "    'Stage2 Circle1 Distance Speed Collect Result_Dam': 'Stage2_Circle_Distance_Speed_Dam',\n",
    "    'Stage3 Circle1 Distance Speed Collect Result_Dam': 'Stage3_Circle_Distance_Speed_Dam'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5a7b8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle4 Distance Speed Collect Result_Dam'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1615d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "### line\n",
    "# line1&3과 line2&4를 합친 파생변수 생성 함수\n",
    "def check_distance_speed(data, stage):\n",
    "    # 단계에 따라 라인 번호 정의\n",
    "    line_pairs = [(1, 3), (2, 4)]\n",
    "    \n",
    "    # 각 라인 쌍에 대해 반복\n",
    "    for line1, line2 in line_pairs:\n",
    "        line1_name = f'Stage{stage} Line{line1} Distance Speed Collect Result_Dam'\n",
    "        line2_name = f'Stage{stage} Line{line2} Distance Speed Collect Result_Dam'\n",
    "        \n",
    "        # 새로운 열 이름 설정\n",
    "        new_col_name = f'stage{stage}_line{line1}{line2}_distance_speed_Dam'\n",
    "        \n",
    "        # 조건에 따라 값 설정\n",
    "        data[new_col_name] = data.apply(\n",
    "            lambda row: row[line1_name] if row[line1_name] == row[line2_name] else 'diff', axis=1\n",
    "        )\n",
    "\n",
    "# train_data와 test_data 모두에 대해 함수 호출\n",
    "for stage in range(1, 4):\n",
    "    check_distance_speed(train_data, stage)\n",
    "    check_distance_speed(test_data, stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "08797342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data에서 변수들을 object 타입으로 변환\n",
    "train_data['stage1_line24_distance_speed_Dam'] = train_data['stage1_line24_distance_speed_Dam'].astype(object)\n",
    "train_data['stage2_line24_distance_speed_Dam'] = train_data['stage2_line24_distance_speed_Dam'].astype(object)\n",
    "train_data['stage3_line24_distance_speed_Dam'] = train_data['stage3_line24_distance_speed_Dam'].astype(object)\n",
    "\n",
    "# test_data에서 변수들을 object 타입으로 변환\n",
    "test_data['stage1_line24_distance_speed_Dam'] = test_data['stage1_line24_distance_speed_Dam'].astype(object)\n",
    "test_data['stage2_line24_distance_speed_Dam'] = test_data['stage2_line24_distance_speed_Dam'].astype(object)\n",
    "test_data['stage3_line24_distance_speed_Dam'] = test_data['stage3_line24_distance_speed_Dam'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e81f2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    'Stage2 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    'Stage3 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line4 Distance Speed Collect Result_Dam'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11294f3d",
   "metadata": {},
   "source": [
    "### 8. Thickness 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "18effadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 개 컬럼의 평균을 계산하여 새로운 컬럼 생성\n",
    "train_data['average_thickness_Dam'] = train_data[['THICKNESS 1 Collect Result_Dam', \n",
    "                                                  'THICKNESS 2 Collect Result_Dam', \n",
    "                                                  'THICKNESS 3 Collect Result_Dam']].mean(axis=1)\n",
    "\n",
    "test_data['average_thickness_Dam'] = test_data[['THICKNESS 1 Collect Result_Dam', \n",
    "                                                'THICKNESS 2 Collect Result_Dam', \n",
    "                                                'THICKNESS 3 Collect Result_Dam']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b28d5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 컬럼 리스트\n",
    "columns_to_drop = [\n",
    "    'THICKNESS 1 Collect Result_Dam',\n",
    "    'THICKNESS 2 Collect Result_Dam',\n",
    "    'THICKNESS 3 Collect Result_Dam'\n",
    "]\n",
    "\n",
    "# 지정한 컬럼 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705aa131",
   "metadata": {},
   "source": [
    "### 9. Autoclave 관련 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a05b046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 압력과 시간의 곱을 담은 새로운 컬럼 생성\n",
    "train_data['1st_pressure_time_AutoClave'] = train_data['1st Pressure Collect Result_AutoClave'] * train_data['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "train_data['2nd_pressure_time_AutoClave'] = train_data['2nd Pressure Collect Result_AutoClave'] * train_data['2nd Pressure Unit Time_AutoClave']\n",
    "train_data['3rd_pressure_time_AutoClave'] = train_data['3rd Pressure Collect Result_AutoClave'] * train_data['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "train_data['avg_pressure_time_AutoClave'] = (train_data['1st_pressure_time_AutoClave'] +\n",
    "                                             train_data['2nd_pressure_time_AutoClave'] +\n",
    "                                             train_data['3rd_pressure_time_AutoClave']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ce0eff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 압력과 시간의 곱을 담은 새로운 컬럼 생성\n",
    "test_data['1st_pressure_time_AutoClave'] = test_data['1st Pressure Collect Result_AutoClave'] * test_data['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "test_data['2nd_pressure_time_AutoClave'] = test_data['2nd Pressure Collect Result_AutoClave'] * test_data['2nd Pressure Unit Time_AutoClave']\n",
    "test_data['3rd_pressure_time_AutoClave'] = test_data['3rd Pressure Collect Result_AutoClave'] * test_data['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "test_data['avg_pressure_time_AutoClave'] = (test_data['1st_pressure_time_AutoClave'] +\n",
    "                                             test_data['2nd_pressure_time_AutoClave'] +\n",
    "                                             test_data['3rd_pressure_time_AutoClave']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db8a8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 컬럼 리스트\n",
    "columns_to_drop = [\n",
    "    '1st Pressure Collect Result_AutoClave',\n",
    "    '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    "    '2nd Pressure Collect Result_AutoClave',\n",
    "    '2nd Pressure Unit Time_AutoClave',\n",
    "    '3rd Pressure Collect Result_AutoClave',\n",
    "    '3rd Pressure Unit Time_AutoClave',\n",
    "]\n",
    "\n",
    "# 지정한 컬럼 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa61589",
   "metadata": {},
   "source": [
    "### 10. Time 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6080c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총시간 대비 비율 변수\n",
    "def calculate_total_time_and_ratios(data):\n",
    "    data['total_time'] = (\n",
    "        data['Machine Tact time Collect Result_Dam'] +\n",
    "        data['Machine Tact time Collect Result_Fill1'] +\n",
    "        data['Machine Tact time Collect Result_Fill2'] +\n",
    "        data['Chamber Temp. Unit Time_AutoClave']\n",
    "    )\n",
    "    data['time_ratio_Dam'] = (data['Machine Tact time Collect Result_Dam'] / data['total_time']).round(3)\n",
    "    data['time_ratio_Fill1'] = (data['Machine Tact time Collect Result_Fill1'] / data['total_time']).round(3)\n",
    "    data['time_ratio_Fill2'] = (data['Machine Tact time Collect Result_Fill2'] / data['total_time']).round(3)\n",
    "    data['time_ratio_AutoClave'] = (data['Chamber Temp. Unit Time_AutoClave'] / data['total_time']).round(3)\n",
    "    return data\n",
    "\n",
    "# train_data와 test_data에 함수 적용\n",
    "train_data = calculate_total_time_and_ratios(train_data)\n",
    "test_data = calculate_total_time_and_ratios(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "68b0d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 제거\n",
    "train_data.drop(columns=[\n",
    "    'total_time'\n",
    "    , 'Machine Tact time Collect Result_Dam'\n",
    "    , 'Machine Tact time Collect Result_Fill1'\n",
    "    , 'Machine Tact time Collect Result_Fill2'\n",
    "    , 'Chamber Temp. Unit Time_AutoClave'], inplace=True)\n",
    "\n",
    "test_data.drop(columns=[\n",
    "    'total_time'\n",
    "    , 'Machine Tact time Collect Result_Dam'\n",
    "    , 'Machine Tact time Collect Result_Fill1'\n",
    "    , 'Machine Tact time Collect Result_Fill2'\n",
    "    , 'Chamber Temp. Unit Time_AutoClave'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8e98c",
   "metadata": {},
   "source": [
    "### 11. 변수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3969dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 변수 리스트\n",
    "columns_to_drop = [\n",
    "    'Chamber Temp. Judge Value_AutoClave', \n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave', \n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave'\n",
    "]\n",
    "\n",
    "train_data = train_data.drop(columns=columns_to_drop)\n",
    "test_data = test_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ca3a1218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제된 train_data 열 개수: 37\n",
      "삭제된 test_data 열 개수: 37\n"
     ]
    }
   ],
   "source": [
    "# 값의 종류가 1개이고 결측값이 없는 열을 제거하는 함수\n",
    "def drop_single_value_columns(df):\n",
    "    cols_to_drop = [col for col in df.columns if col != 'target' and df[col].nunique() == 1 and df[col].isnull().sum() == 0]\n",
    "    df_dropped = df.drop(columns=cols_to_drop)\n",
    "    return df_dropped, cols_to_drop\n",
    "\n",
    "# train_data와 test_data에서 해당 열 제거 및 삭제된 열 이름과 개수 출력\n",
    "train_data, train_cols_dropped = drop_single_value_columns(train_data)\n",
    "test_data, test_cols_dropped = drop_single_value_columns(test_data)\n",
    "\n",
    "# print(\"삭제된 train_data 열 이름:\", train_cols_dropped)\n",
    "print(\"삭제된 train_data 열 개수:\", len(train_cols_dropped))\n",
    "\n",
    "# print(\"삭제된 test_data 열 이름:\", test_cols_dropped)\n",
    "print(\"삭제된 test_data 열 개수:\", len(test_cols_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78b604",
   "metadata": {},
   "source": [
    "### 12. target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "39d41d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['target', 'model_receip', 'cleaned_workorder',\n",
      "       'PalletID_Collect_Result', 'cure_end_position_XZ_Fill2',\n",
      "       'cure_start_position_XZ_Fill2', 'cure_standby_position_XZ_Fill2',\n",
      "       'stage1_line13_distance_speed_Dam', 'stage1_line24_distance_speed_Dam',\n",
      "       'stage2_line13_distance_speed_Dam', 'stage2_line24_distance_speed_Dam',\n",
      "       'stage3_line13_distance_speed_Dam', 'stage3_line24_distance_speed_Dam'],\n",
      "      dtype='object')  train_object_columns 갯수 : 13\n",
      "Index(['Set ID', 'model_receip', 'cleaned_workorder',\n",
      "       'PalletID_Collect_Result', 'cure_end_position_XZ_Fill2',\n",
      "       'cure_start_position_XZ_Fill2', 'cure_standby_position_XZ_Fill2',\n",
      "       'stage1_line13_distance_speed_Dam', 'stage1_line24_distance_speed_Dam',\n",
      "       'stage2_line13_distance_speed_Dam', 'stage2_line24_distance_speed_Dam',\n",
      "       'stage3_line13_distance_speed_Dam', 'stage3_line24_distance_speed_Dam'],\n",
      "      dtype='object')  test_object_columns 갯수 : 13\n",
      "\n",
      "Train Data:\n",
      "target unique 값 갯수: 2\n",
      "model_receip unique 값 갯수: 15\n",
      "cleaned_workorder unique 값 갯수: 29\n",
      "PalletID_Collect_Result unique 값 갯수: 17\n",
      "cure_end_position_XZ_Fill2 unique 값 갯수: 4\n",
      "cure_start_position_XZ_Fill2 unique 값 갯수: 5\n",
      "cure_standby_position_XZ_Fill2 unique 값 갯수: 4\n",
      "stage1_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage1_line24_distance_speed_Dam unique 값 갯수: 7\n",
      "stage2_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage2_line24_distance_speed_Dam unique 값 갯수: 10\n",
      "stage3_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage3_line24_distance_speed_Dam unique 값 갯수: 7\n",
      "\n",
      "Test Data:\n",
      "Set ID unique 값 갯수: 17361\n",
      "model_receip unique 값 갯수: 14\n",
      "cleaned_workorder unique 값 갯수: 29\n",
      "PalletID_Collect_Result unique 값 갯수: 17\n",
      "cure_end_position_XZ_Fill2 unique 값 갯수: 4\n",
      "cure_start_position_XZ_Fill2 unique 값 갯수: 5\n",
      "cure_standby_position_XZ_Fill2 unique 값 갯수: 4\n",
      "stage1_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage1_line24_distance_speed_Dam unique 값 갯수: 7\n",
      "stage2_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage2_line24_distance_speed_Dam unique 값 갯수: 10\n",
      "stage3_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage3_line24_distance_speed_Dam unique 값 갯수: 7\n"
     ]
    }
   ],
   "source": [
    "# object 타입의 변수 출력\n",
    "train_object_columns = train_data.select_dtypes(include=['object']).columns\n",
    "test_object_columns = test_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(train_object_columns, f\" train_object_columns 갯수 : {len(train_object_columns)}\")\n",
    "print(test_object_columns, f\" test_object_columns 갯수 : {len(test_object_columns)}\")\n",
    "\n",
    "# 각 object 변수의 고유 값 개수 출력\n",
    "print(\"\\nTrain Data:\")\n",
    "for col in train_object_columns:\n",
    "    unique_count = train_data[col].nunique()\n",
    "    print(f\"{col} unique 값 갯수: {unique_count}\")\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "for col in test_object_columns:\n",
    "    unique_count = test_data[col].nunique()\n",
    "    print(f\"{col} unique 값 갯수: {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "09284a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 평균 타겟 값 계산 (abnormal 전체 비율)\n",
    "train_data['target_01'] = train_data['target'].apply(lambda x: 1 if x == 'AbNormal' else 0)\n",
    "global_mean = train_data['target_01'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d066a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용할 열 리스트\n",
    "columns_to_encode = [\n",
    "    'model_receip',\n",
    "    'cleaned_workorder',\n",
    "    'PalletID_Collect_Result',\n",
    "    'cure_end_position_XZ_Fill2',\n",
    "    'cure_start_position_XZ_Fill2',\n",
    "    'cure_standby_position_XZ_Fill2',\n",
    "    'stage1_line13_distance_speed_Dam',\n",
    "    'stage1_line24_distance_speed_Dam',\n",
    "    'stage2_line13_distance_speed_Dam',\n",
    "    'stage2_line24_distance_speed_Dam',\n",
    "    'stage3_line13_distance_speed_Dam',\n",
    "    'stage3_line24_distance_speed_Dam'\n",
    "]\n",
    "\n",
    "# 전체 데이터의 평균 타겟값\n",
    "global_mean = train_data['target_01'].mean()\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    # 각 column에 대한 평균 타겟값과 카운트 계산\n",
    "    target_mean = train_data.groupby(column)['target_01'].mean()\n",
    "    count = train_data.groupby(column)['target_01'].count()\n",
    "\n",
    "    # 스무딩 적용\n",
    "    '''\n",
    "    추천 알파 값:\n",
    "    0.5: 일반적으로 많이 사용되는 값으로, 기존 데이터와 전체 평균 간의 균형을 잘 맞춰줍니다.\n",
    "    0.3: 데이터가 충분히 많고 각 카테고리의 타겟 값이 잘 분포되어 있을 때 사용.\n",
    "    0.7: 데이터가 적거나 특정 카테고리가 상대적으로 적을 때 사용.\n",
    "    '''\n",
    "    alpha = 0.5\n",
    "    smoothed_values = (target_mean * count + global_mean * alpha) / (count + alpha)\n",
    "\n",
    "    # 인코딩된 값을 데이터프레임에 추가\n",
    "    train_data[f'{column}_encoded'] = train_data[column].map(smoothed_values)\n",
    "\n",
    "    # test_data에 동일한 인코딩 값을 추가\n",
    "    encoding_dict = train_data.groupby(column)[f'{column}_encoded'].first().to_dict()\n",
    "    test_data[f'{column}_encoded'] = test_data[column].map(encoding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9a4fc9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 열 리스트\n",
    "columns_to_drop = [\n",
    "    'target_01',\n",
    "    'model_receip',\n",
    "    'cleaned_workorder',\n",
    "    'PalletID_Collect_Result',\n",
    "    'cure_end_position_XZ_Fill2',\n",
    "    'cure_start_position_XZ_Fill2',\n",
    "    'cure_standby_position_XZ_Fill2',\n",
    "    'stage1_line13_distance_speed_Dam',\n",
    "    'stage1_line24_distance_speed_Dam',\n",
    "    'stage2_line13_distance_speed_Dam',\n",
    "    'stage2_line24_distance_speed_Dam',\n",
    "    'stage3_line13_distance_speed_Dam',\n",
    "    'stage3_line24_distance_speed_Dam'\n",
    "]\n",
    "\n",
    "# train_data와 test_data에서 열 드랍\n",
    "train_data = train_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "test_data = test_data.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b90622d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 50 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   CURE SPEED Collect Result_Dam                   40506 non-null  int64  \n",
      " 1   DISCHARGED SPEED OF RESIN Collect Result_Dam    40506 non-null  int64  \n",
      " 2   Head Clean Position Z Collect Result_Dam        40506 non-null  float64\n",
      " 3   Head Purge Position Z Collect Result_Dam        40506 non-null  float64\n",
      " 4   Head Zero Position Y Collect Result_Dam         40506 non-null  float64\n",
      " 5   Stage1_Circle_Distance_Speed_Dam                40506 non-null  int64  \n",
      " 6   Stage2_Circle_Distance_Speed_Dam                40506 non-null  int64  \n",
      " 7   Stage3_Circle_Distance_Speed_Dam                40506 non-null  int64  \n",
      " 8   WorkMode Collect Result                         40506 non-null  float64\n",
      " 9   Chamber Temp. Collect Result_AutoClave          40506 non-null  int64  \n",
      " 10  DISCHARGED SPEED OF RESIN Collect Result_Fill1  40506 non-null  float64\n",
      " 11  Head Purge Position Z Collect Result_Fill1      40506 non-null  float64\n",
      " 12  CURE SPEED Collect Result_Fill2                 40506 non-null  int64  \n",
      " 13  Head Purge Position Z Collect Result_Fill2      40506 non-null  float64\n",
      " 14  target                                          40506 non-null  object \n",
      " 15  Equipment_same_num                              40506 non-null  int64  \n",
      " 16  Production_Qty_Collect_Result                   40506 non-null  int64  \n",
      " 17  CURE_DISTANCE_Dam                               40506 non-null  float64\n",
      " 18  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam          40506 non-null  float64\n",
      " 19  HEAD NORMAL DISTANCE_TRIANGLE_area_Dam          40506 non-null  float64\n",
      " 20  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam        40506 non-null  float64\n",
      " 21  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        40506 non-null  float64\n",
      " 22  HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1        40506 non-null  float64\n",
      " 23  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      40506 non-null  float64\n",
      " 24  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2        40506 non-null  float64\n",
      " 25  HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2        40506 non-null  float64\n",
      " 26  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2        40506 non-null  float64\n",
      " 27  volume_time_multip_avg_Dam                      40506 non-null  float64\n",
      " 28  volume_time_multip_avg_Fill1                    40506 non-null  float64\n",
      " 29  average_thickness_Dam                           40506 non-null  float64\n",
      " 30  1st_pressure_time_AutoClave                     40506 non-null  float64\n",
      " 31  2nd_pressure_time_AutoClave                     40506 non-null  float64\n",
      " 32  3rd_pressure_time_AutoClave                     40506 non-null  float64\n",
      " 33  avg_pressure_time_AutoClave                     40506 non-null  float64\n",
      " 34  time_ratio_Dam                                  40506 non-null  float64\n",
      " 35  time_ratio_Fill1                                40506 non-null  float64\n",
      " 36  time_ratio_Fill2                                40506 non-null  float64\n",
      " 37  time_ratio_AutoClave                            40506 non-null  float64\n",
      " 38  model_receip_encoded                            40506 non-null  float64\n",
      " 39  cleaned_workorder_encoded                       40506 non-null  float64\n",
      " 40  PalletID_Collect_Result_encoded                 40506 non-null  float64\n",
      " 41  cure_end_position_XZ_Fill2_encoded              40506 non-null  float64\n",
      " 42  cure_start_position_XZ_Fill2_encoded            40506 non-null  float64\n",
      " 43  cure_standby_position_XZ_Fill2_encoded          40506 non-null  float64\n",
      " 44  stage1_line13_distance_speed_Dam_encoded        40506 non-null  float64\n",
      " 45  stage1_line24_distance_speed_Dam_encoded        40506 non-null  float64\n",
      " 46  stage2_line13_distance_speed_Dam_encoded        40506 non-null  float64\n",
      " 47  stage2_line24_distance_speed_Dam_encoded        40506 non-null  float64\n",
      " 48  stage3_line13_distance_speed_Dam_encoded        40506 non-null  float64\n",
      " 49  stage3_line24_distance_speed_Dam_encoded        40506 non-null  float64\n",
      "dtypes: float64(40), int64(9), object(1)\n",
      "memory usage: 15.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# info 잘리지 않게 출력\n",
    "train_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2a27fef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 51 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Set ID                                          17361 non-null  object \n",
      " 1   CURE SPEED Collect Result_Dam                   17361 non-null  int64  \n",
      " 2   DISCHARGED SPEED OF RESIN Collect Result_Dam    17361 non-null  int64  \n",
      " 3   Head Clean Position Z Collect Result_Dam        17361 non-null  float64\n",
      " 4   Head Purge Position Z Collect Result_Dam        17361 non-null  float64\n",
      " 5   Head Zero Position Y Collect Result_Dam         17361 non-null  float64\n",
      " 6   Stage1_Circle_Distance_Speed_Dam                17361 non-null  int64  \n",
      " 7   Stage2_Circle_Distance_Speed_Dam                17361 non-null  int64  \n",
      " 8   Stage3_Circle_Distance_Speed_Dam                17361 non-null  int64  \n",
      " 9   WorkMode Collect Result                         17361 non-null  float64\n",
      " 10  Chamber Temp. Collect Result_AutoClave          17361 non-null  int64  \n",
      " 11  DISCHARGED SPEED OF RESIN Collect Result_Fill1  17361 non-null  float64\n",
      " 12  Head Purge Position Z Collect Result_Fill1      17361 non-null  float64\n",
      " 13  CURE SPEED Collect Result_Fill2                 17361 non-null  int64  \n",
      " 14  Head Purge Position Z Collect Result_Fill2      17361 non-null  float64\n",
      " 15  target                                          0 non-null      float64\n",
      " 16  Equipment_same_num                              17361 non-null  int64  \n",
      " 17  Production_Qty_Collect_Result                   17361 non-null  int64  \n",
      " 18  CURE_DISTANCE_Dam                               17361 non-null  float64\n",
      " 19  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam          17361 non-null  float64\n",
      " 20  HEAD NORMAL DISTANCE_TRIANGLE_area_Dam          17361 non-null  float64\n",
      " 21  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam        17361 non-null  float64\n",
      " 22  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        17361 non-null  float64\n",
      " 23  HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1        17361 non-null  float64\n",
      " 24  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      17361 non-null  float64\n",
      " 25  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2        17361 non-null  float64\n",
      " 26  HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2        17361 non-null  float64\n",
      " 27  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2        17361 non-null  float64\n",
      " 28  volume_time_multip_avg_Dam                      17361 non-null  float64\n",
      " 29  volume_time_multip_avg_Fill1                    17361 non-null  float64\n",
      " 30  average_thickness_Dam                           17361 non-null  float64\n",
      " 31  1st_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 32  2nd_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 33  3rd_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 34  avg_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 35  time_ratio_Dam                                  17361 non-null  float64\n",
      " 36  time_ratio_Fill1                                17361 non-null  float64\n",
      " 37  time_ratio_Fill2                                17361 non-null  float64\n",
      " 38  time_ratio_AutoClave                            17361 non-null  float64\n",
      " 39  model_receip_encoded                            17361 non-null  float64\n",
      " 40  cleaned_workorder_encoded                       17361 non-null  float64\n",
      " 41  PalletID_Collect_Result_encoded                 17361 non-null  float64\n",
      " 42  cure_end_position_XZ_Fill2_encoded              17361 non-null  float64\n",
      " 43  cure_start_position_XZ_Fill2_encoded            17361 non-null  float64\n",
      " 44  cure_standby_position_XZ_Fill2_encoded          17361 non-null  float64\n",
      " 45  stage1_line13_distance_speed_Dam_encoded        17361 non-null  float64\n",
      " 46  stage1_line24_distance_speed_Dam_encoded        17361 non-null  float64\n",
      " 47  stage2_line13_distance_speed_Dam_encoded        17361 non-null  float64\n",
      " 48  stage2_line24_distance_speed_Dam_encoded        17361 non-null  float64\n",
      " 49  stage3_line13_distance_speed_Dam_encoded        17361 non-null  float64\n",
      " 50  stage3_line24_distance_speed_Dam_encoded        17361 non-null  float64\n",
      "dtypes: float64(41), int64(9), object(1)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# info 잘리지 않게 출력\n",
    "test_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b666c55b",
   "metadata": {},
   "source": [
    "### 13. correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f63651b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dam, fill1, fill2 공통 변수\n",
    "var_dam_fill = [\n",
    "    'Equipment_same_num',\n",
    "    'PalletID_Collect_Result_encoded',\n",
    "    'Production_Qty_Collect_Result',\n",
    "    'WorkMode Collect Result'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "49f00756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 공통 변수\n",
    "### correlation 확인을 위한 변수 리스트\n",
    "var_all_corr = [\n",
    "    'model_receip_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]\n",
    "\n",
    "### train\n",
    "var_all_train = [\n",
    "    'target',\n",
    "    'model_receip_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]\n",
    "\n",
    "### test\n",
    "var_all_test = [\n",
    "    'Set ID',\n",
    "    'target',\n",
    "    'model_receip_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348271f7",
   "metadata": {},
   "source": [
    "- dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bc9de1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equipment_same_num',\n",
       " 'PalletID_Collect_Result_encoded',\n",
       " 'Production_Qty_Collect_Result',\n",
       " 'WorkMode Collect Result',\n",
       " 'model_receip_encoded',\n",
       " 'cleaned_workorder_encoded',\n",
       " 'CURE SPEED Collect Result_Dam',\n",
       " 'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
       " 'Head Clean Position Z Collect Result_Dam',\n",
       " 'Head Purge Position Z Collect Result_Dam',\n",
       " 'Head Zero Position Y Collect Result_Dam',\n",
       " 'Stage1_Circle_Distance_Speed_Dam',\n",
       " 'Stage2_Circle_Distance_Speed_Dam',\n",
       " 'Stage3_Circle_Distance_Speed_Dam',\n",
       " 'CURE_DISTANCE_Dam',\n",
       " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam',\n",
       " 'HEAD NORMAL DISTANCE_TRIANGLE_area_Dam',\n",
       " 'HEAD NORMAL DISTANCE_TRIANGLE_height_Dam',\n",
       " 'volume_time_multip_avg_Dam',\n",
       " 'average_thickness_Dam',\n",
       " 'time_ratio_Dam',\n",
       " 'stage1_line13_distance_speed_Dam_encoded',\n",
       " 'stage1_line24_distance_speed_Dam_encoded',\n",
       " 'stage2_line13_distance_speed_Dam_encoded',\n",
       " 'stage2_line24_distance_speed_Dam_encoded',\n",
       " 'stage3_line13_distance_speed_Dam_encoded',\n",
       " 'stage3_line24_distance_speed_Dam_encoded']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상관관계를 확인할 데이터셋\n",
    "combined_variables = var_dam_fill + var_all_corr + [var for var in train_data.columns if '_Dam' in var]\n",
    "combined_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2e3f5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Equipment_same_num',   \n",
    " 'PalletID_Collect_Result_encoded',\n",
    " 'Production_Qty_Collect_Result',\n",
    " 'WorkMode Collect Result',\n",
    " 'model_receip_encoded',\n",
    " 'cleaned_workorder_encoded',\n",
    " 'CURE SPEED Collect Result_Dam',\n",
    " 'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
    " 'Head Clean Position Z Collect Result_Dam',\n",
    " 'Head Purge Position Z Collect Result_Dam',\n",
    " 'Head Zero Position Y Collect Result_Dam',\n",
    " 'Stage1_Circle_Distance_Speed_Dam',\n",
    " 'Stage2_Circle_Distance_Speed_Dam',\n",
    " 'Stage3_Circle_Distance_Speed_Dam',\n",
    " 'CURE_DISTANCE_Dam',\n",
    " 'HEAD NORMAL DISTANCE_TRIANGLE_area_Dam',\n",
    " 'HEAD NORMAL DISTANCE_TRIANGLE_height_Dam',\n",
    " 'volume_time_multip_avg_Dam',\n",
    " 'average_thickness_Dam',\n",
    " 'time_ratio_Dam',\n",
    " 'stage1_line13_distance_speed_Dam_encoded',\n",
    " 'stage1_line24_distance_speed_Dam_encoded',\n",
    " 'stage2_line13_distance_speed_Dam_encoded',\n",
    " 'stage2_line24_distance_speed_Dam_encoded',\n",
    " 'stage3_line13_distance_speed_Dam_encoded',\n",
    " 'stage3_line24_distance_speed_Dam_encoded',\n",
    " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam']\n",
    "\n",
    "# 변수들로만 이루어진 DataFrame 생성\n",
    "filtered_data = train_data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6ee985c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Variable 1  \\\n",
      "0           Stage1_Circle_Distance_Speed_Dam   \n",
      "1           Stage1_Circle_Distance_Speed_Dam   \n",
      "2           Stage2_Circle_Distance_Speed_Dam   \n",
      "3           Stage3_Circle_Distance_Speed_Dam   \n",
      "4           Stage3_Circle_Distance_Speed_Dam   \n",
      "5     HEAD NORMAL DISTANCE_TRIANGLE_area_Dam   \n",
      "6   HEAD NORMAL DISTANCE_TRIANGLE_height_Dam   \n",
      "7                 volume_time_multip_avg_Dam   \n",
      "8                 volume_time_multip_avg_Dam   \n",
      "9   stage1_line13_distance_speed_Dam_encoded   \n",
      "10  stage1_line13_distance_speed_Dam_encoded   \n",
      "11  stage1_line13_distance_speed_Dam_encoded   \n",
      "12  stage1_line24_distance_speed_Dam_encoded   \n",
      "13  stage1_line24_distance_speed_Dam_encoded   \n",
      "14  stage1_line24_distance_speed_Dam_encoded   \n",
      "15  stage2_line13_distance_speed_Dam_encoded   \n",
      "16  stage3_line13_distance_speed_Dam_encoded   \n",
      "17  stage3_line13_distance_speed_Dam_encoded   \n",
      "18  stage3_line13_distance_speed_Dam_encoded   \n",
      "19  stage3_line24_distance_speed_Dam_encoded   \n",
      "20  stage3_line24_distance_speed_Dam_encoded   \n",
      "21  stage3_line24_distance_speed_Dam_encoded   \n",
      "\n",
      "                                  Variable 2  Correlation  \n",
      "0           Stage3_Circle_Distance_Speed_Dam     0.999912  \n",
      "1                 volume_time_multip_avg_Dam    -0.907644  \n",
      "2   stage2_line13_distance_speed_Dam_encoded     0.951291  \n",
      "3           Stage1_Circle_Distance_Speed_Dam     0.999912  \n",
      "4                 volume_time_multip_avg_Dam    -0.907396  \n",
      "5   HEAD NORMAL DISTANCE_TRIANGLE_height_Dam     1.000000  \n",
      "6     HEAD NORMAL DISTANCE_TRIANGLE_area_Dam     1.000000  \n",
      "7           Stage1_Circle_Distance_Speed_Dam    -0.907644  \n",
      "8           Stage3_Circle_Distance_Speed_Dam    -0.907396  \n",
      "9   stage1_line24_distance_speed_Dam_encoded     0.973816  \n",
      "10  stage3_line13_distance_speed_Dam_encoded     0.953003  \n",
      "11  stage3_line24_distance_speed_Dam_encoded     0.973634  \n",
      "12  stage1_line13_distance_speed_Dam_encoded     0.973816  \n",
      "13  stage3_line13_distance_speed_Dam_encoded     0.975797  \n",
      "14  stage3_line24_distance_speed_Dam_encoded     0.999947  \n",
      "15          Stage2_Circle_Distance_Speed_Dam     0.951291  \n",
      "16  stage1_line13_distance_speed_Dam_encoded     0.953003  \n",
      "17  stage1_line24_distance_speed_Dam_encoded     0.975797  \n",
      "18  stage3_line24_distance_speed_Dam_encoded     0.975849  \n",
      "19  stage1_line13_distance_speed_Dam_encoded     0.973634  \n",
      "20  stage1_line24_distance_speed_Dam_encoded     0.999947  \n",
      "21  stage3_line13_distance_speed_Dam_encoded     0.975849  \n"
     ]
    }
   ],
   "source": [
    "# 자기자신을 제외하고 상관관계 절댓값이 0.9 이상인 조합 찾기\n",
    "correlation_matrix = filtered_data.corr()\n",
    "strong_correlations = correlation_matrix[(correlation_matrix.abs() >= 0.9) & (correlation_matrix != 1)]\n",
    "\n",
    "# 리스트로 변환\n",
    "strong_correlations_pairs = strong_correlations.stack().reset_index()\n",
    "strong_correlations_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "# 결과 출력\n",
    "print(strong_correlations_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ff21e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드랍할 열 목록\n",
    "columns_to_drop = [\n",
    "    'Stage1_Circle_Distance_Speed_Dam',\n",
    "    'Stage3_Circle_Distance_Speed_Dam',\n",
    "    'HEAD NORMAL DISTANCE_TRIANGLE_area_Dam',\n",
    "    'stage1_line24_distance_speed_Dam_encoded',\n",
    "    'stage2_line13_distance_speed_Dam_encoded',\n",
    "    'stage3_line13_distance_speed_Dam_encoded',\n",
    "    'stage3_line24_distance_speed_Dam_encoded'\n",
    "]\n",
    "\n",
    "# 열 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0cc67f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Dam'을 포함하는 변수 선택\n",
    "dam_variables = [var for var in train_data.columns if '_Dam' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + dam_variables\n",
    "train_data_dam = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + dam_variables\n",
    "test_data_dam = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "84281880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 21 columns):\n",
      " #   Column                                        Non-Null Count  Dtype  \n",
      "---  ------                                        --------------  -----  \n",
      " 0   Equipment_same_num                            40506 non-null  int64  \n",
      " 1   PalletID_Collect_Result_encoded               40506 non-null  float64\n",
      " 2   Production_Qty_Collect_Result                 40506 non-null  int64  \n",
      " 3   WorkMode Collect Result                       40506 non-null  float64\n",
      " 4   target                                        40506 non-null  object \n",
      " 5   model_receip_encoded                          40506 non-null  float64\n",
      " 6   cleaned_workorder_encoded                     40506 non-null  float64\n",
      " 7   CURE SPEED Collect Result_Dam                 40506 non-null  int64  \n",
      " 8   DISCHARGED SPEED OF RESIN Collect Result_Dam  40506 non-null  int64  \n",
      " 9   Head Clean Position Z Collect Result_Dam      40506 non-null  float64\n",
      " 10  Head Purge Position Z Collect Result_Dam      40506 non-null  float64\n",
      " 11  Head Zero Position Y Collect Result_Dam       40506 non-null  float64\n",
      " 12  Stage2_Circle_Distance_Speed_Dam              40506 non-null  int64  \n",
      " 13  CURE_DISTANCE_Dam                             40506 non-null  float64\n",
      " 14  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam        40506 non-null  float64\n",
      " 15  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam      40506 non-null  float64\n",
      " 16  volume_time_multip_avg_Dam                    40506 non-null  float64\n",
      " 17  average_thickness_Dam                         40506 non-null  float64\n",
      " 18  time_ratio_Dam                                40506 non-null  float64\n",
      " 19  stage1_line13_distance_speed_Dam_encoded      40506 non-null  float64\n",
      " 20  stage2_line24_distance_speed_Dam_encoded      40506 non-null  float64\n",
      "dtypes: float64(15), int64(5), object(1)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_dam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "edf58e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 22 columns):\n",
      " #   Column                                        Non-Null Count  Dtype  \n",
      "---  ------                                        --------------  -----  \n",
      " 0   Equipment_same_num                            17361 non-null  int64  \n",
      " 1   PalletID_Collect_Result_encoded               17361 non-null  float64\n",
      " 2   Production_Qty_Collect_Result                 17361 non-null  int64  \n",
      " 3   WorkMode Collect Result                       17361 non-null  float64\n",
      " 4   Set ID                                        17361 non-null  object \n",
      " 5   target                                        0 non-null      float64\n",
      " 6   model_receip_encoded                          17361 non-null  float64\n",
      " 7   cleaned_workorder_encoded                     17361 non-null  float64\n",
      " 8   CURE SPEED Collect Result_Dam                 17361 non-null  int64  \n",
      " 9   DISCHARGED SPEED OF RESIN Collect Result_Dam  17361 non-null  int64  \n",
      " 10  Head Clean Position Z Collect Result_Dam      17361 non-null  float64\n",
      " 11  Head Purge Position Z Collect Result_Dam      17361 non-null  float64\n",
      " 12  Head Zero Position Y Collect Result_Dam       17361 non-null  float64\n",
      " 13  Stage2_Circle_Distance_Speed_Dam              17361 non-null  int64  \n",
      " 14  CURE_DISTANCE_Dam                             17361 non-null  float64\n",
      " 15  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam        17361 non-null  float64\n",
      " 16  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam      17361 non-null  float64\n",
      " 17  volume_time_multip_avg_Dam                    17361 non-null  float64\n",
      " 18  average_thickness_Dam                         17361 non-null  float64\n",
      " 19  time_ratio_Dam                                17361 non-null  float64\n",
      " 20  stage1_line13_distance_speed_Dam_encoded      17361 non-null  float64\n",
      " 21  stage2_line24_distance_speed_Dam_encoded      17361 non-null  float64\n",
      "dtypes: float64(16), int64(5), object(1)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data_dam.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc7021",
   "metadata": {},
   "source": [
    "- fill1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e6ad5cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equipment_same_num',\n",
       " 'PalletID_Collect_Result_encoded',\n",
       " 'Production_Qty_Collect_Result',\n",
       " 'WorkMode Collect Result',\n",
       " 'model_receip_encoded',\n",
       " 'cleaned_workorder_encoded',\n",
       " 'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
       " 'Head Purge Position Z Collect Result_Fill1',\n",
       " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1',\n",
       " 'HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1',\n",
       " 'HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1',\n",
       " 'volume_time_multip_avg_Fill1',\n",
       " 'time_ratio_Fill1']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상관관계를 확인할 데이터셋\n",
    "combined_variables = var_dam_fill + var_all_corr + [var for var in train_data.columns if '_Fill1' in var]\n",
    "combined_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5f94ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Equipment_same_num',\n",
    " 'PalletID_Collect_Result_encoded',\n",
    " 'Production_Qty_Collect_Result',\n",
    " 'WorkMode Collect Result',\n",
    " 'model_receip_encoded',\n",
    " 'cleaned_workorder_encoded',\n",
    " 'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
    " 'Head Purge Position Z Collect Result_Fill1',\n",
    " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1',\n",
    " 'HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1',\n",
    " 'HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1',\n",
    " 'volume_time_multip_avg_Fill1',\n",
    " 'time_ratio_Fill1']\n",
    "\n",
    "# 변수들로만 이루어진 DataFrame 생성\n",
    "filtered_data = train_data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "69270bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Variable 1  \\\n",
      "0    HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1   \n",
      "1  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1   \n",
      "\n",
      "                                   Variable 2  Correlation  \n",
      "0  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1     0.923415  \n",
      "1    HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1     0.923415  \n"
     ]
    }
   ],
   "source": [
    "# 자기자신을 제외하고 상관관계 절댓값이 0.9 이상인 조합 찾기\n",
    "correlation_matrix = filtered_data.corr()\n",
    "strong_correlations = correlation_matrix[(correlation_matrix.abs() >= 0.9) & (correlation_matrix != 1)]\n",
    "\n",
    "# 리스트로 변환\n",
    "strong_correlations_pairs = strong_correlations.stack().reset_index()\n",
    "strong_correlations_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "# 결과 출력\n",
    "print(strong_correlations_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f7dbd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드랍할 열 목록\n",
    "columns_to_drop = ['HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1']\n",
    "\n",
    "# 열 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "75cc28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill1'을 포함하는 변수 선택\n",
    "fill1_variables = [var for var in train_data.columns if '_Fill1' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill1_variables\n",
    "train_data_fill1 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill1_variables\n",
    "test_data_fill1 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "88238227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 13 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Equipment_same_num                              40506 non-null  int64  \n",
      " 1   PalletID_Collect_Result_encoded                 40506 non-null  float64\n",
      " 2   Production_Qty_Collect_Result                   40506 non-null  int64  \n",
      " 3   WorkMode Collect Result                         40506 non-null  float64\n",
      " 4   target                                          40506 non-null  object \n",
      " 5   model_receip_encoded                            40506 non-null  float64\n",
      " 6   cleaned_workorder_encoded                       40506 non-null  float64\n",
      " 7   DISCHARGED SPEED OF RESIN Collect Result_Fill1  40506 non-null  float64\n",
      " 8   Head Purge Position Z Collect Result_Fill1      40506 non-null  float64\n",
      " 9   HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        40506 non-null  float64\n",
      " 10  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      40506 non-null  float64\n",
      " 11  volume_time_multip_avg_Fill1                    40506 non-null  float64\n",
      " 12  time_ratio_Fill1                                40506 non-null  float64\n",
      "dtypes: float64(10), int64(2), object(1)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_fill1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e9432e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Equipment_same_num                              17361 non-null  int64  \n",
      " 1   PalletID_Collect_Result_encoded                 17361 non-null  float64\n",
      " 2   Production_Qty_Collect_Result                   17361 non-null  int64  \n",
      " 3   WorkMode Collect Result                         17361 non-null  float64\n",
      " 4   Set ID                                          17361 non-null  object \n",
      " 5   target                                          0 non-null      float64\n",
      " 6   model_receip_encoded                            17361 non-null  float64\n",
      " 7   cleaned_workorder_encoded                       17361 non-null  float64\n",
      " 8   DISCHARGED SPEED OF RESIN Collect Result_Fill1  17361 non-null  float64\n",
      " 9   Head Purge Position Z Collect Result_Fill1      17361 non-null  float64\n",
      " 10  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        17361 non-null  float64\n",
      " 11  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      17361 non-null  float64\n",
      " 12  volume_time_multip_avg_Fill1                    17361 non-null  float64\n",
      " 13  time_ratio_Fill1                                17361 non-null  float64\n",
      "dtypes: float64(11), int64(2), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data_fill1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f4a40",
   "metadata": {},
   "source": [
    "- fill2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "94a9d4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equipment_same_num',\n",
       " 'PalletID_Collect_Result_encoded',\n",
       " 'Production_Qty_Collect_Result',\n",
       " 'WorkMode Collect Result',\n",
       " 'model_receip_encoded',\n",
       " 'cleaned_workorder_encoded',\n",
       " 'CURE SPEED Collect Result_Fill2',\n",
       " 'Head Purge Position Z Collect Result_Fill2',\n",
       " 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2',\n",
       " 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2',\n",
       " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2',\n",
       " 'time_ratio_Fill2',\n",
       " 'cure_end_position_XZ_Fill2_encoded',\n",
       " 'cure_start_position_XZ_Fill2_encoded',\n",
       " 'cure_standby_position_XZ_Fill2_encoded']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상관관계를 확인할 데이터셋\n",
    "combined_variables = var_dam_fill + var_all_corr + [var for var in train_data.columns if '_Fill2' in var]\n",
    "combined_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e4067c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Equipment_same_num',\n",
    " 'PalletID_Collect_Result_encoded',\n",
    " 'Production_Qty_Collect_Result',\n",
    " 'WorkMode Collect Result',\n",
    " 'model_receip_encoded',\n",
    " 'cleaned_workorder_encoded',\n",
    " 'CURE SPEED Collect Result_Fill2',\n",
    " 'Head Purge Position Z Collect Result_Fill2',\n",
    " 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2',\n",
    " 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2',\n",
    " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2',\n",
    " 'time_ratio_Fill2',\n",
    " 'cure_end_position_XZ_Fill2_encoded',\n",
    " 'cure_start_position_XZ_Fill2_encoded',\n",
    " 'cure_standby_position_XZ_Fill2_encoded']\n",
    "\n",
    "# 변수들로만 이루어진 DataFrame 생성\n",
    "filtered_data = train_data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "070ecf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Variable 1  \\\n",
      "0  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2   \n",
      "1  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2   \n",
      "2  HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2   \n",
      "3  HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2   \n",
      "4  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2   \n",
      "5  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2   \n",
      "6      cure_start_position_XZ_Fill2_encoded   \n",
      "7    cure_standby_position_XZ_Fill2_encoded   \n",
      "\n",
      "                                 Variable 2  Correlation  \n",
      "0  HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2     0.999993  \n",
      "1  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2     0.999999  \n",
      "2  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2     0.999993  \n",
      "3  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2     0.999997  \n",
      "4  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2     0.999999  \n",
      "5  HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2     0.999997  \n",
      "6    cure_standby_position_XZ_Fill2_encoded     0.999366  \n",
      "7      cure_start_position_XZ_Fill2_encoded     0.999366  \n"
     ]
    }
   ],
   "source": [
    "# 자기자신을 제외하고 상관관계 절댓값이 0.9 이상인 조합 찾기\n",
    "correlation_matrix = filtered_data.corr()\n",
    "strong_correlations = correlation_matrix[(correlation_matrix.abs() >= 0.9) & (correlation_matrix != 1)]\n",
    "\n",
    "# 리스트로 변환\n",
    "strong_correlations_pairs = strong_correlations.stack().reset_index()\n",
    "strong_correlations_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "# 결과 출력\n",
    "print(strong_correlations_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "21bcc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드랍할 열 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2',\n",
    "    'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2',\n",
    "    'cure_standby_position_XZ_Fill2_encoded'\n",
    "]\n",
    "\n",
    "# 열 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2d8cef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill2'을 포함하는 변수 선택\n",
    "fill2_variables = [var for var in train_data.columns if '_Fill2' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill2_variables\n",
    "train_data_fill2 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill2_variables\n",
    "test_data_fill2 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "61514701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 13 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   Equipment_same_num                          40506 non-null  int64  \n",
      " 1   PalletID_Collect_Result_encoded             40506 non-null  float64\n",
      " 2   Production_Qty_Collect_Result               40506 non-null  int64  \n",
      " 3   WorkMode Collect Result                     40506 non-null  float64\n",
      " 4   target                                      40506 non-null  object \n",
      " 5   model_receip_encoded                        40506 non-null  float64\n",
      " 6   cleaned_workorder_encoded                   40506 non-null  float64\n",
      " 7   CURE SPEED Collect Result_Fill2             40506 non-null  int64  \n",
      " 8   Head Purge Position Z Collect Result_Fill2  40506 non-null  float64\n",
      " 9   HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2    40506 non-null  float64\n",
      " 10  time_ratio_Fill2                            40506 non-null  float64\n",
      " 11  cure_end_position_XZ_Fill2_encoded          40506 non-null  float64\n",
      " 12  cure_start_position_XZ_Fill2_encoded        40506 non-null  float64\n",
      "dtypes: float64(9), int64(3), object(1)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_fill2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "85b317eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   Equipment_same_num                          17361 non-null  int64  \n",
      " 1   PalletID_Collect_Result_encoded             17361 non-null  float64\n",
      " 2   Production_Qty_Collect_Result               17361 non-null  int64  \n",
      " 3   WorkMode Collect Result                     17361 non-null  float64\n",
      " 4   Set ID                                      17361 non-null  object \n",
      " 5   target                                      0 non-null      float64\n",
      " 6   model_receip_encoded                        17361 non-null  float64\n",
      " 7   cleaned_workorder_encoded                   17361 non-null  float64\n",
      " 8   CURE SPEED Collect Result_Fill2             17361 non-null  int64  \n",
      " 9   Head Purge Position Z Collect Result_Fill2  17361 non-null  float64\n",
      " 10  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2    17361 non-null  float64\n",
      " 11  time_ratio_Fill2                            17361 non-null  float64\n",
      " 12  cure_end_position_XZ_Fill2_encoded          17361 non-null  float64\n",
      " 13  cure_start_position_XZ_Fill2_encoded        17361 non-null  float64\n",
      "dtypes: float64(10), int64(3), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data_fill2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03146277",
   "metadata": {},
   "source": [
    "- autoclave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b58e1a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_receip_encoded',\n",
       " 'cleaned_workorder_encoded',\n",
       " 'Chamber Temp. Collect Result_AutoClave',\n",
       " '1st_pressure_time_AutoClave',\n",
       " '2nd_pressure_time_AutoClave',\n",
       " '3rd_pressure_time_AutoClave',\n",
       " 'avg_pressure_time_AutoClave',\n",
       " 'time_ratio_AutoClave']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상관관계를 확인할 데이터셋\n",
    "combined_variables = var_all_corr + [var for var in train_data.columns if '_AutoClave' in var]\n",
    "combined_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ec0bd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['model_receip_encoded',\n",
    " 'cleaned_workorder_encoded',\n",
    " 'Chamber Temp. Collect Result_AutoClave',\n",
    " '1st_pressure_time_AutoClave',\n",
    " '2nd_pressure_time_AutoClave',\n",
    " '3rd_pressure_time_AutoClave',\n",
    " 'avg_pressure_time_AutoClave',\n",
    " 'time_ratio_AutoClave']\n",
    "\n",
    "# 변수들로만 이루어진 DataFrame 생성\n",
    "filtered_data = train_data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0747f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Variable 1                   Variable 2  Correlation\n",
      "0  2nd_pressure_time_AutoClave  avg_pressure_time_AutoClave     0.907845\n",
      "1  avg_pressure_time_AutoClave  2nd_pressure_time_AutoClave     0.907845\n"
     ]
    }
   ],
   "source": [
    "# 자기자신을 제외하고 상관관계 절댓값이 0.9 이상인 조합 찾기\n",
    "correlation_matrix = filtered_data.corr()\n",
    "strong_correlations = correlation_matrix[(correlation_matrix.abs() >= 0.9) & (correlation_matrix != 1)]\n",
    "\n",
    "# 리스트로 변환\n",
    "strong_correlations_pairs = strong_correlations.stack().reset_index()\n",
    "strong_correlations_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "# 결과 출력\n",
    "print(strong_correlations_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7a60741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드랍할 열 목록\n",
    "columns_to_drop = ['avg_pressure_time_AutoClave']\n",
    "\n",
    "# 열 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dbc0f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_AutoClave'을 포함하는 변수 선택\n",
    "autoclave_variables = [var for var in train_data.columns if '_AutoClave' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_all_train + autoclave_variables\n",
    "train_data_autoclave = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_all_test + autoclave_variables\n",
    "test_data_autoclave = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d72a66a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 8 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   target                                  40506 non-null  object \n",
      " 1   model_receip_encoded                    40506 non-null  float64\n",
      " 2   cleaned_workorder_encoded               40506 non-null  float64\n",
      " 3   Chamber Temp. Collect Result_AutoClave  40506 non-null  int64  \n",
      " 4   1st_pressure_time_AutoClave             40506 non-null  float64\n",
      " 5   2nd_pressure_time_AutoClave             40506 non-null  float64\n",
      " 6   3rd_pressure_time_AutoClave             40506 non-null  float64\n",
      " 7   time_ratio_AutoClave                    40506 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_autoclave.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "62c03e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   Set ID                                  17361 non-null  object \n",
      " 1   target                                  0 non-null      float64\n",
      " 2   model_receip_encoded                    17361 non-null  float64\n",
      " 3   cleaned_workorder_encoded               17361 non-null  float64\n",
      " 4   Chamber Temp. Collect Result_AutoClave  17361 non-null  int64  \n",
      " 5   1st_pressure_time_AutoClave             17361 non-null  float64\n",
      " 6   2nd_pressure_time_AutoClave             17361 non-null  float64\n",
      " 7   3rd_pressure_time_AutoClave             17361 non-null  float64\n",
      " 8   time_ratio_AutoClave                    17361 non-null  float64\n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data_autoclave.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "72078d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 38 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   CURE SPEED Collect Result_Dam                   40506 non-null  int64  \n",
      " 1   DISCHARGED SPEED OF RESIN Collect Result_Dam    40506 non-null  int64  \n",
      " 2   Head Clean Position Z Collect Result_Dam        40506 non-null  float64\n",
      " 3   Head Purge Position Z Collect Result_Dam        40506 non-null  float64\n",
      " 4   Head Zero Position Y Collect Result_Dam         40506 non-null  float64\n",
      " 5   Stage2_Circle_Distance_Speed_Dam                40506 non-null  int64  \n",
      " 6   WorkMode Collect Result                         40506 non-null  float64\n",
      " 7   Chamber Temp. Collect Result_AutoClave          40506 non-null  int64  \n",
      " 8   DISCHARGED SPEED OF RESIN Collect Result_Fill1  40506 non-null  float64\n",
      " 9   Head Purge Position Z Collect Result_Fill1      40506 non-null  float64\n",
      " 10  CURE SPEED Collect Result_Fill2                 40506 non-null  int64  \n",
      " 11  Head Purge Position Z Collect Result_Fill2      40506 non-null  float64\n",
      " 12  target                                          40506 non-null  object \n",
      " 13  Equipment_same_num                              40506 non-null  int64  \n",
      " 14  Production_Qty_Collect_Result                   40506 non-null  int64  \n",
      " 15  CURE_DISTANCE_Dam                               40506 non-null  float64\n",
      " 16  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam          40506 non-null  float64\n",
      " 17  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam        40506 non-null  float64\n",
      " 18  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        40506 non-null  float64\n",
      " 19  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      40506 non-null  float64\n",
      " 20  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2        40506 non-null  float64\n",
      " 21  volume_time_multip_avg_Dam                      40506 non-null  float64\n",
      " 22  volume_time_multip_avg_Fill1                    40506 non-null  float64\n",
      " 23  average_thickness_Dam                           40506 non-null  float64\n",
      " 24  1st_pressure_time_AutoClave                     40506 non-null  float64\n",
      " 25  2nd_pressure_time_AutoClave                     40506 non-null  float64\n",
      " 26  3rd_pressure_time_AutoClave                     40506 non-null  float64\n",
      " 27  time_ratio_Dam                                  40506 non-null  float64\n",
      " 28  time_ratio_Fill1                                40506 non-null  float64\n",
      " 29  time_ratio_Fill2                                40506 non-null  float64\n",
      " 30  time_ratio_AutoClave                            40506 non-null  float64\n",
      " 31  model_receip_encoded                            40506 non-null  float64\n",
      " 32  cleaned_workorder_encoded                       40506 non-null  float64\n",
      " 33  PalletID_Collect_Result_encoded                 40506 non-null  float64\n",
      " 34  cure_end_position_XZ_Fill2_encoded              40506 non-null  float64\n",
      " 35  cure_start_position_XZ_Fill2_encoded            40506 non-null  float64\n",
      " 36  stage1_line13_distance_speed_Dam_encoded        40506 non-null  float64\n",
      " 37  stage2_line24_distance_speed_Dam_encoded        40506 non-null  float64\n",
      "dtypes: float64(30), int64(7), object(1)\n",
      "memory usage: 11.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "85988076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 39 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Set ID                                          17361 non-null  object \n",
      " 1   CURE SPEED Collect Result_Dam                   17361 non-null  int64  \n",
      " 2   DISCHARGED SPEED OF RESIN Collect Result_Dam    17361 non-null  int64  \n",
      " 3   Head Clean Position Z Collect Result_Dam        17361 non-null  float64\n",
      " 4   Head Purge Position Z Collect Result_Dam        17361 non-null  float64\n",
      " 5   Head Zero Position Y Collect Result_Dam         17361 non-null  float64\n",
      " 6   Stage2_Circle_Distance_Speed_Dam                17361 non-null  int64  \n",
      " 7   WorkMode Collect Result                         17361 non-null  float64\n",
      " 8   Chamber Temp. Collect Result_AutoClave          17361 non-null  int64  \n",
      " 9   DISCHARGED SPEED OF RESIN Collect Result_Fill1  17361 non-null  float64\n",
      " 10  Head Purge Position Z Collect Result_Fill1      17361 non-null  float64\n",
      " 11  CURE SPEED Collect Result_Fill2                 17361 non-null  int64  \n",
      " 12  Head Purge Position Z Collect Result_Fill2      17361 non-null  float64\n",
      " 13  target                                          0 non-null      float64\n",
      " 14  Equipment_same_num                              17361 non-null  int64  \n",
      " 15  Production_Qty_Collect_Result                   17361 non-null  int64  \n",
      " 16  CURE_DISTANCE_Dam                               17361 non-null  float64\n",
      " 17  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam          17361 non-null  float64\n",
      " 18  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam        17361 non-null  float64\n",
      " 19  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        17361 non-null  float64\n",
      " 20  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      17361 non-null  float64\n",
      " 21  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2        17361 non-null  float64\n",
      " 22  volume_time_multip_avg_Dam                      17361 non-null  float64\n",
      " 23  volume_time_multip_avg_Fill1                    17361 non-null  float64\n",
      " 24  average_thickness_Dam                           17361 non-null  float64\n",
      " 25  1st_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 26  2nd_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 27  3rd_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 28  time_ratio_Dam                                  17361 non-null  float64\n",
      " 29  time_ratio_Fill1                                17361 non-null  float64\n",
      " 30  time_ratio_Fill2                                17361 non-null  float64\n",
      " 31  time_ratio_AutoClave                            17361 non-null  float64\n",
      " 32  model_receip_encoded                            17361 non-null  float64\n",
      " 33  cleaned_workorder_encoded                       17361 non-null  float64\n",
      " 34  PalletID_Collect_Result_encoded                 17361 non-null  float64\n",
      " 35  cure_end_position_XZ_Fill2_encoded              17361 non-null  float64\n",
      " 36  cure_start_position_XZ_Fill2_encoded            17361 non-null  float64\n",
      " 37  stage1_line13_distance_speed_Dam_encoded        17361 non-null  float64\n",
      " 38  stage2_line24_distance_speed_Dam_encoded        17361 non-null  float64\n",
      "dtypes: float64(31), int64(7), object(1)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3397567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 39 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Set ID                                          17361 non-null  object \n",
      " 1   CURE SPEED Collect Result_Dam                   17361 non-null  int64  \n",
      " 2   DISCHARGED SPEED OF RESIN Collect Result_Dam    17361 non-null  int64  \n",
      " 3   Head Clean Position Z Collect Result_Dam        17361 non-null  float64\n",
      " 4   Head Purge Position Z Collect Result_Dam        17361 non-null  float64\n",
      " 5   Head Zero Position Y Collect Result_Dam         17361 non-null  float64\n",
      " 6   Stage2_Circle_Distance_Speed_Dam                17361 non-null  int64  \n",
      " 7   WorkMode Collect Result                         17361 non-null  float64\n",
      " 8   Chamber Temp. Collect Result_AutoClave          17361 non-null  int64  \n",
      " 9   DISCHARGED SPEED OF RESIN Collect Result_Fill1  17361 non-null  float64\n",
      " 10  Head Purge Position Z Collect Result_Fill1      17361 non-null  float64\n",
      " 11  CURE SPEED Collect Result_Fill2                 17361 non-null  int64  \n",
      " 12  Head Purge Position Z Collect Result_Fill2      17361 non-null  float64\n",
      " 13  target                                          0 non-null      float64\n",
      " 14  Equipment_same_num                              17361 non-null  int64  \n",
      " 15  Production_Qty_Collect_Result                   17361 non-null  int64  \n",
      " 16  CURE_DISTANCE_Dam                               17361 non-null  float64\n",
      " 17  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam          17361 non-null  float64\n",
      " 18  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam        17361 non-null  float64\n",
      " 19  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        17361 non-null  float64\n",
      " 20  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      17361 non-null  float64\n",
      " 21  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2        17361 non-null  float64\n",
      " 22  volume_time_multip_avg_Dam                      17361 non-null  float64\n",
      " 23  volume_time_multip_avg_Fill1                    17361 non-null  float64\n",
      " 24  average_thickness_Dam                           17361 non-null  float64\n",
      " 25  1st_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 26  2nd_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 27  3rd_pressure_time_AutoClave                     17361 non-null  float64\n",
      " 28  time_ratio_Dam                                  17361 non-null  float64\n",
      " 29  time_ratio_Fill1                                17361 non-null  float64\n",
      " 30  time_ratio_Fill2                                17361 non-null  float64\n",
      " 31  time_ratio_AutoClave                            17361 non-null  float64\n",
      " 32  model_receip_encoded                            17361 non-null  float64\n",
      " 33  cleaned_workorder_encoded                       17361 non-null  float64\n",
      " 34  PalletID_Collect_Result_encoded                 17361 non-null  float64\n",
      " 35  cure_end_position_XZ_Fill2_encoded              17361 non-null  float64\n",
      " 36  cure_start_position_XZ_Fill2_encoded            17361 non-null  float64\n",
      " 37  stage1_line13_distance_speed_Dam_encoded        17361 non-null  float64\n",
      " 38  stage2_line24_distance_speed_Dam_encoded        17361 non-null  float64\n",
      "dtypes: float64(31), int64(7), object(1)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7192f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame을 CSV 파일로 저장\n",
    "train_data.to_csv('./data/train_data_0825.csv', index=False)\n",
    "test_data.to_csv('./data/test_data_0825.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca12a2",
   "metadata": {},
   "source": [
    "## 3. 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a3316",
   "metadata": {},
   "source": [
    "공정별 데이터 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "da0b5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.3\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# csv 불러오기\n",
    "train_data = pd.read_csv('./data/train_data_0825.csv')\n",
    "test_data = pd.read_csv('./data/test_data_0825.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "81b54ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dam, fill1, fill2 공통 변수\n",
    "var_dam_fill = [\n",
    "    'Equipment_same_num',\n",
    "    'PalletID_Collect_Result_encoded',\n",
    "    'Production_Qty_Collect_Result',\n",
    "    'WorkMode Collect Result'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7344e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 전체 공통 변수\n",
    "### correlation 확인을 위한 변수 리스트\n",
    "var_all_corr = [\n",
    "    'model_receip_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]\n",
    "\n",
    "### train\n",
    "var_all_train = [\n",
    "    'target',\n",
    "    'model_receip_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]\n",
    "\n",
    "### test\n",
    "var_all_test = [\n",
    "    'Set ID',\n",
    "    'target',\n",
    "    'model_receip_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b3e9d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Dam'을 포함하는 변수 선택\n",
    "dam_variables = [var for var in train_data.columns if '_Dam' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + dam_variables\n",
    "train_data_dam = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + dam_variables\n",
    "test_data_dam = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c718433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill1'을 포함하는 변수 선택\n",
    "fill1_variables = [var for var in train_data.columns if '_Fill1' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill1_variables\n",
    "train_data_fill1 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill1_variables\n",
    "test_data_fill1 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c084e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill2'을 포함하는 변수 선택\n",
    "fill2_variables = [var for var in train_data.columns if '_Fill2' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill2_variables\n",
    "train_data_fill2 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill2_variables\n",
    "test_data_fill2 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "71ce913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_AutoClave'을 포함하는 변수 선택\n",
    "autoclave_variables = [var for var in train_data.columns if '_AutoClave' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_all_train + autoclave_variables\n",
    "train_data_autoclave = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_all_test + autoclave_variables\n",
    "test_data_autoclave = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ce506b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train data-----\n",
      "train_data DataFrame의 칼럼 수: 38\n",
      "train_data_dam DataFrame의 칼럼 수: 21\n",
      "train_data_autoclave DataFrame의 칼럼 수: 8\n",
      "train_data_fill1 DataFrame의 칼럼 수: 13\n",
      "train_data_fill2 DataFrame의 칼럼 수: 13\n",
      "----test data-----\n",
      "test_data DataFrame의 칼럼 수: 39\n",
      "test_data_dam DataFrame의 칼럼 수: 22\n",
      "test_data_autoclave DataFrame의 칼럼 수: 9\n",
      "test_data_fill1 DataFrame의 칼럼 수: 14\n",
      "test_data_fill2 DataFrame의 칼럼 수: 14\n"
     ]
    }
   ],
   "source": [
    "# 각 DataFrame의 칼럼 수 계산\n",
    "num_columns_train_data = train_data.shape[1]\n",
    "num_columns_train_data_dam = train_data_dam.shape[1]\n",
    "num_columns_train_data_autoclave = train_data_autoclave.shape[1]\n",
    "num_columns_train_data_fill1 = train_data_fill1.shape[1]\n",
    "num_columns_train_data_fill2 = train_data_fill2.shape[1]\n",
    "\n",
    "num_columns_test_data = test_data.shape[1]\n",
    "num_columns_test_data_dam = test_data_dam.shape[1]\n",
    "num_columns_test_data_autoclave = test_data_autoclave.shape[1]\n",
    "num_columns_test_data_fill1 = test_data_fill1.shape[1]\n",
    "num_columns_test_data_fill2 = test_data_fill2.shape[1]\n",
    "\n",
    "# 각 DataFrame의 칼럼 수 출력\n",
    "print(\"----train data-----\")\n",
    "print(f\"train_data DataFrame의 칼럼 수: {num_columns_train_data}\")\n",
    "print(f\"train_data_dam DataFrame의 칼럼 수: {num_columns_train_data_dam}\")\n",
    "print(f\"train_data_autoclave DataFrame의 칼럼 수: {num_columns_train_data_autoclave}\")\n",
    "print(f\"train_data_fill1 DataFrame의 칼럼 수: {num_columns_train_data_fill1}\")\n",
    "print(f\"train_data_fill2 DataFrame의 칼럼 수: {num_columns_train_data_fill2}\")\n",
    "print(\"----test data-----\")\n",
    "print(f\"test_data DataFrame의 칼럼 수: {num_columns_test_data}\")\n",
    "print(f\"test_data_dam DataFrame의 칼럼 수: {num_columns_test_data_dam}\")\n",
    "print(f\"test_data_autoclave DataFrame의 칼럼 수: {num_columns_test_data_autoclave}\")\n",
    "print(f\"test_data_fill1 DataFrame의 칼럼 수: {num_columns_test_data_fill1}\")\n",
    "print(f\"test_data_fill2 DataFrame의 칼럼 수: {num_columns_test_data_fill2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c88627",
   "metadata": {},
   "source": [
    "모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fb2f64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 스레드홀드 설정\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "# 모델 설정 및 하이퍼파라미터\n",
    "models = {\n",
    "    'et': ExtraTreesClassifier(),\n",
    "    'rf': RandomForestClassifier(),\n",
    "    'cat': CatBoostClassifier(),\n",
    "    'lgbm': LGBMClassifier(),\n",
    "    'xgb': XGBClassifier(),\n",
    "    'dt': DecisionTreeClassifier(),\n",
    "    'ada': AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "def train_and_evaluate_model(model_name, data, **params):\n",
    "    if model_name not in models:\n",
    "        print(f\"{model_name}은(는) 지원되지 않는 모델입니다.\")\n",
    "        return\n",
    "    \n",
    "    # 데이터셋 분할\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        data.drop(\"target\", axis=1),\n",
    "        data[\"target\"].map({'Normal': 0, 'AbNormal': 1}),\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # 모델 선택\n",
    "    model = models[model_name].__class__()  # 새로운 모델 인스턴스 생성\n",
    "\n",
    "    # 하이퍼파라미터 설정\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # 데이터 이름을 자동으로 추출하기 위한 래퍼 함수\n",
    "    data_name = [name for name in globals() if globals()[name] is data][0]\n",
    "\n",
    "    # 예측\n",
    "    y_val_pred_proba = model.predict_proba(x_val)[:, 1]  # 양성 클래스 확률\n",
    "    y_val_pred = (y_val_pred_proba >= THRESHOLD).astype(int)  # 스레드홀드에 따른 예측\n",
    "\n",
    "    # 평가지표 계산\n",
    "    f1 = f1_score(y_val, y_val_pred, average=\"binary\")\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f'{model_name} 모델이 {data_name} 데이터로 학습한 결과:')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('---')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('---')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print('\\n')\n",
    "\n",
    "    return model  # 학습된 모델 반환\n",
    "\n",
    "def fit_all_train_data_function(model_name, data, **params):\n",
    "    if model_name not in models:\n",
    "        print(f\"{model_name}은(는) 지원되지 않는 모델입니다.\")\n",
    "        return None  # 지원되지 않는 모델일 경우 None 반환\n",
    "    \n",
    "    # 모델 선택\n",
    "    model = models[model_name].__class__()  # 새로운 모델 인스턴스 생성\n",
    "\n",
    "    # 하이퍼파라미터 설정\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(data.drop(\"target\", axis=1), data[\"target\"].map({'Normal': 0, 'AbNormal': 1}))\n",
    "\n",
    "    # 데이터 이름을 자동으로 추출하기 위한 래퍼 함수\n",
    "    data_name = [name for name in globals() if globals()[name] is data][0]\n",
    "\n",
    "    print(f'{model_name} 모델이 {data_name} 데이터로 학습 완료')\n",
    "    return model  # 학습된 모델 반환\n",
    "\n",
    "def voting_function(data, estimators, voting='hard', threshold=0.5):\n",
    "    # 데이터셋 분할 # voting='hard'일 경우 threshold는 사용되지 않음\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        data.drop(\"target\", axis=1),\n",
    "        data[\"target\"].map({'Normal': 0, 'AbNormal': 1}),\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # VotingClassifier 설정\n",
    "    voting_clf = VotingClassifier(estimators=estimators, voting=voting)\n",
    "\n",
    "    # 모델 학습\n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "    if voting == 'soft':\n",
    "        # 소프트 보팅의 경우 확률 예측\n",
    "        y_val_pred_proba = voting_clf.predict_proba(x_val)[:, 1]\n",
    "        y_val_pred = (y_val_pred_proba >= threshold).astype(int)\n",
    "    else:\n",
    "        # 하드 보팅의 경우 직접 예측\n",
    "        y_val_pred = voting_clf.predict(x_val)\n",
    "\n",
    "    # 평가지표 계산\n",
    "    f1 = f1_score(y_val, y_val_pred, average=\"binary\")\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f'Voting Classifier로 학습한 결과:')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('---')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('---')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print('\\n')\n",
    "\n",
    "    return voting_clf  # 학습된 VotingClassifier 반환\n",
    "\n",
    "def voting(preds_or_probs, method='soft', threshold=0.3):\n",
    "    \"\"\"\n",
    "    하드 보팅 또는 소프트 보팅을 사용하여 최종 예측을 수행합니다.\n",
    "\n",
    "    Parameters:\n",
    "    preds_or_probs (list of np.array): 각 모델의 예측 배열 리스트 (하드 보팅) 또는 예측 확률 배열 리스트 (소프트 보팅)\n",
    "    method (str): 'soft' 또는 'hard' 보팅 방법 선택\n",
    "    threshold (float): 소프트 보팅 시 예측을 양성으로 간주할 확률 임계값\n",
    "\n",
    "    Returns:\n",
    "    np.array: 최종 예측 결과\n",
    "    \"\"\"\n",
    "    if method == 'soft':\n",
    "        # 소프트 보팅: 각 모델의 확률 평균 계산\n",
    "        soft_voting_probs = np.mean(preds_or_probs, axis=0)\n",
    "        # 최종 예측: 평균 확률에 대해 스레드 홀드 적용\n",
    "        final_predictions = (soft_voting_probs >= threshold).astype(int)\n",
    "    elif method == 'hard':\n",
    "        # 하드 보팅: 각 모델의 예측을 모아서 다수결 원칙 적용\n",
    "        preds = np.array(preds_or_probs)\n",
    "        final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=preds)\n",
    "    else:\n",
    "        raise ValueError(\"method 인자는 'soft' 또는 'hard'여야 합니다.\")\n",
    "    \n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a060f",
   "metadata": {},
   "source": [
    "공정별 모델 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7e2e9",
   "metadata": {},
   "source": [
    "lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dam = fit_all_train_data_function(\n",
    "    'lgbm', train_data_dam\n",
    "    , n_estimators=1467\n",
    "    , num_leaves=2545\n",
    "    , max_depth=37\n",
    "    , learning_rate=0.04353920224587149 \n",
    "    , min_child_samples=83\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'lgbm', train_data_autoclave\n",
    "    , n_estimators=1563\n",
    "    , num_leaves=1885\n",
    "    , max_depth=15\n",
    "    , learning_rate=0.07033655355880039 \n",
    "    , min_child_samples=158\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'lgbm', train_data_fill1\n",
    "    , n_estimators=1452\n",
    "    , num_leaves=1581\n",
    "    , max_depth=22\n",
    "    , learning_rate=0.002000452888170992 \n",
    "    , min_child_samples=43\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'lgbm', train_data_fill2\n",
    "    , n_estimators=1632\n",
    "    , num_leaves=1426\n",
    "    , max_depth=8\n",
    "    , learning_rate=0.07487990991624197 \n",
    "    , min_child_samples=90\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'lgbm', train_data\n",
    "    , n_estimators=2383\n",
    "    , num_leaves=2528\n",
    "    , max_depth=343\n",
    "    , learning_rate=0.04661896043153508\n",
    "    , min_child_samples=209\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]\n",
    "\n",
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(probs, method='soft', threshold=0.26)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data0817_lgbm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e9dc9",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05acdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dam = fit_all_train_data_function(\n",
    "    'xgb', train_data_dam\n",
    "    , n_estimators = 1509\n",
    "    , learning_rate = 0.06418917852996714\n",
    "    , max_depth = 6\n",
    "    , alpha = 0.00017309557032608048\n",
    "    , gamma = 0.00398067155434722\n",
    "    , reg_alpha = 0.756006595834120\n",
    "    , reg_lambda = 0.3962538649486449\n",
    "    , colsample_bytree =0.8752205595930229\n",
    "    , subsample = 0.224637741333797\n",
    "    , objective = 'binary:logistic'\n",
    "    , tree_method = 'exact'\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'xgb', train_data_autoclave,\n",
    "    n_estimators = 1539, \n",
    "    learning_rate = 0.026860419341696404, \n",
    "    max_depth = 14, \n",
    "    alpha = 1.9237525550524492e-05, \n",
    "    gamma = 2.2016346534611754e-05, \n",
    "    reg_alpha = 0.9148863773292526, \n",
    "    reg_lambda = 0.6194458787523232, \n",
    "    colsample_bytree = 0.902872150299903, \n",
    "    subsample = 0.10750014546599479,\n",
    "    objective = 'binary:logistic', \n",
    "    tree_method = \"exact\", \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'xgb', train_data_fill1,\n",
    "    n_estimators = 1707, \n",
    "    learning_rate = 0.0321470219836192, \n",
    "    max_depth = 7, \n",
    "    alpha = 7.368872823521818e-05, \n",
    "    gamma = 0.0007930035188326916, \n",
    "    reg_alpha = 0.644199314174124, \n",
    "    reg_lambda = 0.588270569327407, \n",
    "    colsample_bytree = 0.883929103208459, \n",
    "    subsample = 0.2534703342501092,\n",
    "    objective = 'binary:logistic',\n",
    "    tree_method = 'exact',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'xgb', train_data_fill2,\n",
    "    n_estimators = 1998, \n",
    "    learning_rate = 0.030898693059763598, \n",
    "    max_depth = 8, \n",
    "    alpha = 0.0017554538174868774, \n",
    "    gamma = 0.0007257577447593802, \n",
    "    reg_alpha = 0.7581280398368035, \n",
    "    reg_lambda = 0.5872331353519633, \n",
    "    colsample_bytree = 0.56275606593282, \n",
    "    subsample = 0.8342870707789082,\n",
    "    objective = 'binary:logistic',\n",
    "    tree_method = 'exact',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'xgb', train_data,\n",
    "    n_estimators = 2287, \n",
    "    learning_rate = 0.046904208411195795, \n",
    "    max_depth = 8, \n",
    "    alpha = 1.9343531171735368e-05, \n",
    "    gamma = 0.002118564280859176, \n",
    "    reg_alpha = 0.6827713868263061, \n",
    "    reg_lambda = 0.05035980721174918, \n",
    "    colsample_bytree = 0.8959193125044248, \n",
    "    subsample = 0.43471952905681815,\n",
    "    objective = 'binary:logistic',  \n",
    "    tree_method = \"exact\", \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]\n",
    "\n",
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(probs, method='soft', threshold=0.24)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data0817_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dam = fit_all_train_data_function(\n",
    "    'cat', train_data_dam,\n",
    "    iterations=2083,\n",
    "    learning_rate=0.023925705983940986,\n",
    "    depth=11,\n",
    "    l2_leaf_reg=0.05919257514332274,\n",
    "    random_strength=7.259397831551647,\n",
    "    bagging_temperature=5.39094676652102,\n",
    "    border_count=234,\n",
    "    scale_pos_weight=1.776413991309166,\n",
    "    grow_policy='Lossguide',\n",
    "    random_seed=RANDOM_STATE,\n",
    "    eval_metric='F1',\n",
    "    logging_level='Silent',\n",
    "    boosting_type='Plain'\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'cat', train_data_autoclave,\n",
    "    iterations = 2786, \n",
    "    learning_rate = 0.016342560305036093, \n",
    "    depth = 8, \n",
    "    l2_leaf_reg = 3.7187150890684246, \n",
    "    random_strength = 0.13164684607188099, \n",
    "    bagging_temperature = 9.823498597792092, \n",
    "    border_count = 158, \n",
    "    scale_pos_weight = 1.8735070170496537,\n",
    "    grow_policy = 'SymmetricTree',\n",
    "    random_state = RANDOM_STATE,\n",
    "    eval_metric = 'F1',\n",
    "    logging_level = 'Silent',\n",
    "    boosting_type = 'Plain'\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'cat', train_data_fill1,\n",
    "    iterations=1489,\n",
    "    learning_rate=0.011481405951174946,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=0.12082259365361882,\n",
    "    random_strength=2.5111358694495056,\n",
    "    bagging_temperature=2.06264856742851,\n",
    "    border_count=331,\n",
    "    scale_pos_weight=2.3505422278535173,\n",
    "    grow_policy='Lossguide',\n",
    "    random_seed=RANDOM_STATE,\n",
    "    eval_metric='F1',\n",
    "    logging_level='Silent',\n",
    "    boosting_type='Plain'\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'cat', train_data_fill2,\n",
    "    iterations=481,\n",
    "    learning_rate=0.018742270357007457,\n",
    "    depth=5,\n",
    "    l2_leaf_reg=1.0871571324663387,\n",
    "    random_strength=3.49632241801363,\n",
    "    bagging_temperature=5.717049796462913,\n",
    "    border_count=183,\n",
    "    scale_pos_weight=3.4406776189795383,\n",
    "    grow_policy='SymmetricTree',\n",
    "    random_seed=RANDOM_STATE,\n",
    "    eval_metric='F1',\n",
    "    logging_level='Silent',\n",
    "    boosting_type='Plain'\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'cat', train_data,\n",
    "    iterations=2752,\n",
    "    learning_rate=0.01750223610140175,\n",
    "    depth=7,\n",
    "    l2_leaf_reg=1.0323106799772723,\n",
    "    random_strength=11.120538157516553,\n",
    "    bagging_temperature=9.844903580231264,\n",
    "    border_count=140,\n",
    "    scale_pos_weight=2.5890657068422374,\n",
    "    grow_policy='Depthwise',\n",
    "    random_seed=RANDOM_STATE,\n",
    "    eval_metric='F1',\n",
    "    logging_level='Silent',\n",
    "    boosting_type='Plain'\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]\n",
    "\n",
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(probs, method='soft', threshold=0.3)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data0817_cat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c6f69",
   "metadata": {},
   "source": [
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_Dam = fit_all_train_data_function(\n",
    "    'et', train_data_dam\n",
    "    , n_estimators = 2242\n",
    "    , max_depth = 32\n",
    "    , min_samples_split = 2\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'et', train_data_autoclave,\n",
    "    n_estimators = 2708,\n",
    "    max_depth = 41,\n",
    "    min_samples_split = 8,\n",
    "    min_samples_leaf = 1,\n",
    "    criterion = 'entropy',\n",
    "    bootstrap = False,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'et', train_data_fill1,\n",
    "    n_estimators = 1520,\n",
    "    max_depth = 30,\n",
    "    min_samples_split = 2,\n",
    "    min_samples_leaf = 1,\n",
    "    criterion = 'entropy',\n",
    "    bootstrap = False,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'et', train_data_fill2\n",
    "    , n_estimators = 1001\n",
    "    , max_depth = 45\n",
    "    , min_samples_split = 3\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'et', train_data\n",
    "    , n_estimators = 2884\n",
    "    , max_depth = 56\n",
    "    , min_samples_split = 3\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]\n",
    "\n",
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(probs, method='soft', threshold=0.3)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data0817_et.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec2d7c",
   "metadata": {},
   "source": [
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dam = fit_all_train_data_function(\n",
    "    'rf', train_data_dam\n",
    "    , n_estimators = 1330\n",
    "    , max_depth = 36\n",
    "    , min_samples_split = 6\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'rf', train_data_autoclave\n",
    "    , n_estimators = 1103\n",
    "    , max_depth = 36\n",
    "    , min_samples_split = 8\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'rf', train_data_fill1\n",
    "    , n_estimators = 1861\n",
    "    , max_depth = 91\n",
    "    , min_samples_split = 7\n",
    "    , min_samples_leaf = 5\n",
    "    , criterion = 'entropy'\n",
    "    , class_weight = 'balanced'\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'rf', train_data_fill2\n",
    "    , n_estimators = 2663\n",
    "    , max_depth = 100\n",
    "    , min_samples_split = 6\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , class_weight = 'balanced'\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'rf', train_data\n",
    "    , n_estimators = 1082\n",
    "    , max_depth = 54\n",
    "    , min_samples_split = 6\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , class_weight = 'balanced'\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]\n",
    "\n",
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(probs, method='soft', threshold=0.3)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data0817_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d80be",
   "metadata": {},
   "source": [
    "ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657cdfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=22,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=0.9449544624225188,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Dam = fit_all_train_data_function(\n",
    "    'ada', train_data_dam\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=439\n",
    "    , learning_rate=0.30985993372769294\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=13,\n",
    "    min_samples_split=34,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=0.7473309094470472,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'ada', train_data_autoclave\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=570\n",
    "    , learning_rate=0.2040105705276999\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=14,\n",
    "    min_samples_split=33,\n",
    "    min_samples_leaf=8,\n",
    "    max_features=0.7113128413756866,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'ada', train_data_fill1\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=913\n",
    "    , learning_rate=0.055237331816147595\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=7,\n",
    "    min_samples_split=13,\n",
    "    min_samples_leaf=8,\n",
    "    max_features=0.6266118401157937,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'ada', train_data_fill2\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=293\n",
    "    , learning_rate=0.620377973483163\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=6,\n",
    "    min_samples_split=28,\n",
    "    min_samples_leaf=7,\n",
    "    max_features=0.7331591188366589,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'ada', train_data\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=677\n",
    "    , learning_rate=0.6713565955468803\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]\n",
    "\n",
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(probs, method='soft', threshold=0.3)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data0817_ada.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c9c18",
   "metadata": {},
   "source": [
    "### Hard voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162c6a5",
   "metadata": {},
   "source": [
    "공정구분하여 학습한 개별 모델들에 대해서 hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def read_submission_files(file_paths):\n",
    "    \"\"\"\n",
    "    제출 파일을 읽어와서 예측 결과를 반환합니다.\n",
    "\n",
    "    Parameters:\n",
    "    file_paths (list of str): 제출 파일 경로 리스트\n",
    "\n",
    "    Returns:\n",
    "    list of np.array: 각 제출 파일의 예측 결과 리스트\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        preds = df['target'].apply(lambda x: 1 if x == 'AbNormal' else 0).values\n",
    "        predictions.append(preds)\n",
    "    return predictions\n",
    "\n",
    "def hard_voting(preds):\n",
    "    \"\"\"\n",
    "    하드 보팅을 사용하여 최종 예측을 수행합니다.\n",
    "\n",
    "    Parameters:\n",
    "    preds (list of np.array): 각 모델의 예측 배열 리스트\n",
    "\n",
    "    Returns:\n",
    "    np.array: 최종 예측 결과\n",
    "    \"\"\"\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    # 각 샘플의 예측 결과를 문자열로 변환하여 리스트에 저장\n",
    "    sample_predictions = [''.join(map(str, x)) for x in preds.T]\n",
    "    \n",
    "    # 각 예측 결과의 빈도수를 계산\n",
    "    prediction_counts = Counter(sample_predictions)\n",
    "    \n",
    "    # 빈도수 출력\n",
    "    for pred, count in prediction_counts.items():\n",
    "        print(f\"Prediction {pred}: {count} times\")\n",
    "    \n",
    "    # 하드 보팅을 통해 최종 예측을 계산\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=preds)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31e6e7",
   "metadata": {},
   "source": [
    "성능 좋은 3가지 모델(lgbm, xgb, cat)에 대해서 2번 넣어줌으로서 가중치를 주는 효과  \n",
    "종합적으로 [2,2,2,1,1,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 공통 경로\n",
    "common_path = \"./data/\"\n",
    "\n",
    "# 제출 파일 이름 리스트\n",
    "file_names = [\n",
    "    \"data0817_lgbm.csv\"\n",
    "    , \"data0817_lgbm.csv\"\n",
    "    , \"data0817_xgb.csv\"\n",
    "    , \"data0817_xgb.csv\"\n",
    "    , \"data0817_cat.csv\"\n",
    "    , \"data0817_cat.csv\"\n",
    "    , \"data0817_et.csv\"\n",
    "    , \"data0817_rf.csv\"\n",
    "    , \"data0817_ada.csv\"   \n",
    "    # 파일 추가 가능  <----- 파일 필요시 추가하세요!!\n",
    "]\n",
    "\n",
    "# 경로를 추가하는 함수\n",
    "def add_common_path(file_names, common_path):\n",
    "    return [common_path + file_name for file_name in file_names]\n",
    "\n",
    "# 경로가 추가된 파일 리스트\n",
    "file_paths = add_common_path(file_names, common_path)\n",
    "\n",
    "# 제출 파일에서 예측 결과 읽어오기\n",
    "predictions = read_submission_files(file_paths)\n",
    "\n",
    "# 하드 보팅 결과\n",
    "final_predictions_hard = hard_voting(predictions)\n",
    "\n",
    "# 결과를 새로운 제출 파일로 저장할 파일 이름\n",
    "output_file_name = \"submission.csv\" # <----- 파일 이름을 변경하세요!!\n",
    "\n",
    "# 결과를 새로운 제출 파일로 저장\n",
    "df_sub = pd.read_csv(file_paths[0])\n",
    "df_sub[\"target\"] = final_predictions_hard\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "df_sub.to_csv(output_file_name, index=False)\n",
    "\n",
    "print(f\"최종 제출 파일이 '{output_file_name}'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba51ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33325daa",
   "metadata": {},
   "source": [
    "우측 상단의 제출 버튼을 클릭해 결과를 확인하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a06d40",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
