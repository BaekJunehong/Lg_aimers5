{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017e9265",
   "metadata": {},
   "source": [
    "# 제품 이상여부 판별 프로젝트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab431",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8341e8",
   "metadata": {},
   "source": [
    "### 필수 라이브러리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5cd513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "def get_clf_eval(y_test, y_pred_proba, threshold=0.5):\n",
    "    # 확률을 기준으로 예측 레이블 생성\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)  # 0.5 이상의 확률을 양성으로 간주\n",
    "\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\", confusion)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03981606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d054e30",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0b4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.3\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "## --- 해당하는 데이터로 변경해주기!! --- ##\n",
    "train_data = pd.read_csv(\"C:/Users/KimDongyoung/Desktop/git_LGaimers5/Lg_aimers5/data/train_data_0817.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/KimDongyoung/Desktop/git_LGaimers5/Lg_aimers5/data/test_data_0817.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d13d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dam, fill1, fill2 공통 변수\n",
    "var_dam_fill = [\n",
    "    'Equipment_same_num',\n",
    "    'PalletID_Collect_Result_encoded',\n",
    "    'Production_Qty_Collect_Result',\n",
    "    'WorkMode Collect Result'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3803747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 공통 변수\n",
    "### correlation 확인을 위한 변수 리스트\n",
    "var_all_corr = [\n",
    "    'model_receip_encoded',\n",
    "    'workorder_receip_encoded'\n",
    "]\n",
    "\n",
    "### train\n",
    "var_all_train = [\n",
    "    'target',\n",
    "    'model_receip_encoded',\n",
    "    'workorder_receip_encoded'\n",
    "]\n",
    "\n",
    "### test\n",
    "var_all_test = [\n",
    "    'Set ID',\n",
    "    'target',\n",
    "    'model_receip_encoded',\n",
    "    'workorder_receip_encoded'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207e8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Dam'을 포함하는 변수 선택\n",
    "dam_variables = [var for var in train_data.columns if '_Dam' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + dam_variables\n",
    "train_data_dam = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + dam_variables\n",
    "test_data_dam = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c666baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill1'을 포함하는 변수 선택\n",
    "fill1_variables = [var for var in train_data.columns if '_Fill1' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill1_variables\n",
    "train_data_fill1 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill1_variables\n",
    "test_data_fill1 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f6c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill2'을 포함하는 변수 선택\n",
    "fill2_variables = [var for var in train_data.columns if '_Fill2' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill2_variables\n",
    "train_data_fill2 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill2_variables\n",
    "test_data_fill2 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f031d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_AutoClave'을 포함하는 변수 선택\n",
    "autoclave_variables = [var for var in train_data.columns if '_AutoClave' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_all_train + autoclave_variables\n",
    "train_data_autoclave = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_all_test + autoclave_variables\n",
    "test_data_autoclave = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45a10a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e5b59",
   "metadata": {},
   "source": [
    "## 3. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e56224",
   "metadata": {},
   "source": [
    "### optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc134a0",
   "metadata": {},
   "source": [
    "1. lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a75b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objectiveLGBM_dart(trial, x_tr, y_tr, x_val, y_val):\n",
    "    \n",
    "#     # 'Normal'과 'AbNormal'을 숫자로 변환\n",
    "#     y_tr = y_tr.map({'Normal': 0, 'AbNormal': 1})\n",
    "#     y_val = y_val.map({'Normal': 0, 'AbNormal': 1})\n",
    "    \n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 500, 3000),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 10, 300),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 3, 300),\n",
    "#         'boosting_type': 'dart', \n",
    "#         'random_state': RANDOM_STATE,\n",
    "#         'verbose': -1\n",
    "#     }\n",
    "       \n",
    "#     model = LGBMClassifier(**param)\n",
    "#     model.fit(x_tr, y_tr)\n",
    "#     pred_proba = model.predict_proba(x_val)[:, 1]  # 양성 클래스 확률\n",
    "#     pred = (pred_proba >= THRESHOLD).astype(int)  # 스레드홀드에 따른 예측\n",
    "    \n",
    "#     score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# # 데이터셋 분할\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     train_data.drop(\"target\", axis=1), # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     train_data[\"target\"],              # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "\n",
    "# # 하이퍼 파라미터 튜닝\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "# study.optimize(lambda trial: objectiveLGBM_dart(trial, x_train, y_train, x_val, y_val), n_trials=300)\n",
    "\n",
    "# print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa857a77",
   "metadata": {},
   "source": [
    "2. xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab290efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objectiveXGB(trial, x_tr, y_tr, x_val, y_val):\n",
    "    \n",
    "#     # 'Normal'과 'AbNormal'을 숫자로 변환\n",
    "#     y_tr = y_tr.map({'Normal': 0, 'AbNormal': 1})\n",
    "#     y_val = y_val.map({'Normal': 0, 'AbNormal': 1})\n",
    "\n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 1200, 3200)\n",
    "#         , 'learning_rate': trial.suggest_float('learning_rate', 0.002, 0.2)\n",
    "#         , 'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "\n",
    "#         , 'alpha': trial.suggest_float('alpha', 0.00001, 0.01, log=True)\n",
    "#         , 'gamma': trial.suggest_float('gamma', 0.00001, 0.01, log=True)\n",
    "\n",
    "#         , 'reg_alpha' : trial.suggest_float('reg_alpha', 0.01, 1)\n",
    "#         , 'reg_lambda' : trial.suggest_float('reg_lambda', 0.01, 1)\n",
    "        \n",
    "#         , 'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.01, 1)\n",
    "#         , 'subsample' : trial.suggest_float('subsample', 0.03, 1)\n",
    "#         , 'objective': 'binary:logistic'  \n",
    "#         , 'tree_method' : \"exact\"        \n",
    "#         , 'random_state': RANDOM_STATE\n",
    "#     }\n",
    "       \n",
    "#     model = XGBClassifier(**param)\n",
    "#     model.fit(x_tr, y_tr)\n",
    "#     pred_proba = model.predict_proba(x_val)[:, 1]  # 양성 클래스 확률\n",
    "#     pred = (pred_proba >= THRESHOLD).astype(int)  # 스레드홀드에 따른 예측\n",
    "    \n",
    "#     score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# # 데이터셋 분할\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     train_data.drop(\"target\", axis=1),  # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     train_data[\"target\"],               # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "\n",
    "# # 하이퍼 파라미터 튜닝\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "# study.optimize(lambda trial: objectiveXGB(trial, x_train, y_train, x_val, y_val), n_trials=200)\n",
    "\n",
    "# print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef424b5",
   "metadata": {},
   "source": [
    "3. catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f9a82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objectiveCatBoost(trial, x_tr, y_tr, x_val, y_val):\n",
    "    \n",
    "#     # 'Normal'과 'AbNormal'을 숫자로 변환\n",
    "#     y_tr = y_tr.map({'Normal': 0, 'AbNormal': 1})\n",
    "#     y_val = y_val.map({'Normal': 0, 'AbNormal': 1})\n",
    "    \n",
    "#     param = {\n",
    "#         'iterations': trial.suggest_int('iterations', 400, 1500),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.4),\n",
    "#         'depth': trial.suggest_int('depth', 3, 14),\n",
    "#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.05, 5.0, log=True),\n",
    "#         'random_strength': trial.suggest_float('random_strength', 0.1, 10.0),\n",
    "#         'bagging_temperature': trial.suggest_float('bagging_temperature', 0.1, 10.0),\n",
    "#         'border_count': trial.suggest_int('border_count', 70, 270),\n",
    "#         'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.5, 1.2),\n",
    "\n",
    "#         'random_seed': RANDOM_STATE,\n",
    "#         'eval_metric': 'F1',\n",
    "#         'logging_level': 'Silent',\n",
    "#         'boosting_type': 'Plain'\n",
    "#     }\n",
    "\n",
    "#     model = CatBoostClassifier(**param)\n",
    "#     model.fit(x_tr, y_tr)\n",
    "#     pred_proba = model.predict_proba(x_val)[:, 1]  # 양성 클래스 확률\n",
    "#     pred = (pred_proba >= THRESHOLD).astype(int)  # 스레드홀드에 따른 예측\n",
    "    \n",
    "#     score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# # 데이터셋 분할\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     train_data.drop(\"target\", axis=1),  # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     train_data[\"target\"],               # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "\n",
    "# # 하이퍼 파라미터 튜닝\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "# study.optimize(lambda trial: objectiveCatBoost(trial, x_train, y_train, x_val, y_val), n_trials=100)\n",
    "\n",
    "# print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b26796",
   "metadata": {},
   "source": [
    "4. randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6a92217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objectiveRandomForestClassifier(trial, x_tr, y_tr, x_val, y_val):\n",
    "    \n",
    "#     # 'Normal'과 'AbNormal'을 숫자로 변환\n",
    "#     y_tr = y_tr.map({'Normal': 0, 'AbNormal': 1})\n",
    "#     y_val = y_val.map({'Normal': 0, 'AbNormal': 1})\n",
    "    \n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 1000, 2500)\n",
    "#         , 'max_depth': trial.suggest_int('max_depth', 2, 50)\n",
    "#         , 'min_samples_split': trial.suggest_int('min_samples_split', 2, 12)\n",
    "#         , 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 7)\n",
    "#         , 'criterion': trial.suggest_categorical('criterion', ['entropy'])\n",
    "#         , 'bootstrap': trial.suggest_categorical('bootstrap', [False])\n",
    "#         , 'random_state': RANDOM_STATE\n",
    "#     }\n",
    "    \n",
    "#     model = RandomForestClassifier(**param)\n",
    "#     model.fit(x_tr, y_tr)\n",
    "#     pred = model.predict(x_val)\n",
    "#     score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# # 데이터셋 분할\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     train_data.drop(\"target\", axis=1),  # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     train_data[\"target\"],               # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "\n",
    "# # 하이퍼 파라미터 튜닝\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "# study.optimize(lambda trial: objectiveRandomForestClassifier(trial, x_train, y_train, x_val, y_val), n_trials=200)\n",
    "\n",
    "# print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04b824",
   "metadata": {},
   "source": [
    "5. extra tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c80be75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def objectiveExtraTreesClassifier(trial, x_tr, y_tr, x_val, y_val):\n",
    "    \n",
    "#     # 'Normal'과 'AbNormal'을 숫자로 변환\n",
    "#     y_tr = y_tr.map({'Normal': 0, 'AbNormal': 1})\n",
    "#     y_val = y_val.map({'Normal': 0, 'AbNormal': 1})\n",
    "    \n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 1000, 2500)\n",
    "#         , 'max_depth': trial.suggest_int('max_depth', 2, 50)\n",
    "#         , 'min_samples_split': trial.suggest_int('min_samples_split', 2, 12)\n",
    "#         , 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 7)\n",
    "#         , 'criterion': trial.suggest_categorical('criterion', ['entropy'])\n",
    "#         , 'bootstrap': trial.suggest_categorical('bootstrap', [False])\n",
    "#         , 'random_state': RANDOM_STATE\n",
    "#     }\n",
    "    \n",
    "#     model = ExtraTreesClassifier(**param)\n",
    "#     model.fit(x_tr, y_tr)\n",
    "#     pred = model.predict(x_val)\n",
    "#     score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# # 데이터셋 분할\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     train_data.drop(\"target\", axis=1),  # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     train_data[\"target\"],               # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "\n",
    "# # 하이퍼 파라미터 튜닝\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "# study.optimize(lambda trial: objectiveExtraTreesClassifier(trial, x_train, y_train, x_val, y_val), n_trials=400)\n",
    "\n",
    "# print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238c03b",
   "metadata": {},
   "source": [
    "6. adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ca6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objectiveAdaBoost(trial, x_tr, y_tr, x_val, y_val):\n",
    "    \n",
    "#     # 'Normal'과 'AbNormal'을 숫자로 변환\n",
    "#     y_tr = y_tr.map({'Normal': 0, 'AbNormal': 1})\n",
    "#     y_val = y_val.map({'Normal': 0, 'AbNormal': 1})\n",
    "\n",
    "#     # Base Estimator 설정\n",
    "#     base_estimator = DecisionTreeClassifier(\n",
    "#         max_depth=trial.suggest_int('max_depth', 1, 10),\n",
    "#         min_samples_split=trial.suggest_int('min_samples_split', 2, 20),\n",
    "#         min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "#         max_features=trial.suggest_float('max_features', 0.1, 1.0),\n",
    "#         random_state=RANDOM_STATE  \n",
    "#     )\n",
    "\n",
    "#     # AdaBoost 모델 설정\n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "#         'base_estimator': base_estimator,\n",
    "#         'random_state': RANDOM_STATE  \n",
    "#     }\n",
    "    \n",
    "#     model = AdaBoostClassifier(**param)\n",
    "#     model.fit(x_tr, y_tr)\n",
    "#     pred = model.predict(x_val)\n",
    "#     score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# # 데이터셋 분할\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     train_data.drop(\"target\", axis=1),  # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     train_data[\"target\"],               # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "\n",
    "# # 하이퍼 파라미터 튜닝\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "# study.optimize(lambda trial: objectiveAdaBoost(trial, x_train, y_train, x_val, y_val), n_trials=100)\n",
    "\n",
    "# print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1e924",
   "metadata": {},
   "source": [
    "7. Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92d88693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-23 23:31:55,826] A new study created in memory with name: no-name-3285e6d6-b36d-4654-8c95-973008d08ad9\n",
      "[I 2024-08-23 23:41:32,406] Trial 0 finished with value: 0.23347398030942335 and parameters: {'n_estimators': 790, 'max_depth': 199, 'learning_rate': 0.038183309313687845, 'min_samples_split': 185, 'min_samples_leaf': 68}. Best is trial 0 with value: 0.23347398030942335.\n",
      "[I 2024-08-24 00:13:08,014] Trial 1 finished with value: 0.2344213649851632 and parameters: {'n_estimators': 2206, 'max_depth': 287, 'learning_rate': 0.06408507099977911, 'min_samples_split': 155, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.2344213649851632.\n",
      "[I 2024-08-24 00:33:18,447] Trial 2 finished with value: 0.21654135338345865 and parameters: {'n_estimators': 1469, 'max_depth': 89, 'learning_rate': 0.014404705911514527, 'min_samples_split': 5, 'min_samples_leaf': 42}. Best is trial 1 with value: 0.2344213649851632.\n",
      "[I 2024-08-24 00:38:23,156] Trial 3 finished with value: 0.22058823529411764 and parameters: {'n_estimators': 1731, 'max_depth': 9, 'learning_rate': 0.05639149688396562, 'min_samples_split': 25, 'min_samples_leaf': 41}. Best is trial 1 with value: 0.2344213649851632.\n",
      "[I 2024-08-24 00:58:56,143] Trial 4 finished with value: 0.2314540059347181 and parameters: {'n_estimators': 1285, 'max_depth': 232, 'learning_rate': 0.06050095819207001, 'min_samples_split': 126, 'min_samples_leaf': 34}. Best is trial 1 with value: 0.2344213649851632.\n",
      "[I 2024-08-24 01:03:34,221] Trial 5 finished with value: 0.21758569299552907 and parameters: {'n_estimators': 562, 'max_depth': 29, 'learning_rate': 0.08555435121723459, 'min_samples_split': 148, 'min_samples_leaf': 25}. Best is trial 1 with value: 0.2344213649851632.\n",
      "[I 2024-08-24 01:40:38,208] Trial 6 finished with value: 0.23076923076923078 and parameters: {'n_estimators': 2403, 'max_depth': 163, 'learning_rate': 0.03917592676400414, 'min_samples_split': 194, 'min_samples_leaf': 18}. Best is trial 1 with value: 0.2344213649851632.\n",
      "[I 2024-08-24 01:50:53,757] Trial 7 finished with value: 0.21553090332805072 and parameters: {'n_estimators': 2273, 'max_depth': 14, 'learning_rate': 0.06039260026994052, 'min_samples_split': 62, 'min_samples_leaf': 25}. Best is trial 1 with value: 0.2344213649851632.\n",
      "[I 2024-08-24 02:12:40,423] Trial 8 finished with value: 0.21597633136094674 and parameters: {'n_estimators': 1498, 'max_depth': 215, 'learning_rate': 0.07945176670763587, 'min_samples_split': 269, 'min_samples_leaf': 23}. Best is trial 1 with value: 0.2344213649851632.\n",
      "[I 2024-08-24 02:17:11,131] Trial 9 finished with value: 0.23744292237442927 and parameters: {'n_estimators': 532, 'max_depth': 229, 'learning_rate': 0.019083553963246288, 'min_samples_split': 239, 'min_samples_leaf': 96}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 02:21:26,741] Trial 10 finished with value: 0.17087378640776701 and parameters: {'n_estimators': 1012, 'max_depth': 299, 'learning_rate': 0.0012050976444510988, 'min_samples_split': 296, 'min_samples_leaf': 99}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 02:45:55,748] Trial 11 finished with value: 0.23262411347517734 and parameters: {'n_estimators': 2006, 'max_depth': 299, 'learning_rate': 0.02262894354049991, 'min_samples_split': 236, 'min_samples_leaf': 67}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 03:07:04,677] Trial 12 finished with value: 0.2146189735614308 and parameters: {'n_estimators': 2853, 'max_depth': 253, 'learning_rate': 0.0715550206920303, 'min_samples_split': 99, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 03:39:55,562] Trial 13 finished with value: 0.22190611664295876 and parameters: {'n_estimators': 2922, 'max_depth': 262, 'learning_rate': 0.038670561050366245, 'min_samples_split': 220, 'min_samples_leaf': 90}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 04:15:56,445] Trial 14 finished with value: 0.21732745961820849 and parameters: {'n_estimators': 2516, 'max_depth': 171, 'learning_rate': 0.026805635264272057, 'min_samples_split': 179, 'min_samples_leaf': 61}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 04:41:21,607] Trial 15 finished with value: 0.22507122507122507 and parameters: {'n_estimators': 2036, 'max_depth': 90, 'learning_rate': 0.09581664060225872, 'min_samples_split': 248, 'min_samples_leaf': 83}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 04:55:30,949] Trial 16 finished with value: 0.22255192878338279 and parameters: {'n_estimators': 1164, 'max_depth': 263, 'learning_rate': 0.00477599733828999, 'min_samples_split': 96, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 05:22:52,350] Trial 17 finished with value: 0.22507122507122507 and parameters: {'n_estimators': 1928, 'max_depth': 119, 'learning_rate': 0.04853573519979007, 'min_samples_split': 296, 'min_samples_leaf': 55}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 05:57:27,801] Trial 18 finished with value: 0.21605839416058395 and parameters: {'n_estimators': 2639, 'max_depth': 192, 'learning_rate': 0.06862905612935215, 'min_samples_split': 152, 'min_samples_leaf': 78}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 06:04:44,774] Trial 19 finished with value: 0.21961932650073204 and parameters: {'n_estimators': 500, 'max_depth': 237, 'learning_rate': 0.048282421171586466, 'min_samples_split': 260, 'min_samples_leaf': 12}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 06:24:27,883] Trial 20 finished with value: 0.23347398030942335 and parameters: {'n_estimators': 1711, 'max_depth': 281, 'learning_rate': 0.028871669095188523, 'min_samples_split': 206, 'min_samples_leaf': 100}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 06:35:27,945] Trial 21 finished with value: 0.22031473533619458 and parameters: {'n_estimators': 826, 'max_depth': 205, 'learning_rate': 0.03712667053554418, 'min_samples_split': 169, 'min_samples_leaf': 73}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 06:43:29,792] Trial 22 finished with value: 0.23008849557522124 and parameters: {'n_estimators': 789, 'max_depth': 134, 'learning_rate': 0.013969406440707712, 'min_samples_split': 218, 'min_samples_leaf': 88}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 06:53:03,773] Trial 23 finished with value: 0.22189349112426032 and parameters: {'n_estimators': 746, 'max_depth': 188, 'learning_rate': 0.015436068588412538, 'min_samples_split': 120, 'min_samples_leaf': 49}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 07:07:13,934] Trial 24 finished with value: 0.23142857142857143 and parameters: {'n_estimators': 1044, 'max_depth': 226, 'learning_rate': 0.033096384456226424, 'min_samples_split': 188, 'min_samples_leaf': 69}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 07:42:01,657] Trial 25 finished with value: 0.23248882265275708 and parameters: {'n_estimators': 2223, 'max_depth': 253, 'learning_rate': 0.0432122326919141, 'min_samples_split': 139, 'min_samples_leaf': 57}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 07:50:15,677] Trial 26 finished with value: 0.22503516174402252 and parameters: {'n_estimators': 683, 'max_depth': 271, 'learning_rate': 0.0535509470157729, 'min_samples_split': 168, 'min_samples_leaf': 91}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 08:01:13,726] Trial 27 finished with value: 0.23076923076923078 and parameters: {'n_estimators': 927, 'max_depth': 184, 'learning_rate': 0.022284243200629817, 'min_samples_split': 238, 'min_samples_leaf': 78}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 08:20:48,625] Trial 28 finished with value: 0.21875 and parameters: {'n_estimators': 1424, 'max_depth': 143, 'learning_rate': 0.07556359280751118, 'min_samples_split': 272, 'min_samples_leaf': 64}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 08:40:49,588] Trial 29 finished with value: 0.22580645161290322 and parameters: {'n_estimators': 1541, 'max_depth': 210, 'learning_rate': 0.010542039086334321, 'min_samples_split': 216, 'min_samples_leaf': 46}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 08:51:40,118] Trial 30 finished with value: 0.22692889561270801 and parameters: {'n_estimators': 626, 'max_depth': 240, 'learning_rate': 0.06621277323487762, 'min_samples_split': 59, 'min_samples_leaf': 40}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 09:09:30,399] Trial 31 finished with value: 0.23268698060941828 and parameters: {'n_estimators': 1770, 'max_depth': 283, 'learning_rate': 0.02922149884541353, 'min_samples_split': 199, 'min_samples_leaf': 98}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 09:25:18,130] Trial 32 finished with value: 0.22503516174402252 and parameters: {'n_estimators': 1725, 'max_depth': 282, 'learning_rate': 0.01887488281249572, 'min_samples_split': 204, 'min_samples_leaf': 100}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[I 2024-08-24 09:37:21,112] Trial 33 finished with value: 0.23066104078762306 and parameters: {'n_estimators': 1252, 'max_depth': 278, 'learning_rate': 0.03226753290698582, 'min_samples_split': 161, 'min_samples_leaf': 92}. Best is trial 9 with value: 0.23744292237442927.\n",
      "[W 2024-08-24 09:55:22,946] Trial 34 failed with parameters: {'n_estimators': 2181, 'max_depth': 239, 'learning_rate': 0.04611640919676818, 'min_samples_split': 230, 'min_samples_leaf': 35} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\KimDongyoung\\AppData\\Local\\Temp\\ipykernel_15204\\1814564895.py\", line 36, in <lambda>\n",
      "    study.optimize(lambda trial: objectiveGBM(trial, x_train, y_train, x_val, y_val), n_trials=300)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\KimDongyoung\\AppData\\Local\\Temp\\ipykernel_15204\\1814564895.py\", line 17, in objectiveGBM\n",
      "    model.fit(x_tr, y_tr)\n",
      "  File \"c:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 532, in fit\n",
      "    n_stages = self._fit_stages(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 610, in _fit_stages\n",
      "    raw_predictions = self._fit_stage(\n",
      "                      ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 245, in _fit_stage\n",
      "    tree.fit(X, residual, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1320, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "KeyboardInterrupt\n",
      "[W 2024-08-24 09:55:22,964] Trial 34 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 하이퍼 파라미터 튜닝\u001b[39;00m\n\u001b[0;32m     35\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mRANDOM_STATE))\n\u001b[1;32m---> 36\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectiveGBM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial: score \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mparams \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams))\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[16], line 36\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 하이퍼 파라미터 튜닝\u001b[39;00m\n\u001b[0;32m     35\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mRANDOM_STATE))\n\u001b[1;32m---> 36\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjectiveGBM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial: score \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mparams \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams))\n",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m, in \u001b[0;36mobjectiveGBM\u001b[1;34m(trial, x_tr, y_tr, x_val, y_val)\u001b[0m\n\u001b[0;32m      7\u001b[0m param \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m3000\u001b[39m),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m300\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: RANDOM_STATE\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(x_val)[:, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# 양성 클래스 확률\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m (pred_proba \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m THRESHOLD)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# 스레드홀드에 따른 예측\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    607\u001b[0m         )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    250\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    258\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KimDongyoung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objectiveGBM(trial, x_tr, y_tr, x_val, y_val):\n",
    "    \n",
    "    # 'Normal'과 'AbNormal'을 숫자로 변환\n",
    "    y_tr = y_tr.map({'Normal': 0, 'AbNormal': 1})\n",
    "    y_val = y_val.map({'Normal': 0, 'AbNormal': 1})\n",
    "    \n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 300),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "       \n",
    "    model = GradientBoostingClassifier(**param)\n",
    "    model.fit(x_tr, y_tr)\n",
    "    pred_proba = model.predict_proba(x_val)[:, 1]  # 양성 클래스 확률\n",
    "    pred = (pred_proba >= THRESHOLD).astype(int)  # 스레드홀드에 따른 예측\n",
    "    \n",
    "    score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "# 데이터셋 분할\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    train_data.drop(\"target\", axis=1), # <--- 해당하는 데이터로 변경해주기!!\n",
    "    train_data[\"target\"],              # <--- 해당하는 데이터로 변경해주기!!\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# 하이퍼 파라미터 튜닝\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "study.optimize(lambda trial: objectiveGBM(trial, x_train, y_train, x_val, y_val), n_trials=300)\n",
    "\n",
    "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2b124",
   "metadata": {},
   "source": [
    "Trial 9 finished with value: 0.23744292237442927 and parameters: {'n_estimators': 532, 'max_depth': 229, 'learning_rate': 0.019083553963246288, 'min_samples_split': 239, 'min_samples_leaf': 96}. Best is trial 9 with value: 0.23744292237442927."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578b187",
   "metadata": {},
   "source": [
    "8. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objectiveDecisionTree(trial, x_tr, y_tr, x_val, y_val):\n",
    "    \n",
    "#     # 'Normal'과 'AbNormal'을 숫자로 변환\n",
    "#     y_tr = y_tr.map({'Normal': 0, 'AbNormal': 1})\n",
    "#     y_val = y_val.map({'Normal': 0, 'AbNormal': 1})\n",
    "    \n",
    "#     param = {\n",
    "#         'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "#         'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]),\n",
    "#         'random_state': RANDOM_STATE\n",
    "#     }\n",
    "       \n",
    "#     model = DecisionTreeClassifier(**param)\n",
    "#     model.fit(x_tr, y_tr)\n",
    "#     pred_proba = model.predict_proba(x_val)[:, 1]  # 양성 클래스 확률\n",
    "#     pred = (pred_proba >= THRESHOLD).astype(int)  # 스레드홀드에 따른 예측\n",
    "    \n",
    "#     score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# # 데이터셋 분할\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     train_data.drop(\"target\", axis=1), # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     train_data[\"target\"],              # <--- 해당하는 데이터로 변경해주기!!\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "\n",
    "# # 하이퍼 파라미터 튜닝\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "# study.optimize(lambda trial: objectiveDecisionTree(trial, x_train, y_train, x_val, y_val), n_trials=300)\n",
    "\n",
    "# print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
