{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2340621",
   "metadata": {},
   "source": [
    "# 제품 이상여부 판별 프로젝트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "id": "a199baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "id": "2463633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(\"./final_data/train.csv\")\n",
    "test_data = pd.read_csv(\"./final_data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003e2fc",
   "metadata": {},
   "source": [
    "### 0. 결측값 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "id": "fcf17763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def shift_row_values(row, start_col_index, move_limit, total_columns):\n",
    "#     move_count = 0  # 이동 카운터 초기화\n",
    "#     for col_index in range(start_col_index, total_columns):  # 모든 열을 대상으로\n",
    "#         if pd.isna(row[col_index]) or row[col_index] == \"OK\":  # 빈값 또는 \"OK\" 확인\n",
    "#             # 빈값 또는 \"OK\"가 발견되면 현재 위치부터 이후 3칸 간격의 변수 값을 앞으로 이동\n",
    "#             for shift_index in range(col_index, total_columns - 3, 3):  # 3칸씩 이동\n",
    "#                 # 값을 이동\n",
    "#                 row[shift_index] = row[shift_index + 3]\n",
    "#                 row[shift_index + 3] = None  # 원래 자리 비우기\n",
    "#                 move_count += 1  # 이동 카운트 증가\n",
    "\n",
    "#                 if move_count >= move_limit:  # 설정된 횟수에 도달하면 중지\n",
    "#                     break\n",
    "#         if move_count >= move_limit:  # 외부 루프에서도 체크\n",
    "#             break\n",
    "#     return row\n",
    "\n",
    "# def shift_values(data, start_col_index, move_limit):\n",
    "#     total_columns = data.shape[1]\n",
    "#     data = data.apply(shift_row_values, axis=1, args=(start_col_index, move_limit, total_columns))\n",
    "#     return data\n",
    "\n",
    "# # 변수 이름 설정 및 시작 열 인덱스 및 이동 횟수 설정\n",
    "# variables_with_limits = [\n",
    "#     ('HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 52),\n",
    "#     ('HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 22),\n",
    "#     ('HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2', 22)\n",
    "# ]\n",
    "\n",
    "# # 각 변수에 대해 함수 호출\n",
    "# def process_data(data, variables_with_limits, output_file):\n",
    "#     for start_var, move_limit in variables_with_limits:\n",
    "#         start_col_index = data.columns.get_loc(start_var)  # 각 변수의 시작 열 인덱스 찾기\n",
    "#         data = shift_values(data, start_col_index, move_limit)\n",
    "#     data.to_csv(output_file, index=False)\n",
    "#     print(f'데이터가 성공적으로 수정되고 저장되었습니다: {output_file}')\n",
    "\n",
    "# # 데이터 처리\n",
    "# process_data(train_data, variables_with_limits, './data/clean_train_data.csv')\n",
    "# process_data(test_data, variables_with_limits, './data/clean_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "670da700",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(\"./final_data/clean_final_train.csv\")\n",
    "test_data = pd.read_csv(\"./final_data/clean_final_test.csv\")\n",
    "plus_data = pd.read_excel('./final_data/hand_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6f25e-68c6-45b2-8888-d762a4199bb5",
   "metadata": {},
   "source": [
    "### 1. 기본 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "id": "ef282a8f-1717-4582-9abb-b064b243bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 열을 임시로 분리\n",
    "target_train = train_data['target']\n",
    "target_test = test_data['target']\n",
    "\n",
    "# 모든 값이 NaN인 열 제거\n",
    "train_data = train_data.dropna(axis=1, how='all')\n",
    "test_data = test_data.dropna(axis=1, how='all')\n",
    "\n",
    "# target 열을 다시 결합\n",
    "train_data['target'] = target_train\n",
    "test_data['target'] = target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "8f245f17-b82f-4406-8f9d-cccf488bf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wip Line 열 제거\n",
    "wip_line_columns = train_data.filter(like='Wip Line').columns\n",
    "\n",
    "train_data.drop(columns=wip_line_columns, inplace=True)\n",
    "test_data.drop(columns=wip_line_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "id": "25a4af25-3399-4762-be4e-7c77d67e677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Desc 열 제거\n",
    "Process_Desc_col = train_data.filter(like='Process Desc').columns\n",
    "\n",
    "train_data.drop(columns=Process_Desc_col, inplace=True)\n",
    "test_data.drop(columns=Process_Desc_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "c75a3e26-5efa-420e-998f-b0df0040df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insp. Seq No 열 제거\n",
    "Insp_Seq_No_col = train_data.filter(like='Insp. Seq No').columns\n",
    "\n",
    "train_data.drop(columns=Insp_Seq_No_col, inplace=True)\n",
    "test_data.drop(columns=Insp_Seq_No_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "f2a9ca83-eca1-4a22-b0c9-027f1960239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insp Judge Code 열 제거\n",
    "Insp_Judge_Code_col = train_data.filter(like='Insp Judge Code').columns\n",
    "\n",
    "train_data.drop(columns=Insp_Judge_Code_col, inplace=True)\n",
    "test_data.drop(columns=Insp_Judge_Code_col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e2803-88fe-4d64-9f9d-fa60b6bb5299",
   "metadata": {},
   "source": [
    "### 2. 제품 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e33e3-9684-4c66-b33a-db13c3ca21e0",
   "metadata": {},
   "source": [
    "- receip no, workorder, model.suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b486615",
   "metadata": {},
   "source": [
    "Receip_No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "ba667da8-fe3c-4eb2-87de-bd10d7300815",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Receip_No\n",
    "# 파생변수 생성: Receip_No 3개의 컬럼 값이 모두 동일하면 해당 값을 저장, 아니면 diff\n",
    "train_data['Receip_No'] = train_data.apply(\n",
    "    lambda row: row['Receip No Collect Result_Dam'] if (row['Receip No Collect Result_Dam'] == row['Receip No Collect Result_Fill1'] == row['Receip No Collect Result_Fill2']) else 'diff',\n",
    "    axis=1\n",
    ")\n",
    "test_data['Receip_No'] = test_data.apply(\n",
    "    lambda row: row['Receip No Collect Result_Dam'] if (row['Receip No Collect Result_Dam'] == row['Receip No Collect Result_Fill1'] == row['Receip No Collect Result_Fill2']) else 'diff',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# receip_no 열을 object 타입으로 변경\n",
    "train_data['Receip_No'] = train_data['Receip_No'].astype('object')\n",
    "test_data['Receip_No'] = test_data['Receip_No'].astype('object')\n",
    "\n",
    "# 필요없는 변수 삭제\n",
    "train_data = train_data.drop(columns=['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'])\n",
    "test_data = test_data.drop(columns=['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4a0af",
   "metadata": {},
   "source": [
    "workorder_receip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "4d83525c-04c8-4dd7-a62c-28c70c182f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### workorder_receip\n",
    "# Workorder -뒤의 번호 구분을 제거\n",
    "train_data['cleaned_workorder'] = train_data['Workorder_Dam'].str.split('-').str[0]\n",
    "test_data['cleaned_workorder'] = test_data['Workorder_Dam'].str.split('-').str[0]\n",
    "\n",
    "# 필요없는 변수 삭제\n",
    "train_data = train_data.drop(columns=['Workorder_Dam', 'Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2'])\n",
    "test_data = test_data.drop(columns=['Workorder_Dam', 'Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63fec6",
   "metadata": {},
   "source": [
    "model_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "id": "408f56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### model_suffix\n",
    "# 열 이름 변경\n",
    "train_data.rename(columns={'Model.Suffix_Dam': 'model_suffix'}, inplace=True)\n",
    "test_data.rename(columns={'Model.Suffix_Dam': 'model_suffix'}, inplace=True)\n",
    "\n",
    "# 필요없는 변수 삭제\n",
    "train_data = train_data.drop(columns=['Model.Suffix_AutoClave', 'Model.Suffix_Fill1', 'Model.Suffix_Fill2'])\n",
    "test_data = test_data.drop(columns=['Model.Suffix_AutoClave', 'Model.Suffix_Fill1', 'Model.Suffix_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "id": "cbd255c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_suffix 값이 'AJX75334503'인 경우 1, 아닌 경우 0을 부여하는 파생변수 생성\n",
    "train_data['model_suffix_03'] = train_data['model_suffix'].apply(lambda x: 1 if x == 'AJX75334503' else 0)\n",
    "test_data['model_suffix_03'] = test_data['model_suffix'].apply(lambda x: 1 if x == 'AJX75334503' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "id": "be2975ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receip_No가 3이고 model_suffix_03이 1인(model_suffix가 3번) 경우 \n",
    "# 1을 부여하고 아닌 경우 0을 부여하는 파생변수 생성\n",
    "train_data['Receip_n_suffix_3'] = train_data.apply(lambda row: 1 if row['Receip_No'] == 3 and row['model_suffix_03'] == 1 else 0, axis=1)\n",
    "test_data['Receip_n_suffix_3'] = test_data.apply(lambda row: 1 if row['Receip_No'] == 3 and row['model_suffix_03'] == 1 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191565e",
   "metadata": {},
   "source": [
    "model_suffix_03, Receip_No 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "22d8f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns=['model_suffix_03', 'Receip_No'])\n",
    "test_data = test_data.drop(columns=['model_suffix_03', 'Receip_No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c6ca4-45bd-4c39-8af6-7fd0d61e103c",
   "metadata": {},
   "source": [
    "### 3. 공통 변수 (dam, fill1, fill2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4651eb9-eb1a-44a4-8979-2a38a8b9c532",
   "metadata": {},
   "source": [
    "- workmode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "6a33f774-81d7-4716-9176-79eaee466ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorkMode Collect Result_Dam의 이름을 WorkMode Collect Result로 변경\n",
    "train_data = train_data.rename(columns={'WorkMode Collect Result_Dam': 'WorkMode Collect Result'})\n",
    "test_data = test_data.rename(columns={'WorkMode Collect Result_Dam': 'WorkMode Collect Result'})\n",
    "\n",
    "# WorkMode Collect Result_Fill1, WorkMode Collect Result_Fill2 열 드롭\n",
    "train_data = train_data.drop(columns=['WorkMode Collect Result_Fill1', 'WorkMode Collect Result_Fill2'])\n",
    "test_data = test_data.drop(columns=['WorkMode Collect Result_Fill1', 'WorkMode Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "58781314-726c-42d1-a34f-cc12ce092852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorkMode Collect Result 열의 값이 7인 행을 1로 변경\n",
    "train_data['WorkMode Collect Result'] = train_data['WorkMode Collect Result'].replace(7, 1)\n",
    "test_data['WorkMode Collect Result'] = test_data['WorkMode Collect Result'].replace(7, 1)\n",
    "\n",
    "# WorkMode Collect Result 열의 결측값을 0으로 채움\n",
    "train_data['WorkMode Collect Result'] = train_data['WorkMode Collect Result'].fillna(0)\n",
    "test_data['WorkMode Collect Result'] = test_data['WorkMode Collect Result'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f80682-b12e-4f8a-8ff8-39423a35e2d7",
   "metadata": {},
   "source": [
    "- equipment\n",
    "<br>(dispenser1 & dispenser2 변수를 만들 경우 다른 변수들에 의해 이미 설명이 되는 변수라 상관계수가 너무 높아서 제거하게 됨. 따라서 equipment가 같은지만 판단하는 파생변수 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "id": "8140b28a-2781-4356-9fe5-ac8d50beb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment로 시작하는 열 필터링\n",
    "Equipment_col = train_data.filter(like='Equipment').columns\n",
    "Equipment_col2 = test_data.filter(like='Equipment').columns\n",
    "\n",
    "new_train = train_data.filter(items=Equipment_col)\n",
    "new_test = test_data.filter(items=Equipment_col2)\n",
    "\n",
    "# Equipment_same_num 파생변수 생성\n",
    "def determine_equipment_same_num(row):\n",
    "    if (row['Equipment_Dam'] == 'Dam dispenser #1' and row['Equipment_AutoClave'] == 'Auto Clave Out' and \n",
    "        row['Equipment_Fill1'] == 'Fill1 dispenser #1' and row['Equipment_Fill2'] == 'Fill2 dispenser #1') or \\\n",
    "       (row['Equipment_Dam'] == 'Dam dispenser #2' and row['Equipment_AutoClave'] == 'Auto Clave Out' and \n",
    "        row['Equipment_Fill1'] == 'Fill1 dispenser #2' and row['Equipment_Fill2'] == 'Fill2 dispenser #2'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train_data['Equipment_same_num'] = new_train.apply(determine_equipment_same_num, axis=1)\n",
    "test_data['Equipment_same_num'] = new_test.apply(determine_equipment_same_num, axis=1)\n",
    "\n",
    "train_data = train_data.drop(columns=['Equipment_Dam', 'Equipment_AutoClave', 'Equipment_Fill1', 'Equipment_Fill2'])\n",
    "test_data = test_data.drop(columns=['Equipment_Dam', 'Equipment_AutoClave', 'Equipment_Fill1', 'Equipment_Fill2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07a545-9df9-488c-a27a-4104a8336fb0",
   "metadata": {},
   "source": [
    "- palletID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "d399216f-b736-4190-be4b-3108366606c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 변수의 값이 동일하면 해당 값을 가져가고, 하나라도 일치하지 않으면 diff의 값을 가지는 파생 변수 생성 함수\n",
    "def create_palletid_collect_result(df):\n",
    "    df['PalletID_Collect_Result'] = df.apply(\n",
    "        lambda row: row['PalletID Collect Result_Dam'] \n",
    "                    if (row['PalletID Collect Result_Dam'] == row['PalletID Collect Result_Fill1'] == row['PalletID Collect Result_Fill2']) \n",
    "                    else 'diff', \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# 함수 적용\n",
    "create_palletid_collect_result(train_data)\n",
    "create_palletid_collect_result(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "id": "5f91f21e-89f3-4b69-afc1-6d5ab6d820f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'PalletID Collect Result_Dam',\n",
    "    'PalletID Collect Result_Fill1',\n",
    "    'PalletID Collect Result_Fill2'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5310e-3d7e-4765-b902-91172707ff5a",
   "metadata": {},
   "source": [
    "- production Qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "id": "c478d82f-a486-471a-a12e-3ef6e9bc875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 변수의 값이 동일하면 해당 값을 가져가고, 하나라도 일치하지 않으면 0의 값을 가지는 파생 변수 생성 함수\n",
    "def create_palletid_collect_result(df):\n",
    "    df['Production_Qty_Collect_Result'] = df.apply(\n",
    "        lambda row: row['Production Qty Collect Result_Dam'] \n",
    "                    if (row['Production Qty Collect Result_Dam'] == row['Production Qty Collect Result_Fill1'] == row['Production Qty Collect Result_Fill2']) \n",
    "                    else 'diff', \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# 함수 적용\n",
    "create_palletid_collect_result(train_data)\n",
    "create_palletid_collect_result(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "id": "bd7b99c2-4d0a-40cc-8ec4-6a31b2862bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'Production Qty Collect Result_Dam',\n",
    "    'Production Qty Collect Result_Fill1',\n",
    "    'Production Qty Collect Result_Fill2'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de023c44-9098-4852-8650-f30408ded1dd",
   "metadata": {},
   "source": [
    "### 4. CURE 변수\n",
    "- dam -> distance 파생변수 (standby는 단일값, start와 end는 값은 여러개지만 distance 파생변수를 만들었을 때 더 의미있었음)\n",
    "- fill2 -> 변수값 범주화 (start, end, standby를 각각 범주화했을 때가 합쳐서 distance 만들었을 때보다 더 의미있었음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "id": "9ec88825-bef2-443c-8722-6c3f3867a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dam\n",
    "# 시작 위치와 끝 위치 열 이름\n",
    "start_x_col = 'CURE START POSITION X Collect Result_Dam'\n",
    "start_z_col = 33.5\n",
    "end_x_col = 'CURE END POSITION X Collect Result_Dam'\n",
    "end_z_col = 'CURE END POSITION Z Collect Result_Dam'\n",
    "\n",
    "# 시작 위치와 끝 위치 사이의 거리 계산\n",
    "train_data['CURE_DISTANCE_Dam'] = np.sqrt(\n",
    "    (train_data[end_x_col] - train_data[start_x_col]) ** 2 +\n",
    "    (train_data[end_z_col] - start_z_col) ** 2\n",
    ")\n",
    "\n",
    "test_data['CURE_DISTANCE_Dam'] = np.sqrt(\n",
    "    (train_data[end_x_col] - train_data[start_x_col]) ** 2 +\n",
    "    (train_data[end_z_col] - start_z_col) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "372b636c-6dff-42db-a9c8-f4abdb726159",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fill2\n",
    "# UV 경화 좌표 합치기\n",
    "def create_coordinate_columns(data):\n",
    "    # Fill2\n",
    "    # cure end\n",
    "    data['cure_end_position_XZ_Fill2'] = (\n",
    "        data['CURE END POSITION X Collect Result_Fill2'].astype(str) + ',' +\n",
    "        data['CURE END POSITION Z Collect Result_Fill2'].astype(str) \n",
    "    )\n",
    "\n",
    "    # cure start\n",
    "    data['cure_start_position_XZ_Fill2'] = (\n",
    "        data['CURE START POSITION X Collect Result_Fill2'].astype(str) + ',' +\n",
    "        data['CURE START POSITION Z Collect Result_Fill2'].astype(str) \n",
    "    )\n",
    "\n",
    "    # cure standby\n",
    "    data['cure_standby_position_XZ_Fill2'] = (\n",
    "        data['CURE STANDBY POSITION X Collect Result_Fill2'].astype(str) + ',' +\n",
    "        data['CURE STANDBY POSITION Z Collect Result_Fill2'].astype(str) \n",
    "    )\n",
    "\n",
    "# train_data와 test_data에 대해 함수 호출\n",
    "create_coordinate_columns(train_data)\n",
    "create_coordinate_columns(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "eb7f394f-640b-4bef-b01e-008310aa2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'CURE END POSITION X Collect Result_Dam',\n",
    "    'CURE END POSITION Z Collect Result_Dam',\n",
    "    'CURE END POSITION Θ Collect Result_Dam',\n",
    "    'CURE START POSITION X Collect Result_Dam',\n",
    "    'CURE START POSITION Z Collect Result_Dam',\n",
    "    'CURE START POSITION Θ Collect Result_Dam',\n",
    "\n",
    "    'CURE END POSITION X Collect Result_Fill2',\n",
    "    'CURE END POSITION Z Collect Result_Fill2',\n",
    "    'CURE END POSITION Θ Collect Result_Fill2',\n",
    "    'CURE START POSITION X Collect Result_Fill2',\n",
    "    'CURE START POSITION Z Collect Result_Fill2',\n",
    "    'CURE START POSITION Θ Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION X Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION Θ Collect Result_Fill2'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a4b1b-3e7b-4413-9704-d35f02fac78a",
   "metadata": {},
   "source": [
    "### 5. HEAD 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6fe5f-96d8-450c-9ef6-ae8c17a7901c",
   "metadata": {},
   "source": [
    "- dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "id": "7bda1832-ce60-4c22-abb7-a760df286801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 스테이지의 좌표 열 정의\n",
    "stage1_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam']\n",
    "\n",
    "stage2_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam']\n",
    "\n",
    "stage3_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "# 거리 계산 함수\n",
    "def calculate_distances(data):\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE2_Dam'] = np.sqrt(\n",
    "        (data[stage2_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage2_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage2_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE2_STAGE3_Dam'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage2_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage2_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage2_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_distances(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_distances(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "id": "8db711e9-4ca2-4076-b689-241d786d968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 열 이름\n",
    "stage1_stage2_col = 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Dam'\n",
    "stage2_stage3_col = 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Dam'\n",
    "stage1_stage3_col = 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam'\n",
    "\n",
    "# 삼각형의 넓이와 높이를 계산하는 함수\n",
    "def calculate_triangle_features(data):\n",
    "    a = data[stage1_stage2_col]\n",
    "    b = data[stage2_stage3_col]\n",
    "    c = data[stage1_stage3_col]\n",
    "\n",
    "    # 헤론의 공식에 따른 삼각형의 넓이 계산\n",
    "    s = (a + b + c) / 2\n",
    "    area = np.sqrt(s * (s - a) * (s - b) * (s - c))\n",
    "\n",
    "    # 높이 계산 (밑변을 c로 가정)\n",
    "    height = (2 * area) / c\n",
    "\n",
    "    # 결과를 새로운 열에 저장\n",
    "    data['HEAD NORMAL DISTANCE_TRIANGLE_area_Dam'] = area\n",
    "    data['HEAD NORMAL DISTANCE_TRIANGLE_height_Dam'] = height\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_triangle_features(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_triangle_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "id": "16669b71-e0b1-46de-b64c-4b014010aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'\n",
    "\n",
    "    , 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Dam'\n",
    "    , 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Dam'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "id": "fb6bf635-6060-4fd1-ac71-d057fae56513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dam 노즐 zero 위치 Z좌표 드롭\n",
    "train_data.drop(columns='Head Zero Position Z Collect Result_Dam', inplace=True)\n",
    "test_data.drop(columns='Head Zero Position Z Collect Result_Dam', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bffb86-f2cc-44ad-a495-a107d3b9c98a",
   "metadata": {},
   "source": [
    "- fill1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "id": "53e42711-b516-492c-a89a-2358839af5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 스테이지의 좌표 열 정의\n",
    "stage1_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1']\n",
    "\n",
    "stage2_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1']\n",
    "\n",
    "stage3_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "# 거리 계산 함수\n",
    "def calculate_distances(data):\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill1'] = np.sqrt(\n",
    "        (data[stage2_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage2_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage2_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill1'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage2_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage2_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage2_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_distances(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_distances(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "id": "76c1438e-9ead-47dd-80fd-b08faa85f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 열 이름\n",
    "stage1_stage2_col = 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill1'\n",
    "stage2_stage3_col = 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill1'\n",
    "stage1_stage3_col = 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1'\n",
    "\n",
    "# 삼각형의 넓이와 높이를 계산하는 함수\n",
    "def calculate_triangle_features(data):\n",
    "    a = data[stage1_stage2_col]\n",
    "    b = data[stage2_stage3_col]\n",
    "    c = data[stage1_stage3_col]\n",
    "\n",
    "    # 헤론의 공식에 따른 삼각형의 넓이 계산\n",
    "    s = (a + b + c) / 2\n",
    "    area = np.sqrt(s * (s - a) * (s - b) * (s - c))\n",
    "\n",
    "    # 높이 계산 (밑변을 c로 가정)\n",
    "    height = (2 * area) / c\n",
    "\n",
    "    # 결과를 새로운 열에 저장\n",
    "    data['HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1'] = area\n",
    "    data['HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1'] = height\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_triangle_features(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_triangle_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "id": "fa31162e-094d-47fd-bef2-9e55bc4f9dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'\n",
    "\n",
    "    , 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill1'\n",
    "    , 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill1'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be5165-b1d0-412f-b293-1207d90e433b",
   "metadata": {},
   "source": [
    "- fill2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "id": "836359e0-e3f3-487d-92d8-61869849bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 스테이지의 좌표 열 정의\n",
    "stage1_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2']\n",
    "\n",
    "stage2_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2']\n",
    "\n",
    "stage3_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2',\n",
    "               'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2']\n",
    "\n",
    "# 거리 계산 함수\n",
    "def calculate_distances(data):\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2'] = np.sqrt(\n",
    "        (data[stage2_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage2_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage2_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage2_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage2_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage2_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    data['HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2'] = np.sqrt(\n",
    "        (data[stage3_cols[0]] - data[stage1_cols[0]]) ** 2 +\n",
    "        (data[stage3_cols[1]] - data[stage1_cols[1]]) ** 2 +\n",
    "        (data[stage3_cols[2]] - data[stage1_cols[2]]) ** 2\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "# train_data에 적용\n",
    "train_data = calculate_distances(train_data)\n",
    "\n",
    "# test_data에 적용\n",
    "test_data = calculate_distances(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "id": "53a7041f-8fe0-4e57-8559-115828424a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2'\n",
    "\n",
    "    , 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2'\n",
    "    , 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a64bf1-d745-47da-aedf-02361f2f2329",
   "metadata": {},
   "source": [
    "### 6. Resin 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3106d-63d0-411b-a83b-9baeedfa874a",
   "metadata": {},
   "source": [
    "- dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "id": "f9857a16-8c84-4381-a5bd-ecc78aa2ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume*time 파생변수 - Dam\n",
    "train_data['volume_time_multip_stage1_Dam'] = train_data['Dispense Volume(Stage1) Collect Result_Dam'] * train_data['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "train_data['volume_time_multip_stage2_Dam'] = train_data['Dispense Volume(Stage2) Collect Result_Dam'] * train_data['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "train_data['volume_time_multip_stage3_Dam'] = train_data['Dispense Volume(Stage3) Collect Result_Dam'] * train_data['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "train_data['volume_time_multip_avg_Dam'] = (train_data['volume_time_multip_stage1_Dam'] + \n",
    "                                            train_data['volume_time_multip_stage2_Dam'] + \n",
    "                                            train_data['volume_time_multip_stage3_Dam']) / 3\n",
    "\n",
    "# volume*time 파생변수 - Dam\n",
    "test_data['volume_time_multip_stage1_Dam'] = test_data['Dispense Volume(Stage1) Collect Result_Dam'] * test_data['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "test_data['volume_time_multip_stage2_Dam'] = test_data['Dispense Volume(Stage2) Collect Result_Dam'] * test_data['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "test_data['volume_time_multip_stage3_Dam'] = test_data['Dispense Volume(Stage3) Collect Result_Dam'] * test_data['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "test_data['volume_time_multip_avg_Dam'] = (test_data['volume_time_multip_stage1_Dam'] + \n",
    "                                            test_data['volume_time_multip_stage2_Dam'] + \n",
    "                                            test_data['volume_time_multip_stage3_Dam']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "id": "0e0ebd5c-6fd7-4c5b-89cd-042b3a3671e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 열 목록 추가\n",
    "columns_to_drop = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage1) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage2) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage3) Collect Result_Dam',\n",
    "    #'volume_time_multip_stage1_Dam',\n",
    "    #'volume_time_multip_stage2_Dam',\n",
    "    #'volume_time_multip_stage3_Dam'\n",
    "]\n",
    "\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9986f1-a251-430e-bbd9-705bdd0cfbda",
   "metadata": {},
   "source": [
    "- fill1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "id": "b2b05f8b-7a55-40be-aa70-02faa32cffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume*time 파생변수 - Fill1\n",
    "train_data['volume_time_multip_stage1_Fill1'] = train_data['Dispense Volume(Stage1) Collect Result_Fill1'] * train_data['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "train_data['volume_time_multip_stage2_Fill1'] = train_data['Dispense Volume(Stage2) Collect Result_Fill1'] * train_data['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "train_data['volume_time_multip_stage3_Fill1'] = train_data['Dispense Volume(Stage3) Collect Result_Fill1'] * train_data['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "\n",
    "train_data['volume_time_multip_avg_Fill1'] = (train_data['volume_time_multip_stage1_Fill1'] + \n",
    "                                            train_data['volume_time_multip_stage2_Fill1'] + \n",
    "                                            train_data['volume_time_multip_stage3_Fill1']) / 3\n",
    "\n",
    "# volume*time 파생변수 - Fill1\n",
    "test_data['volume_time_multip_stage1_Fill1'] = test_data['Dispense Volume(Stage1) Collect Result_Fill1'] * test_data['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "test_data['volume_time_multip_stage2_Fill1'] = test_data['Dispense Volume(Stage2) Collect Result_Fill1'] * test_data['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "test_data['volume_time_multip_stage3_Fill1'] = test_data['Dispense Volume(Stage3) Collect Result_Fill1'] * test_data['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "\n",
    "test_data['volume_time_multip_avg_Fill1'] = (test_data['volume_time_multip_stage1_Fill1'] + \n",
    "                                            test_data['volume_time_multip_stage2_Fill1'] + \n",
    "                                            test_data['volume_time_multip_stage3_Fill1']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "id": "bff6cf24-453d-4bba-a5e3-bddededfb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 열 목록 추가\n",
    "columns_to_drop = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage2) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage3) Collect Result_Fill1',\n",
    "    #'volume_time_multip_stage1_Fill1',\n",
    "    #'volume_time_multip_stage2_Fill1',\n",
    "    #'volume_time_multip_stage3_Fill1'\n",
    "]\n",
    "\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56006cb7-66fa-4757-816c-260e12d6555e",
   "metadata": {},
   "source": [
    "### 7. Circle, Line 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "id": "6d445887-0bf3-4a31-956b-bdc711c606c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### circle\n",
    "# 열 이름 변경\n",
    "train_data.rename(columns={\n",
    "    'Stage1 Circle1 Distance Speed Collect Result_Dam': 'Stage1_Circle_Distance_Speed_Dam',\n",
    "    'Stage2 Circle1 Distance Speed Collect Result_Dam': 'Stage2_Circle_Distance_Speed_Dam',\n",
    "    'Stage3 Circle1 Distance Speed Collect Result_Dam': 'Stage3_Circle_Distance_Speed_Dam'\n",
    "}, inplace=True)\n",
    "\n",
    "test_data.rename(columns={\n",
    "    'Stage1 Circle1 Distance Speed Collect Result_Dam': 'Stage1_Circle_Distance_Speed_Dam',\n",
    "    'Stage2 Circle1 Distance Speed Collect Result_Dam': 'Stage2_Circle_Distance_Speed_Dam',\n",
    "    'Stage3 Circle1 Distance Speed Collect Result_Dam': 'Stage3_Circle_Distance_Speed_Dam'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "id": "61d337af-f8b5-4a00-a32a-8f23936a5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle4 Distance Speed Collect Result_Dam'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "id": "635f16a4-2bc5-43c2-8d71-0c25dc28fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### line\n",
    "# line1&3과 line2&4를 합친 파생변수 생성 함수\n",
    "def check_distance_speed(data, stage):\n",
    "    # 단계에 따라 라인 번호 정의\n",
    "    line_pairs = [(1, 3), (2, 4)]\n",
    "    \n",
    "    # 각 라인 쌍에 대해 반복\n",
    "    for line1, line2 in line_pairs:\n",
    "        line1_name = f'Stage{stage} Line{line1} Distance Speed Collect Result_Dam'\n",
    "        line2_name = f'Stage{stage} Line{line2} Distance Speed Collect Result_Dam'\n",
    "        \n",
    "        # 새로운 열 이름 설정\n",
    "        new_col_name = f'stage{stage}_line{line1}{line2}_distance_speed_Dam'\n",
    "        \n",
    "        # 조건에 따라 값 설정\n",
    "        data[new_col_name] = data.apply(\n",
    "            lambda row: row[line1_name] if row[line1_name] == row[line2_name] else 'diff', axis=1\n",
    "        )\n",
    "\n",
    "# train_data와 test_data 모두에 대해 함수 호출\n",
    "for stage in range(1, 4):\n",
    "    check_distance_speed(train_data, stage)\n",
    "    check_distance_speed(test_data, stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "id": "ba83e4f4-f481-468c-9b97-e0cbe31ad381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data에서 변수들을 object 타입으로 변환\n",
    "train_data['stage1_line24_distance_speed_Dam'] = train_data['stage1_line24_distance_speed_Dam'].astype(object)\n",
    "train_data['stage2_line24_distance_speed_Dam'] = train_data['stage2_line24_distance_speed_Dam'].astype(object)\n",
    "train_data['stage3_line24_distance_speed_Dam'] = train_data['stage3_line24_distance_speed_Dam'].astype(object)\n",
    "\n",
    "# test_data에서 변수들을 object 타입으로 변환\n",
    "test_data['stage1_line24_distance_speed_Dam'] = test_data['stage1_line24_distance_speed_Dam'].astype(object)\n",
    "test_data['stage2_line24_distance_speed_Dam'] = test_data['stage2_line24_distance_speed_Dam'].astype(object)\n",
    "test_data['stage3_line24_distance_speed_Dam'] = test_data['stage3_line24_distance_speed_Dam'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "id": "fbe85479-dc33-4121-a8ab-5cc3bd6b5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    'Stage2 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    'Stage3 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line4 Distance Speed Collect Result_Dam'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d80796e-f6ac-434a-9ab6-e967ca205fe5",
   "metadata": {},
   "source": [
    "### 8. Thickness 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "id": "769fdc40-d747-4671-b611-bab9d874055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 개 컬럼의 평균을 계산하여 새로운 컬럼 생성\n",
    "train_data['average_thickness_Dam'] = train_data[['THICKNESS 1 Collect Result_Dam', \n",
    "                                                  'THICKNESS 2 Collect Result_Dam', \n",
    "                                                  'THICKNESS 3 Collect Result_Dam']].mean(axis=1)\n",
    "\n",
    "test_data['average_thickness_Dam'] = test_data[['THICKNESS 1 Collect Result_Dam', \n",
    "                                                'THICKNESS 2 Collect Result_Dam', \n",
    "                                                'THICKNESS 3 Collect Result_Dam']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "id": "9bb9b4ca-e018-4b75-aa27-eb85bd990683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 컬럼 리스트\n",
    "columns_to_drop = [\n",
    "    'THICKNESS 1 Collect Result_Dam',\n",
    "    'THICKNESS 2 Collect Result_Dam',\n",
    "    'THICKNESS 3 Collect Result_Dam'\n",
    "]\n",
    "\n",
    "# 지정한 컬럼 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c7b4d-1be0-431c-ab45-ff09a5e96f0f",
   "metadata": {},
   "source": [
    "### 9. Autoclave 관련 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "id": "b1f89fb6-59bd-4203-8c2d-0b68548b1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 압력과 시간의 곱을 담은 새로운 컬럼 생성\n",
    "train_data['1st_pressure_time_AutoClave'] = train_data['1st Pressure Collect Result_AutoClave'] * train_data['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "train_data['2nd_pressure_time_AutoClave'] = train_data['2nd Pressure Collect Result_AutoClave'] * train_data['2nd Pressure Unit Time_AutoClave']\n",
    "train_data['3rd_pressure_time_AutoClave'] = train_data['3rd Pressure Collect Result_AutoClave'] * train_data['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "train_data['avg_pressure_time_AutoClave'] = (train_data['1st_pressure_time_AutoClave'] +\n",
    "                                             train_data['2nd_pressure_time_AutoClave'] +\n",
    "                                             train_data['3rd_pressure_time_AutoClave']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "id": "f53849fa-07d1-4d3e-a25d-79fa1a9525c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 압력과 시간의 곱을 담은 새로운 컬럼 생성\n",
    "test_data['1st_pressure_time_AutoClave'] = test_data['1st Pressure Collect Result_AutoClave'] * test_data['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "test_data['2nd_pressure_time_AutoClave'] = test_data['2nd Pressure Collect Result_AutoClave'] * test_data['2nd Pressure Unit Time_AutoClave']\n",
    "test_data['3rd_pressure_time_AutoClave'] = test_data['3rd Pressure Collect Result_AutoClave'] * test_data['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "test_data['avg_pressure_time_AutoClave'] = (test_data['1st_pressure_time_AutoClave'] +\n",
    "                                             test_data['2nd_pressure_time_AutoClave'] +\n",
    "                                             test_data['3rd_pressure_time_AutoClave']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "id": "0cf9e13a-fada-448a-b045-3400cb9467e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 컬럼 리스트\n",
    "columns_to_drop = [\n",
    "    '1st Pressure Collect Result_AutoClave',\n",
    "    '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    "    '2nd Pressure Collect Result_AutoClave',\n",
    "    '2nd Pressure Unit Time_AutoClave',\n",
    "    '3rd Pressure Collect Result_AutoClave',\n",
    "    '3rd Pressure Unit Time_AutoClave',\n",
    "]\n",
    "\n",
    "# 지정한 컬럼 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e41272-4e95-40f2-832e-c7a4ad1d5460",
   "metadata": {},
   "source": [
    "### 10. Time 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "id": "f9923c80-82b6-45e6-a2f0-032cb54c6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총시간 대비 비율 변수\n",
    "def calculate_total_time_and_ratios(data):\n",
    "    data['total_time'] = (\n",
    "        data['Machine Tact time Collect Result_Dam'] +\n",
    "        data['Machine Tact time Collect Result_Fill1'] +\n",
    "        data['Machine Tact time Collect Result_Fill2'] +\n",
    "        data['Chamber Temp. Unit Time_AutoClave']\n",
    "    )\n",
    "    data['time_ratio_Dam'] = (data['Machine Tact time Collect Result_Dam'] / data['total_time']).round(3)\n",
    "    data['time_ratio_Fill1'] = (data['Machine Tact time Collect Result_Fill1'] / data['total_time']).round(3)\n",
    "    data['time_ratio_Fill2'] = (data['Machine Tact time Collect Result_Fill2'] / data['total_time']).round(3)\n",
    "    data['time_ratio_AutoClave'] = (data['Chamber Temp. Unit Time_AutoClave'] / data['total_time']).round(3)\n",
    "    return data\n",
    "\n",
    "# train_data와 test_data에 함수 적용\n",
    "train_data = calculate_total_time_and_ratios(train_data)\n",
    "test_data = calculate_total_time_and_ratios(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "id": "71df6a7f-21bf-4b8d-9def-75dd8a376b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 제거\n",
    "train_data.drop(columns=[\n",
    "    'total_time'\n",
    "    , 'Machine Tact time Collect Result_Dam'\n",
    "    , 'Machine Tact time Collect Result_Fill1'\n",
    "    , 'Machine Tact time Collect Result_Fill2'\n",
    "    , 'Chamber Temp. Unit Time_AutoClave'], inplace=True)\n",
    "\n",
    "test_data.drop(columns=[\n",
    "    'total_time'\n",
    "    , 'Machine Tact time Collect Result_Dam'\n",
    "    , 'Machine Tact time Collect Result_Fill1'\n",
    "    , 'Machine Tact time Collect Result_Fill2'\n",
    "    , 'Chamber Temp. Unit Time_AutoClave'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d574b",
   "metadata": {},
   "source": [
    "### 00. 추가된 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "id": "2abc23ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Process Desc 포함 변수 >\n",
      "Collect Date_Dam\n",
      "Collect Date_AutoClave\n",
      "Collect Date_Fill1\n",
      "Collect Date_Fill2\n"
     ]
    }
   ],
   "source": [
    "# 'Process Desc'를 포함하는 열 이름 필터링\n",
    "filter_col = train_data.filter(like='Collect Date_').columns\n",
    "\n",
    "print(\"< Process Desc 포함 변수 >\")\n",
    "for col in filter_col:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "id": "4f3b3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Collect Date 열들을 datetime 형식으로 변환\n",
    "train_data['Collect Date_Dam'] = pd.to_datetime(train_data['Collect Date_Dam'])\n",
    "train_data['Collect Date_Fill1'] = pd.to_datetime(train_data['Collect Date_Fill1'])\n",
    "train_data['Collect Date_Fill2'] = pd.to_datetime(train_data['Collect Date_Fill2'])\n",
    "train_data['Collect Date_AutoClave'] = pd.to_datetime(train_data['Collect Date_AutoClave'])\n",
    "\n",
    "test_data['Collect Date_Dam'] = pd.to_datetime(test_data['Collect Date_Dam'])\n",
    "test_data['Collect Date_Fill1'] = pd.to_datetime(test_data['Collect Date_Fill1'])\n",
    "test_data['Collect Date_Fill2'] = pd.to_datetime(test_data['Collect Date_Fill2'])\n",
    "test_data['Collect Date_AutoClave'] = pd.to_datetime(test_data['Collect Date_AutoClave'])\n",
    "\n",
    "# 각 열 간의 시간 차이를 계산하여 새로운 변수 생성\n",
    "train_data['time_diff_Dam_Fill1'] = (train_data['Collect Date_Fill1'] - train_data['Collect Date_Dam']).dt.total_seconds() / 60\n",
    "train_data['time_diff_Fill1_Fill2'] = (train_data['Collect Date_Fill2'] - train_data['Collect Date_Fill1']).dt.total_seconds() / 60\n",
    "train_data['time_diff_Fill2_AutoClave'] = (train_data['Collect Date_AutoClave'] - train_data['Collect Date_Fill2']).dt.total_seconds() / 60\n",
    "train_data['time_diff_Dam_AutoClave'] = (train_data['Collect Date_AutoClave'] - train_data['Collect Date_Dam']).dt.total_seconds() / 60 \n",
    "\n",
    "test_data['time_diff_Dam_Fill1'] = (test_data['Collect Date_Fill1'] - test_data['Collect Date_Dam']).dt.total_seconds() / 60\n",
    "test_data['time_diff_Fill1_Fill2'] = (test_data['Collect Date_Fill2'] - test_data['Collect Date_Fill1']).dt.total_seconds() / 60\n",
    "test_data['time_diff_Fill2_AutoClave'] = (test_data['Collect Date_AutoClave'] - test_data['Collect Date_Fill2']).dt.total_seconds() / 60\n",
    "test_data['time_diff_Dam_AutoClave'] = (test_data['Collect Date_AutoClave'] - test_data['Collect Date_Dam']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "id": "d93605fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# time_diff_Dam_Fill1의 값이 20 이상이면 1, 아니면 0을 부여하는 파생 변수 생성\n",
    "train_data['time_gap_Dam'] = train_data['time_diff_Dam_Fill1'].apply(lambda x: 1 if x >= 20 else 0)\n",
    "test_data['time_gap_Dam'] = test_data['time_diff_Dam_Fill1'].apply(lambda x: 1 if x >= 20 else 0)\n",
    "\n",
    "# time_diff_Fill1_Fill2의 값이 20 이상이면 1, 아니면 0을 부여하는 파생 변수 생성\n",
    "train_data['time_gap_Fill1'] = train_data['time_diff_Fill1_Fill2'].apply(lambda x: 1 if x >= 20 else 0)\n",
    "test_data['time_gap_Fill1'] = test_data['time_diff_Fill1_Fill2'].apply(lambda x: 1 if x >= 20 else 0)\n",
    "\n",
    "# time_diff_Fill2_AutoClave의 값이 120 이상이면 1을 부여하는 파생 변수 생성\n",
    "train_data['time_gap_Fill2'] = train_data['time_diff_Fill2_AutoClave'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "test_data['time_gap_Fill2'] = test_data['time_diff_Fill2_AutoClave'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "# time_diff_Dam_AutoClave의 값이 130 이상이면 1을 부여하는 파생 변수 생성\n",
    "train_data['time_gap_All'] = train_data['time_diff_Dam_AutoClave'].apply(lambda x: 1 if x >= 130 else 0)\n",
    "test_data['time_gap_All'] = test_data['time_diff_Dam_AutoClave'].apply(lambda x: 1 if x >= 130 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "id": "7682de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 변수 목록\n",
    "columns_to_drop = [\n",
    "    # 'Collect Date_Dam', 'Collect Date_AutoClave', 'Collect Date_Fill1', 'Collect Date_Fill2',\n",
    "    'time_diff_Dam_Fill1', 'time_diff_Fill1_Fill2', 'time_diff_Fill2_AutoClave', 'time_diff_Dam_AutoClave'\n",
    "]\n",
    "\n",
    "# 변수 제거\n",
    "train_data = train_data.drop(columns=columns_to_drop)\n",
    "test_data = test_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "id": "22bebf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 105 columns):\n",
      " #    Column                                                 Dtype         \n",
      "---   ------                                                 -----         \n",
      " 0    model_suffix                                           object        \n",
      " 1    Collect Date_Dam                                       datetime64[ns]\n",
      " 2    CURE SPEED Collect Result_Dam                          int64         \n",
      " 3    CURE STANDBY POSITION X Collect Result_Dam             int64         \n",
      " 4    CURE STANDBY POSITION Z Collect Result_Dam             float64       \n",
      " 5    CURE STANDBY POSITION Θ Collect Result_Dam             int64         \n",
      " 6    DISCHARGED SPEED OF RESIN Collect Result_Dam           int64         \n",
      " 7    HEAD Standby Position X Collect Result_Dam             float64       \n",
      " 8    HEAD Standby Position Y Collect Result_Dam             int64         \n",
      " 9    HEAD Standby Position Z Collect Result_Dam             float64       \n",
      " 10   Head Clean Position X Collect Result_Dam               float64       \n",
      " 11   Head Clean Position Y Collect Result_Dam               float64       \n",
      " 12   Head Clean Position Z Collect Result_Dam               float64       \n",
      " 13   Head Purge Position X Collect Result_Dam               float64       \n",
      " 14   Head Purge Position Y Collect Result_Dam               float64       \n",
      " 15   Head Purge Position Z Collect Result_Dam               float64       \n",
      " 16   Head Zero Position X Collect Result_Dam                float64       \n",
      " 17   Head Zero Position Y Collect Result_Dam                float64       \n",
      " 18   Stage1_Circle_Distance_Speed_Dam                       int64         \n",
      " 19   Stage2_Circle_Distance_Speed_Dam                       int64         \n",
      " 20   Stage3_Circle_Distance_Speed_Dam                       int64         \n",
      " 21   WorkMode Collect Result                                float64       \n",
      " 22   Collect Date_AutoClave                                 datetime64[ns]\n",
      " 23   1st Pressure Judge Value_AutoClave                     object        \n",
      " 24   2nd Pressure Judge Value_AutoClave                     object        \n",
      " 25   3rd Pressure Judge Value_AutoClave                     object        \n",
      " 26   Chamber Temp. Collect Result_AutoClave                 int64         \n",
      " 27   Chamber Temp. Judge Value_AutoClave                    object        \n",
      " 28   GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave   object        \n",
      " 29   GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave      object        \n",
      " 30   Collect Date_Fill1                                     datetime64[ns]\n",
      " 31   DISCHARGED SPEED OF RESIN Collect Result_Fill1         float64       \n",
      " 32   HEAD Standby Position X Collect Result_Fill1           float64       \n",
      " 33   HEAD Standby Position Y Collect Result_Fill1           int64         \n",
      " 34   HEAD Standby Position Z Collect Result_Fill1           float64       \n",
      " 35   Head Clean Position X Collect Result_Fill1             float64       \n",
      " 36   Head Clean Position Y Collect Result_Fill1             float64       \n",
      " 37   Head Clean Position Z Collect Result_Fill1             float64       \n",
      " 38   Head Purge Position X Collect Result_Fill1             float64       \n",
      " 39   Head Purge Position Y Collect Result_Fill1             int64         \n",
      " 40   Head Purge Position Z Collect Result_Fill1             float64       \n",
      " 41   Collect Date_Fill2                                     datetime64[ns]\n",
      " 42   CURE SPEED Collect Result_Fill2                        int64         \n",
      " 43   DISCHARGED SPEED OF RESIN Collect Result_Fill2         int64         \n",
      " 44   DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill2  int64         \n",
      " 45   DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill2  float64       \n",
      " 46   DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill2  int64         \n",
      " 47   Dispense Volume(Stage1) Collect Result_Fill2           int64         \n",
      " 48   Dispense Volume(Stage2) Collect Result_Fill2           float64       \n",
      " 49   Dispense Volume(Stage3) Collect Result_Fill2           int64         \n",
      " 50   HEAD Standby Position X Collect Result_Fill2           float64       \n",
      " 51   HEAD Standby Position Y Collect Result_Fill2           int64         \n",
      " 52   HEAD Standby Position Z Collect Result_Fill2           int64         \n",
      " 53   Head Clean Position X Collect Result_Fill2             int64         \n",
      " 54   Head Clean Position Y Collect Result_Fill2             float64       \n",
      " 55   Head Clean Position Z Collect Result_Fill2             float64       \n",
      " 56   Head Purge Position X Collect Result_Fill2             float64       \n",
      " 57   Head Purge Position Y Collect Result_Fill2             int64         \n",
      " 58   Head Purge Position Z Collect Result_Fill2             float64       \n",
      " 59   target                                                 object        \n",
      " 60   cleaned_workorder                                      object        \n",
      " 61   Receip_n_suffix_3                                      int64         \n",
      " 62   Equipment_same_num                                     int64         \n",
      " 63   PalletID_Collect_Result                                object        \n",
      " 64   Production_Qty_Collect_Result                          object        \n",
      " 65   CURE_DISTANCE_Dam                                      float64       \n",
      " 66   cure_end_position_XZ_Fill2                             object        \n",
      " 67   cure_start_position_XZ_Fill2                           object        \n",
      " 68   cure_standby_position_XZ_Fill2                         object        \n",
      " 69   HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam                 float64       \n",
      " 70   HEAD NORMAL DISTANCE_TRIANGLE_area_Dam                 float64       \n",
      " 71   HEAD NORMAL DISTANCE_TRIANGLE_height_Dam               float64       \n",
      " 72   HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1               float64       \n",
      " 73   HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1               float64       \n",
      " 74   HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1             float64       \n",
      " 75   HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2               float64       \n",
      " 76   HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2               float64       \n",
      " 77   HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2               float64       \n",
      " 78   volume_time_multip_stage1_Dam                          float64       \n",
      " 79   volume_time_multip_stage2_Dam                          float64       \n",
      " 80   volume_time_multip_stage3_Dam                          float64       \n",
      " 81   volume_time_multip_avg_Dam                             float64       \n",
      " 82   volume_time_multip_stage1_Fill1                        float64       \n",
      " 83   volume_time_multip_stage2_Fill1                        float64       \n",
      " 84   volume_time_multip_stage3_Fill1                        float64       \n",
      " 85   volume_time_multip_avg_Fill1                           float64       \n",
      " 86   stage1_line13_distance_speed_Dam                       object        \n",
      " 87   stage1_line24_distance_speed_Dam                       object        \n",
      " 88   stage2_line13_distance_speed_Dam                       object        \n",
      " 89   stage2_line24_distance_speed_Dam                       object        \n",
      " 90   stage3_line13_distance_speed_Dam                       object        \n",
      " 91   stage3_line24_distance_speed_Dam                       object        \n",
      " 92   average_thickness_Dam                                  float64       \n",
      " 93   1st_pressure_time_AutoClave                            float64       \n",
      " 94   2nd_pressure_time_AutoClave                            float64       \n",
      " 95   3rd_pressure_time_AutoClave                            float64       \n",
      " 96   avg_pressure_time_AutoClave                            float64       \n",
      " 97   time_ratio_Dam                                         float64       \n",
      " 98   time_ratio_Fill1                                       float64       \n",
      " 99   time_ratio_Fill2                                       float64       \n",
      " 100  time_ratio_AutoClave                                   float64       \n",
      " 101  time_gap_Dam                                           int64         \n",
      " 102  time_gap_Fill1                                         int64         \n",
      " 103  time_gap_Fill2                                         int64         \n",
      " 104  time_gap_All                                           int64         \n",
      "dtypes: datetime64[ns](4), float64(54), int64(27), object(20)\n",
      "memory usage: 32.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407c0d4-c6a4-4b05-9f23-6d5bbbd9eb0f",
   "metadata": {},
   "source": [
    "### 11. 변수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "id": "037c7017-2c2c-41d7-a126-0435efac77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 변수 리스트\n",
    "columns_to_drop = [\n",
    "    'Chamber Temp. Judge Value_AutoClave', \n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave', \n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave'\n",
    "]\n",
    "\n",
    "train_data = train_data.drop(columns=columns_to_drop)\n",
    "test_data = test_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "id": "9f949535-f844-493c-84df-48fe1bf57930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제된 train_data 열 개수: 37\n",
      "삭제된 test_data 열 개수: 37\n"
     ]
    }
   ],
   "source": [
    "# 값의 종류가 1개이고 결측값이 없는 열을 제거하는 함수\n",
    "def drop_single_value_columns(df):\n",
    "    cols_to_drop = [col for col in df.columns if col != 'target' and df[col].nunique() == 1 and df[col].isnull().sum() == 0]\n",
    "    df_dropped = df.drop(columns=cols_to_drop)\n",
    "    return df_dropped, cols_to_drop\n",
    "\n",
    "# train_data와 test_data에서 해당 열 제거 및 삭제된 열 이름과 개수 출력\n",
    "train_data, train_cols_dropped = drop_single_value_columns(train_data)\n",
    "test_data, test_cols_dropped = drop_single_value_columns(test_data)\n",
    "\n",
    "# print(\"삭제된 train_data 열 이름:\", train_cols_dropped)\n",
    "print(\"삭제된 train_data 열 개수:\", len(train_cols_dropped))\n",
    "\n",
    "# print(\"삭제된 test_data 열 이름:\", test_cols_dropped)\n",
    "print(\"삭제된 test_data 열 개수:\", len(test_cols_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88801ff-c9a9-45dd-825d-005dbf75943f",
   "metadata": {},
   "source": [
    "### 12. target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "id": "1b040b33-0137-4968-b99b-c632d1e8473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model_suffix', 'target', 'cleaned_workorder',\n",
      "       'PalletID_Collect_Result', 'Production_Qty_Collect_Result',\n",
      "       'cure_end_position_XZ_Fill2', 'cure_start_position_XZ_Fill2',\n",
      "       'cure_standby_position_XZ_Fill2', 'stage1_line13_distance_speed_Dam',\n",
      "       'stage1_line24_distance_speed_Dam', 'stage2_line13_distance_speed_Dam',\n",
      "       'stage2_line24_distance_speed_Dam', 'stage3_line13_distance_speed_Dam',\n",
      "       'stage3_line24_distance_speed_Dam'],\n",
      "      dtype='object')  train_object_columns 갯수 : 14\n",
      "Index(['Set ID', 'model_suffix', 'cleaned_workorder',\n",
      "       'PalletID_Collect_Result', 'Production_Qty_Collect_Result',\n",
      "       'cure_end_position_XZ_Fill2', 'cure_start_position_XZ_Fill2',\n",
      "       'cure_standby_position_XZ_Fill2', 'stage1_line13_distance_speed_Dam',\n",
      "       'stage1_line24_distance_speed_Dam', 'stage2_line13_distance_speed_Dam',\n",
      "       'stage2_line24_distance_speed_Dam', 'stage3_line13_distance_speed_Dam',\n",
      "       'stage3_line24_distance_speed_Dam'],\n",
      "      dtype='object')  test_object_columns 갯수 : 14\n",
      "\n",
      "Train Data:\n",
      "model_suffix unique 값 갯수: 7\n",
      "target unique 값 갯수: 2\n",
      "cleaned_workorder unique 값 갯수: 568\n",
      "PalletID_Collect_Result unique 값 갯수: 17\n",
      "Production_Qty_Collect_Result unique 값 갯수: 608\n",
      "cure_end_position_XZ_Fill2 unique 값 갯수: 4\n",
      "cure_start_position_XZ_Fill2 unique 값 갯수: 5\n",
      "cure_standby_position_XZ_Fill2 unique 값 갯수: 4\n",
      "stage1_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage1_line24_distance_speed_Dam unique 값 갯수: 7\n",
      "stage2_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage2_line24_distance_speed_Dam unique 값 갯수: 10\n",
      "stage3_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage3_line24_distance_speed_Dam unique 값 갯수: 7\n",
      "\n",
      "Test Data:\n",
      "Set ID unique 값 갯수: 17361\n",
      "model_suffix unique 값 갯수: 7\n",
      "cleaned_workorder unique 값 갯수: 566\n",
      "PalletID_Collect_Result unique 값 갯수: 17\n",
      "Production_Qty_Collect_Result unique 값 갯수: 572\n",
      "cure_end_position_XZ_Fill2 unique 값 갯수: 4\n",
      "cure_start_position_XZ_Fill2 unique 값 갯수: 5\n",
      "cure_standby_position_XZ_Fill2 unique 값 갯수: 4\n",
      "stage1_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage1_line24_distance_speed_Dam unique 값 갯수: 7\n",
      "stage2_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage2_line24_distance_speed_Dam unique 값 갯수: 10\n",
      "stage3_line13_distance_speed_Dam unique 값 갯수: 9\n",
      "stage3_line24_distance_speed_Dam unique 값 갯수: 7\n"
     ]
    }
   ],
   "source": [
    "# object 타입의 변수 출력\n",
    "train_object_columns = train_data.select_dtypes(include=['object']).columns\n",
    "test_object_columns = test_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(train_object_columns, f\" train_object_columns 갯수 : {len(train_object_columns)}\")\n",
    "print(test_object_columns, f\" test_object_columns 갯수 : {len(test_object_columns)}\")\n",
    "\n",
    "# 각 object 변수의 고유 값 개수 출력\n",
    "print(\"\\nTrain Data:\")\n",
    "for col in train_object_columns:\n",
    "    unique_count = train_data[col].nunique()\n",
    "    print(f\"{col} unique 값 갯수: {unique_count}\")\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "for col in test_object_columns:\n",
    "    unique_count = test_data[col].nunique()\n",
    "    print(f\"{col} unique 값 갯수: {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "id": "2e9822a2-42bc-499f-9c46-cc5e0289731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 평균 타겟 값 계산 (abnormal 전체 비율)\n",
    "train_data['target_01'] = train_data['target'].apply(lambda x: 1 if x == 'AbNormal' else 0)\n",
    "global_mean = train_data['target_01'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "id": "a6c17701-dd06-4dcd-93ac-47558ef7f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용할 열 리스트\n",
    "columns_to_encode = [\n",
    "    'model_suffix',\n",
    "    'cleaned_workorder',\n",
    "    'PalletID_Collect_Result',\n",
    "    'cure_end_position_XZ_Fill2',\n",
    "    'cure_start_position_XZ_Fill2',\n",
    "    'cure_standby_position_XZ_Fill2',\n",
    "    'stage1_line13_distance_speed_Dam',\n",
    "    'stage1_line24_distance_speed_Dam',\n",
    "    'stage2_line13_distance_speed_Dam',\n",
    "    'stage2_line24_distance_speed_Dam',\n",
    "    'stage3_line13_distance_speed_Dam',\n",
    "    'stage3_line24_distance_speed_Dam',\n",
    "\n",
    "    # 추가로 Production_Qty_Collect_Result도 인코딩\n",
    "    'Production_Qty_Collect_Result'\n",
    "]\n",
    "\n",
    "# 전체 데이터의 평균 타겟값\n",
    "global_mean = train_data['target_01'].mean()\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    # 각 column에 대한 평균 타겟값과 카운트 계산\n",
    "    target_mean = train_data.groupby(column)['target_01'].mean()\n",
    "    count = train_data.groupby(column)['target_01'].count()\n",
    "\n",
    "    # 스무딩 적용\n",
    "    '''\n",
    "    추천 알파 값:\n",
    "    0.5: 일반적으로 많이 사용되는 값으로, 기존 데이터와 전체 평균 간의 균형을 잘 맞춰줍니다.\n",
    "    0.3: 데이터가 충분히 많고 각 카테고리의 타겟 값이 잘 분포되어 있을 때 사용.\n",
    "    0.7: 데이터가 적거나 특정 카테고리가 상대적으로 적을 때 사용.\n",
    "    '''\n",
    "    alpha = 0.5\n",
    "    smoothed_values = (target_mean * count + global_mean * alpha) / (count + alpha)\n",
    "\n",
    "    # 인코딩된 값을 데이터프레임에 추가\n",
    "    train_data[f'{column}_encoded'] = train_data[column].map(smoothed_values)\n",
    "\n",
    "    # test_data에 동일한 인코딩 값을 추가\n",
    "    encoding_dict = train_data.groupby(column)[f'{column}_encoded'].first().to_dict()\n",
    "    test_data[f'{column}_encoded'] = test_data[column].map(encoding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "id": "0326880b-fcdd-493f-8bda-b53932d5313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제할 열 리스트\n",
    "columns_to_drop = [\n",
    "    'target_01',\n",
    "    'model_suffix',\n",
    "    'cleaned_workorder',\n",
    "    'PalletID_Collect_Result',\n",
    "    'cure_end_position_XZ_Fill2',\n",
    "    'cure_start_position_XZ_Fill2',\n",
    "    'cure_standby_position_XZ_Fill2',\n",
    "    'stage1_line13_distance_speed_Dam',\n",
    "    'stage1_line24_distance_speed_Dam',\n",
    "    'stage2_line13_distance_speed_Dam',\n",
    "    'stage2_line24_distance_speed_Dam',\n",
    "    'stage3_line13_distance_speed_Dam',\n",
    "    'stage3_line24_distance_speed_Dam',\n",
    "\n",
    "    'Production_Qty_Collect_Result'\n",
    "]\n",
    "\n",
    "# train_data와 test_data에서 열 드랍\n",
    "train_data = train_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "test_data = test_data.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "id": "22bf67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 열을 제외한 나머지 열의 결측치를 0.05로 채우기\n",
    "test_data.fillna(value={col: 0.05 for col in test_data.columns if col != 'target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "25c940cd-54ae-4277-b259-dee92b98f837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 65 columns):\n",
      " #   Column                                          Non-Null Count  Dtype         \n",
      "---  ------                                          --------------  -----         \n",
      " 0   Collect Date_Dam                                40506 non-null  datetime64[ns]\n",
      " 1   CURE SPEED Collect Result_Dam                   40506 non-null  int64         \n",
      " 2   DISCHARGED SPEED OF RESIN Collect Result_Dam    40506 non-null  int64         \n",
      " 3   Head Clean Position Z Collect Result_Dam        40506 non-null  float64       \n",
      " 4   Head Purge Position Z Collect Result_Dam        40506 non-null  float64       \n",
      " 5   Head Zero Position Y Collect Result_Dam         40506 non-null  float64       \n",
      " 6   Stage1_Circle_Distance_Speed_Dam                40506 non-null  int64         \n",
      " 7   Stage2_Circle_Distance_Speed_Dam                40506 non-null  int64         \n",
      " 8   Stage3_Circle_Distance_Speed_Dam                40506 non-null  int64         \n",
      " 9   WorkMode Collect Result                         40506 non-null  float64       \n",
      " 10  Collect Date_AutoClave                          40506 non-null  datetime64[ns]\n",
      " 11  Chamber Temp. Collect Result_AutoClave          40506 non-null  int64         \n",
      " 12  Collect Date_Fill1                              40506 non-null  datetime64[ns]\n",
      " 13  DISCHARGED SPEED OF RESIN Collect Result_Fill1  40506 non-null  float64       \n",
      " 14  Head Purge Position Z Collect Result_Fill1      40506 non-null  float64       \n",
      " 15  Collect Date_Fill2                              40506 non-null  datetime64[ns]\n",
      " 16  CURE SPEED Collect Result_Fill2                 40506 non-null  int64         \n",
      " 17  Head Purge Position Z Collect Result_Fill2      40506 non-null  float64       \n",
      " 18  target                                          40506 non-null  object        \n",
      " 19  Receip_n_suffix_3                               40506 non-null  int64         \n",
      " 20  Equipment_same_num                              40506 non-null  int64         \n",
      " 21  CURE_DISTANCE_Dam                               40506 non-null  float64       \n",
      " 22  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam          40506 non-null  float64       \n",
      " 23  HEAD NORMAL DISTANCE_TRIANGLE_area_Dam          40506 non-null  float64       \n",
      " 24  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam        40506 non-null  float64       \n",
      " 25  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        40506 non-null  float64       \n",
      " 26  HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1        40506 non-null  float64       \n",
      " 27  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      40506 non-null  float64       \n",
      " 28  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2        40506 non-null  float64       \n",
      " 29  HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2        40506 non-null  float64       \n",
      " 30  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2        40506 non-null  float64       \n",
      " 31  volume_time_multip_stage1_Dam                   40506 non-null  float64       \n",
      " 32  volume_time_multip_stage2_Dam                   40506 non-null  float64       \n",
      " 33  volume_time_multip_stage3_Dam                   40506 non-null  float64       \n",
      " 34  volume_time_multip_avg_Dam                      40506 non-null  float64       \n",
      " 35  volume_time_multip_stage1_Fill1                 40506 non-null  float64       \n",
      " 36  volume_time_multip_stage2_Fill1                 40506 non-null  float64       \n",
      " 37  volume_time_multip_stage3_Fill1                 40506 non-null  float64       \n",
      " 38  volume_time_multip_avg_Fill1                    40506 non-null  float64       \n",
      " 39  average_thickness_Dam                           40506 non-null  float64       \n",
      " 40  1st_pressure_time_AutoClave                     40506 non-null  float64       \n",
      " 41  2nd_pressure_time_AutoClave                     40506 non-null  float64       \n",
      " 42  3rd_pressure_time_AutoClave                     40506 non-null  float64       \n",
      " 43  avg_pressure_time_AutoClave                     40506 non-null  float64       \n",
      " 44  time_ratio_Dam                                  40506 non-null  float64       \n",
      " 45  time_ratio_Fill1                                40506 non-null  float64       \n",
      " 46  time_ratio_Fill2                                40506 non-null  float64       \n",
      " 47  time_ratio_AutoClave                            40506 non-null  float64       \n",
      " 48  time_gap_Dam                                    40506 non-null  int64         \n",
      " 49  time_gap_Fill1                                  40506 non-null  int64         \n",
      " 50  time_gap_Fill2                                  40506 non-null  int64         \n",
      " 51  time_gap_All                                    40506 non-null  int64         \n",
      " 52  model_suffix_encoded                            40506 non-null  float64       \n",
      " 53  cleaned_workorder_encoded                       40506 non-null  float64       \n",
      " 54  PalletID_Collect_Result_encoded                 40506 non-null  float64       \n",
      " 55  cure_end_position_XZ_Fill2_encoded              40506 non-null  float64       \n",
      " 56  cure_start_position_XZ_Fill2_encoded            40506 non-null  float64       \n",
      " 57  cure_standby_position_XZ_Fill2_encoded          40506 non-null  float64       \n",
      " 58  stage1_line13_distance_speed_Dam_encoded        40506 non-null  float64       \n",
      " 59  stage1_line24_distance_speed_Dam_encoded        40506 non-null  float64       \n",
      " 60  stage2_line13_distance_speed_Dam_encoded        40506 non-null  float64       \n",
      " 61  stage2_line24_distance_speed_Dam_encoded        40506 non-null  float64       \n",
      " 62  stage3_line13_distance_speed_Dam_encoded        40506 non-null  float64       \n",
      " 63  stage3_line24_distance_speed_Dam_encoded        40506 non-null  float64       \n",
      " 64  Production_Qty_Collect_Result_encoded           40506 non-null  float64       \n",
      "dtypes: datetime64[ns](4), float64(47), int64(13), object(1)\n",
      "memory usage: 20.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# info 잘리지 않게 출력\n",
    "train_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "ae3dd21b-7dcb-4150-9eb4-37fe114ba4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 66 columns):\n",
      " #   Column                                          Non-Null Count  Dtype         \n",
      "---  ------                                          --------------  -----         \n",
      " 0   Set ID                                          17361 non-null  object        \n",
      " 1   Collect Date_Dam                                17361 non-null  datetime64[ns]\n",
      " 2   CURE SPEED Collect Result_Dam                   17361 non-null  int64         \n",
      " 3   DISCHARGED SPEED OF RESIN Collect Result_Dam    17361 non-null  int64         \n",
      " 4   Head Clean Position Z Collect Result_Dam        17361 non-null  float64       \n",
      " 5   Head Purge Position Z Collect Result_Dam        17361 non-null  float64       \n",
      " 6   Head Zero Position Y Collect Result_Dam         17361 non-null  float64       \n",
      " 7   Stage1_Circle_Distance_Speed_Dam                17361 non-null  int64         \n",
      " 8   Stage2_Circle_Distance_Speed_Dam                17361 non-null  int64         \n",
      " 9   Stage3_Circle_Distance_Speed_Dam                17361 non-null  int64         \n",
      " 10  WorkMode Collect Result                         17361 non-null  float64       \n",
      " 11  Collect Date_AutoClave                          17361 non-null  datetime64[ns]\n",
      " 12  Chamber Temp. Collect Result_AutoClave          17361 non-null  int64         \n",
      " 13  Collect Date_Fill1                              17361 non-null  datetime64[ns]\n",
      " 14  DISCHARGED SPEED OF RESIN Collect Result_Fill1  17361 non-null  float64       \n",
      " 15  Head Purge Position Z Collect Result_Fill1      17361 non-null  float64       \n",
      " 16  Collect Date_Fill2                              17361 non-null  datetime64[ns]\n",
      " 17  CURE SPEED Collect Result_Fill2                 17361 non-null  int64         \n",
      " 18  Head Purge Position Z Collect Result_Fill2      17361 non-null  float64       \n",
      " 19  target                                          0 non-null      float64       \n",
      " 20  Receip_n_suffix_3                               17361 non-null  int64         \n",
      " 21  Equipment_same_num                              17361 non-null  int64         \n",
      " 22  CURE_DISTANCE_Dam                               17361 non-null  float64       \n",
      " 23  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam          17361 non-null  float64       \n",
      " 24  HEAD NORMAL DISTANCE_TRIANGLE_area_Dam          17361 non-null  float64       \n",
      " 25  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam        17361 non-null  float64       \n",
      " 26  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        17361 non-null  float64       \n",
      " 27  HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1        17361 non-null  float64       \n",
      " 28  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      17361 non-null  float64       \n",
      " 29  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2        17361 non-null  float64       \n",
      " 30  HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2        17361 non-null  float64       \n",
      " 31  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2        17361 non-null  float64       \n",
      " 32  volume_time_multip_stage1_Dam                   17361 non-null  float64       \n",
      " 33  volume_time_multip_stage2_Dam                   17361 non-null  float64       \n",
      " 34  volume_time_multip_stage3_Dam                   17361 non-null  float64       \n",
      " 35  volume_time_multip_avg_Dam                      17361 non-null  float64       \n",
      " 36  volume_time_multip_stage1_Fill1                 17361 non-null  float64       \n",
      " 37  volume_time_multip_stage2_Fill1                 17361 non-null  float64       \n",
      " 38  volume_time_multip_stage3_Fill1                 17361 non-null  float64       \n",
      " 39  volume_time_multip_avg_Fill1                    17361 non-null  float64       \n",
      " 40  average_thickness_Dam                           17361 non-null  float64       \n",
      " 41  1st_pressure_time_AutoClave                     17361 non-null  float64       \n",
      " 42  2nd_pressure_time_AutoClave                     17361 non-null  float64       \n",
      " 43  3rd_pressure_time_AutoClave                     17361 non-null  float64       \n",
      " 44  avg_pressure_time_AutoClave                     17361 non-null  float64       \n",
      " 45  time_ratio_Dam                                  17361 non-null  float64       \n",
      " 46  time_ratio_Fill1                                17361 non-null  float64       \n",
      " 47  time_ratio_Fill2                                17361 non-null  float64       \n",
      " 48  time_ratio_AutoClave                            17361 non-null  float64       \n",
      " 49  time_gap_Dam                                    17361 non-null  int64         \n",
      " 50  time_gap_Fill1                                  17361 non-null  int64         \n",
      " 51  time_gap_Fill2                                  17361 non-null  int64         \n",
      " 52  time_gap_All                                    17361 non-null  int64         \n",
      " 53  model_suffix_encoded                            17361 non-null  float64       \n",
      " 54  cleaned_workorder_encoded                       17361 non-null  float64       \n",
      " 55  PalletID_Collect_Result_encoded                 17361 non-null  float64       \n",
      " 56  cure_end_position_XZ_Fill2_encoded              17361 non-null  float64       \n",
      " 57  cure_start_position_XZ_Fill2_encoded            17361 non-null  float64       \n",
      " 58  cure_standby_position_XZ_Fill2_encoded          17361 non-null  float64       \n",
      " 59  stage1_line13_distance_speed_Dam_encoded        17361 non-null  float64       \n",
      " 60  stage1_line24_distance_speed_Dam_encoded        17361 non-null  float64       \n",
      " 61  stage2_line13_distance_speed_Dam_encoded        17361 non-null  float64       \n",
      " 62  stage2_line24_distance_speed_Dam_encoded        17361 non-null  float64       \n",
      " 63  stage3_line13_distance_speed_Dam_encoded        17361 non-null  float64       \n",
      " 64  stage3_line24_distance_speed_Dam_encoded        17361 non-null  float64       \n",
      " 65  Production_Qty_Collect_Result_encoded           17361 non-null  float64       \n",
      "dtypes: datetime64[ns](4), float64(48), int64(13), object(1)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# info 잘리지 않게 출력\n",
    "test_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877efc6-7851-408a-99b4-a0f8bc9bcad1",
   "metadata": {},
   "source": [
    "### 13. correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "id": "ca33a0a4-7be6-4244-85ad-019d5807eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dam, fill1, fill2 공통 변수\n",
    "var_dam_fill = [\n",
    "    'Equipment_same_num',\n",
    "    'PalletID_Collect_Result_encoded',\n",
    "    'Production_Qty_Collect_Result_encoded',\n",
    "    'WorkMode Collect Result',\n",
    "    'Receip_n_suffix_3',\n",
    "    'time_gap_All'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "id": "45924c5c-f6be-4b17-87ff-9c414a6c8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 공통 변수\n",
    "### correlation 확인을 위한 변수 리스트\n",
    "var_all_corr = [\n",
    "    'model_suffix_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]\n",
    "\n",
    "### train\n",
    "var_all_train = [\n",
    "    'target',\n",
    "    'model_suffix_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]\n",
    "\n",
    "### test\n",
    "var_all_test = [\n",
    "    'Set ID',\n",
    "    'target',\n",
    "    'model_suffix_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b5986-ccde-4df4-bc02-c9812daeaaf4",
   "metadata": {},
   "source": [
    "- dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "id": "ccc357da-1f9b-45ca-a63c-66530ad718ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equipment_same_num',\n",
       " 'PalletID_Collect_Result_encoded',\n",
       " 'Production_Qty_Collect_Result_encoded',\n",
       " 'WorkMode Collect Result',\n",
       " 'Receip_n_suffix_3',\n",
       " 'time_gap_All',\n",
       " 'model_suffix_encoded',\n",
       " 'cleaned_workorder_encoded',\n",
       " 'Collect Date_Dam',\n",
       " 'CURE SPEED Collect Result_Dam',\n",
       " 'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
       " 'Head Clean Position Z Collect Result_Dam',\n",
       " 'Head Purge Position Z Collect Result_Dam',\n",
       " 'Head Zero Position Y Collect Result_Dam',\n",
       " 'Stage1_Circle_Distance_Speed_Dam',\n",
       " 'Stage2_Circle_Distance_Speed_Dam',\n",
       " 'Stage3_Circle_Distance_Speed_Dam',\n",
       " 'CURE_DISTANCE_Dam',\n",
       " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam',\n",
       " 'HEAD NORMAL DISTANCE_TRIANGLE_area_Dam',\n",
       " 'HEAD NORMAL DISTANCE_TRIANGLE_height_Dam',\n",
       " 'volume_time_multip_stage1_Dam',\n",
       " 'volume_time_multip_stage2_Dam',\n",
       " 'volume_time_multip_stage3_Dam',\n",
       " 'volume_time_multip_avg_Dam',\n",
       " 'average_thickness_Dam',\n",
       " 'time_ratio_Dam',\n",
       " 'time_gap_Dam',\n",
       " 'stage1_line13_distance_speed_Dam_encoded',\n",
       " 'stage1_line24_distance_speed_Dam_encoded',\n",
       " 'stage2_line13_distance_speed_Dam_encoded',\n",
       " 'stage2_line24_distance_speed_Dam_encoded',\n",
       " 'stage3_line13_distance_speed_Dam_encoded',\n",
       " 'stage3_line24_distance_speed_Dam_encoded']"
      ]
     },
     "execution_count": 1237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상관관계를 확인할 데이터셋\n",
    "combined_variables = var_dam_fill + var_all_corr + [var for var in train_data.columns if '_Dam' in var]\n",
    "combined_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "id": "8f3b4213-f3cd-4b1b-a443-33f0ef7c116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Equipment_same_num',\n",
    " 'PalletID_Collect_Result_encoded',\n",
    " 'Production_Qty_Collect_Result_encoded',\n",
    " 'WorkMode Collect Result',\n",
    " 'Receip_n_suffix_3',\n",
    " 'time_gap_All',\n",
    " 'CURE SPEED Collect Result_Dam',\n",
    " 'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
    " 'Head Clean Position Z Collect Result_Dam',\n",
    " 'Head Purge Position Z Collect Result_Dam',\n",
    " 'Head Zero Position Y Collect Result_Dam',\n",
    " #'Stage1_Circle_Distance_Speed_Dam',\n",
    " 'Stage2_Circle_Distance_Speed_Dam',\n",
    " #'Stage3_Circle_Distance_Speed_Dam',\n",
    " 'CURE_DISTANCE_Dam',\n",
    " #'HEAD NORMAL DISTANCE_TRIANGLE_area_Dam',\n",
    " 'HEAD NORMAL DISTANCE_TRIANGLE_height_Dam',\n",
    " 'volume_time_multip_stage1_Dam',\n",
    " 'volume_time_multip_stage2_Dam',\n",
    " #'volume_time_multip_stage3_Dam',\n",
    " #'volume_time_multip_avg_Dam',\n",
    " 'average_thickness_Dam',\n",
    " 'time_ratio_Dam',\n",
    "  'time_gap_Dam',\n",
    " #'stage1_line13_distance_speed_Dam_encoded',\n",
    " 'stage1_line24_distance_speed_Dam_encoded',\n",
    " #'stage2_line13_distance_speed_Dam_encoded',\n",
    " 'stage2_line24_distance_speed_Dam_encoded',\n",
    " #'stage3_line13_distance_speed_Dam_encoded',\n",
    " #'stage3_line24_distance_speed_Dam_encoded',\n",
    " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam',\n",
    " 'time_gap_Dam'\n",
    " ]\n",
    "\n",
    "# 변수들로만 이루어진 DataFrame 생성\n",
    "filtered_data = train_data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "id": "191577f3-3bf0-40c3-a84c-17a36794e491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Variable 1, Variable 2, Correlation]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 자기자신을 제외하고 상관관계 절댓값이 0.9 이상인 조합 찾기\n",
    "correlation_matrix = filtered_data.corr()\n",
    "strong_correlations = correlation_matrix[(correlation_matrix.abs() >= 0.9) & (correlation_matrix != 1)]\n",
    "\n",
    "# 리스트로 변환\n",
    "strong_correlations_pairs = strong_correlations.stack().reset_index()\n",
    "strong_correlations_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "# 결과 출력\n",
    "print(strong_correlations_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "id": "e46e96d2-e4ce-4700-accf-ee19b5c8a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드랍할 열 목록\n",
    "columns_to_drop = [\n",
    "    'Stage1_Circle_Distance_Speed_Dam',\n",
    "    'Stage3_Circle_Distance_Speed_Dam',\n",
    "    'HEAD NORMAL DISTANCE_TRIANGLE_area_Dam',\n",
    "    'stage1_line24_distance_speed_Dam_encoded',\n",
    "    'stage2_line13_distance_speed_Dam_encoded',\n",
    "    'stage3_line13_distance_speed_Dam_encoded',\n",
    "    'stage3_line24_distance_speed_Dam_encoded',\n",
    "    'volume_time_multip_stage3_Dam',\n",
    "    'volume_time_multip_avg_Dam'\n",
    "]\n",
    "\n",
    "# 열 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "id": "546c8eb3-b124-4668-a421-3d80a4614070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Dam'을 포함하는 변수 선택\n",
    "dam_variables = [var for var in train_data.columns if '_Dam' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + dam_variables\n",
    "train_data_dam = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + dam_variables\n",
    "test_data_dam = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "id": "06ebb3fc-5dca-467b-a339-10dc195f6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 26 columns):\n",
      " #   Column                                        Non-Null Count  Dtype         \n",
      "---  ------                                        --------------  -----         \n",
      " 0   Equipment_same_num                            40506 non-null  int64         \n",
      " 1   PalletID_Collect_Result_encoded               40506 non-null  float64       \n",
      " 2   Production_Qty_Collect_Result_encoded         40506 non-null  float64       \n",
      " 3   WorkMode Collect Result                       40506 non-null  float64       \n",
      " 4   Receip_n_suffix_3                             40506 non-null  int64         \n",
      " 5   time_gap_All                                  40506 non-null  int64         \n",
      " 6   target                                        40506 non-null  object        \n",
      " 7   model_suffix_encoded                          40506 non-null  float64       \n",
      " 8   cleaned_workorder_encoded                     40506 non-null  float64       \n",
      " 9   Collect Date_Dam                              40506 non-null  datetime64[ns]\n",
      " 10  CURE SPEED Collect Result_Dam                 40506 non-null  int64         \n",
      " 11  DISCHARGED SPEED OF RESIN Collect Result_Dam  40506 non-null  int64         \n",
      " 12  Head Clean Position Z Collect Result_Dam      40506 non-null  float64       \n",
      " 13  Head Purge Position Z Collect Result_Dam      40506 non-null  float64       \n",
      " 14  Head Zero Position Y Collect Result_Dam       40506 non-null  float64       \n",
      " 15  Stage2_Circle_Distance_Speed_Dam              40506 non-null  int64         \n",
      " 16  CURE_DISTANCE_Dam                             40506 non-null  float64       \n",
      " 17  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam        40506 non-null  float64       \n",
      " 18  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam      40506 non-null  float64       \n",
      " 19  volume_time_multip_stage1_Dam                 40506 non-null  float64       \n",
      " 20  volume_time_multip_stage2_Dam                 40506 non-null  float64       \n",
      " 21  average_thickness_Dam                         40506 non-null  float64       \n",
      " 22  time_ratio_Dam                                40506 non-null  float64       \n",
      " 23  time_gap_Dam                                  40506 non-null  int64         \n",
      " 24  stage1_line13_distance_speed_Dam_encoded      40506 non-null  float64       \n",
      " 25  stage2_line24_distance_speed_Dam_encoded      40506 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(17), int64(7), object(1)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_dam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "id": "55af86b5-3648-403e-a628-5f9338d2b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                        Non-Null Count  Dtype         \n",
      "---  ------                                        --------------  -----         \n",
      " 0   Equipment_same_num                            17361 non-null  int64         \n",
      " 1   PalletID_Collect_Result_encoded               17361 non-null  float64       \n",
      " 2   Production_Qty_Collect_Result_encoded         17361 non-null  float64       \n",
      " 3   WorkMode Collect Result                       17361 non-null  float64       \n",
      " 4   Receip_n_suffix_3                             17361 non-null  int64         \n",
      " 5   time_gap_All                                  17361 non-null  int64         \n",
      " 6   Set ID                                        17361 non-null  object        \n",
      " 7   target                                        0 non-null      float64       \n",
      " 8   model_suffix_encoded                          17361 non-null  float64       \n",
      " 9   cleaned_workorder_encoded                     17361 non-null  float64       \n",
      " 10  Collect Date_Dam                              17361 non-null  datetime64[ns]\n",
      " 11  CURE SPEED Collect Result_Dam                 17361 non-null  int64         \n",
      " 12  DISCHARGED SPEED OF RESIN Collect Result_Dam  17361 non-null  int64         \n",
      " 13  Head Clean Position Z Collect Result_Dam      17361 non-null  float64       \n",
      " 14  Head Purge Position Z Collect Result_Dam      17361 non-null  float64       \n",
      " 15  Head Zero Position Y Collect Result_Dam       17361 non-null  float64       \n",
      " 16  Stage2_Circle_Distance_Speed_Dam              17361 non-null  int64         \n",
      " 17  CURE_DISTANCE_Dam                             17361 non-null  float64       \n",
      " 18  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam        17361 non-null  float64       \n",
      " 19  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam      17361 non-null  float64       \n",
      " 20  volume_time_multip_stage1_Dam                 17361 non-null  float64       \n",
      " 21  volume_time_multip_stage2_Dam                 17361 non-null  float64       \n",
      " 22  average_thickness_Dam                         17361 non-null  float64       \n",
      " 23  time_ratio_Dam                                17361 non-null  float64       \n",
      " 24  time_gap_Dam                                  17361 non-null  int64         \n",
      " 25  stage1_line13_distance_speed_Dam_encoded      17361 non-null  float64       \n",
      " 26  stage2_line24_distance_speed_Dam_encoded      17361 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(18), int64(7), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data_dam.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7ec95-7c4b-4dd3-a4ee-071014866652",
   "metadata": {},
   "source": [
    "- fill1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "id": "954ac003-7985-46ce-bfda-ab7faf341348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equipment_same_num',\n",
       " 'PalletID_Collect_Result_encoded',\n",
       " 'Production_Qty_Collect_Result_encoded',\n",
       " 'WorkMode Collect Result',\n",
       " 'Receip_n_suffix_3',\n",
       " 'time_gap_All',\n",
       " 'model_suffix_encoded',\n",
       " 'cleaned_workorder_encoded',\n",
       " 'Collect Date_Fill1',\n",
       " 'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
       " 'Head Purge Position Z Collect Result_Fill1',\n",
       " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1',\n",
       " 'HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1',\n",
       " 'HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1',\n",
       " 'volume_time_multip_stage1_Fill1',\n",
       " 'volume_time_multip_stage2_Fill1',\n",
       " 'volume_time_multip_stage3_Fill1',\n",
       " 'volume_time_multip_avg_Fill1',\n",
       " 'time_ratio_Fill1',\n",
       " 'time_gap_Fill1']"
      ]
     },
     "execution_count": 1244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상관관계를 확인할 데이터셋\n",
    "combined_variables = var_dam_fill + var_all_corr + [var for var in train_data.columns if '_Fill1' in var]\n",
    "combined_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "id": "12fb0b0a-ebf3-42a5-9d69-8032db061a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Equipment_same_num',\n",
    " 'PalletID_Collect_Result_encoded',\n",
    " 'Production_Qty_Collect_Result_encoded',\n",
    " 'WorkMode Collect Result',\n",
    " 'Receip_n_suffix_3',\n",
    " 'time_gap_All',\n",
    " 'model_suffix_encoded',\n",
    " 'cleaned_workorder_encoded',\n",
    " 'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
    " 'Head Purge Position Z Collect Result_Fill1',\n",
    " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1',\n",
    " #'HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1',\n",
    " 'HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1',\n",
    " #'volume_time_multip_avg_Fill1',\n",
    " 'volume_time_multip_stage1_Fill1',\n",
    " #'volume_time_multip_stage2_Fill1',\n",
    " #'volume_time_multip_stage3_Fill1',\n",
    " 'time_ratio_Fill1',\n",
    "  'time_gap_Fill1']\n",
    "\n",
    "# 변수들로만 이루어진 DataFrame 생성\n",
    "filtered_data = train_data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "id": "99c6f69f-b57e-4c54-a42b-7f81a9fb3e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Variable 1, Variable 2, Correlation]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 자기자신을 제외하고 상관관계 절댓값이 0.9 이상인 조합 찾기\n",
    "correlation_matrix = filtered_data.corr()\n",
    "strong_correlations = correlation_matrix[(correlation_matrix.abs() >= 0.9) & (correlation_matrix != 1)]\n",
    "\n",
    "# 리스트로 변환\n",
    "strong_correlations_pairs = strong_correlations.stack().reset_index()\n",
    "strong_correlations_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "# 결과 출력\n",
    "print(strong_correlations_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "id": "d308bfe8-5f8d-4943-b8b7-2476ac8977d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드랍할 열 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL DISTANCE_TRIANGLE_area_Fill1',\n",
    "    'volume_time_multip_avg_Fill1',\n",
    "    'volume_time_multip_stage2_Fill1',\n",
    "    'volume_time_multip_stage3_Fill1'\n",
    "]\n",
    "\n",
    "# 열 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "id": "10e0b1bb-f000-49c2-8432-d595149766a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill1'을 포함하는 변수 선택\n",
    "fill1_variables = [var for var in train_data.columns if '_Fill1' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill1_variables\n",
    "train_data_fill1 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill1_variables\n",
    "test_data_fill1 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "id": "fface952-5ca0-4d5b-9184-71de30e02084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                          Non-Null Count  Dtype         \n",
      "---  ------                                          --------------  -----         \n",
      " 0   Equipment_same_num                              40506 non-null  int64         \n",
      " 1   PalletID_Collect_Result_encoded                 40506 non-null  float64       \n",
      " 2   Production_Qty_Collect_Result_encoded           40506 non-null  float64       \n",
      " 3   WorkMode Collect Result                         40506 non-null  float64       \n",
      " 4   Receip_n_suffix_3                               40506 non-null  int64         \n",
      " 5   time_gap_All                                    40506 non-null  int64         \n",
      " 6   target                                          40506 non-null  object        \n",
      " 7   model_suffix_encoded                            40506 non-null  float64       \n",
      " 8   cleaned_workorder_encoded                       40506 non-null  float64       \n",
      " 9   Collect Date_Fill1                              40506 non-null  datetime64[ns]\n",
      " 10  DISCHARGED SPEED OF RESIN Collect Result_Fill1  40506 non-null  float64       \n",
      " 11  Head Purge Position Z Collect Result_Fill1      40506 non-null  float64       \n",
      " 12  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        40506 non-null  float64       \n",
      " 13  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      40506 non-null  float64       \n",
      " 14  volume_time_multip_stage1_Fill1                 40506 non-null  float64       \n",
      " 15  time_ratio_Fill1                                40506 non-null  float64       \n",
      " 16  time_gap_Fill1                                  40506 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(11), int64(4), object(1)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_fill1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "id": "55dedb1b-a829-4dc4-aaf7-960a5e66725d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                          Non-Null Count  Dtype         \n",
      "---  ------                                          --------------  -----         \n",
      " 0   Equipment_same_num                              17361 non-null  int64         \n",
      " 1   PalletID_Collect_Result_encoded                 17361 non-null  float64       \n",
      " 2   Production_Qty_Collect_Result_encoded           17361 non-null  float64       \n",
      " 3   WorkMode Collect Result                         17361 non-null  float64       \n",
      " 4   Receip_n_suffix_3                               17361 non-null  int64         \n",
      " 5   time_gap_All                                    17361 non-null  int64         \n",
      " 6   Set ID                                          17361 non-null  object        \n",
      " 7   target                                          0 non-null      float64       \n",
      " 8   model_suffix_encoded                            17361 non-null  float64       \n",
      " 9   cleaned_workorder_encoded                       17361 non-null  float64       \n",
      " 10  Collect Date_Fill1                              17361 non-null  datetime64[ns]\n",
      " 11  DISCHARGED SPEED OF RESIN Collect Result_Fill1  17361 non-null  float64       \n",
      " 12  Head Purge Position Z Collect Result_Fill1      17361 non-null  float64       \n",
      " 13  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill1        17361 non-null  float64       \n",
      " 14  HEAD NORMAL DISTANCE_TRIANGLE_height_Fill1      17361 non-null  float64       \n",
      " 15  volume_time_multip_stage1_Fill1                 17361 non-null  float64       \n",
      " 16  time_ratio_Fill1                                17361 non-null  float64       \n",
      " 17  time_gap_Fill1                                  17361 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(12), int64(4), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data_fill1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9feaf-2d9e-48e6-ac73-0b2643b5f1ad",
   "metadata": {},
   "source": [
    "- fill2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "id": "24b711c3-9126-4c06-bf22-a2d9a6c6f4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equipment_same_num',\n",
       " 'PalletID_Collect_Result_encoded',\n",
       " 'Production_Qty_Collect_Result_encoded',\n",
       " 'WorkMode Collect Result',\n",
       " 'Receip_n_suffix_3',\n",
       " 'time_gap_All',\n",
       " 'model_suffix_encoded',\n",
       " 'cleaned_workorder_encoded',\n",
       " 'Collect Date_Fill2',\n",
       " 'CURE SPEED Collect Result_Fill2',\n",
       " 'Head Purge Position Z Collect Result_Fill2',\n",
       " 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2',\n",
       " 'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2',\n",
       " 'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2',\n",
       " 'time_ratio_Fill2',\n",
       " 'time_gap_Fill2',\n",
       " 'cure_end_position_XZ_Fill2_encoded',\n",
       " 'cure_start_position_XZ_Fill2_encoded',\n",
       " 'cure_standby_position_XZ_Fill2_encoded']"
      ]
     },
     "execution_count": 1251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상관관계를 확인할 데이터셋\n",
    "combined_variables = var_dam_fill + var_all_corr + [var for var in train_data.columns if '_Fill2' in var]\n",
    "combined_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "id": "724c38f3-162a-40b8-9d89-16e60a4352a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'Equipment_same_num',\n",
    " 'PalletID_Collect_Result_encoded',\n",
    " 'Production_Qty_Collect_Result_encoded',\n",
    " 'WorkMode Collect Result',\n",
    " 'Receip_n_suffix_3',\n",
    " 'time_gap_All',\n",
    " 'model_suffix_encoded',\n",
    " 'cleaned_workorder_encoded',\n",
    " 'CURE SPEED Collect Result_Fill2',\n",
    " 'Head Purge Position Z Collect Result_Fill2',\n",
    " 'HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2',\n",
    " #'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2',\n",
    " #'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2',\n",
    " 'time_ratio_Fill2',\n",
    " 'time_gap_Fill2',\n",
    " 'cure_end_position_XZ_Fill2_encoded',\n",
    " 'cure_start_position_XZ_Fill2_encoded']\n",
    " #'cure_standby_position_XZ_Fill2_encoded']\n",
    "\n",
    "# 변수들로만 이루어진 DataFrame 생성\n",
    "filtered_data = train_data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "id": "d073348d-b1f7-4c87-b50b-29c2f134ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Variable 1, Variable 2, Correlation]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 자기자신을 제외하고 상관관계 절댓값이 0.9 이상인 조합 찾기\n",
    "correlation_matrix = filtered_data.corr()\n",
    "strong_correlations = correlation_matrix[(correlation_matrix.abs() >= 0.9) & (correlation_matrix != 1)]\n",
    "\n",
    "# 리스트로 변환\n",
    "strong_correlations_pairs = strong_correlations.stack().reset_index()\n",
    "strong_correlations_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "# 결과 출력\n",
    "print(strong_correlations_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "id": "fa5ccf89-2670-4213-a96e-8df0f8696147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드랍할 열 목록\n",
    "columns_to_drop = [\n",
    "    'HEAD NORMAL DISTANCE_STAGE2_STAGE3_Fill2',\n",
    "    'HEAD NORMAL DISTANCE_STAGE1_STAGE3_Fill2',\n",
    "    'cure_standby_position_XZ_Fill2_encoded'\n",
    "]\n",
    "\n",
    "# 열 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "id": "52952fee-4bfa-4e5a-a2af-c78c32ccd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill2'을 포함하는 변수 선택\n",
    "fill2_variables = [var for var in train_data.columns if '_Fill2' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill2_variables\n",
    "train_data_fill2 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill2_variables\n",
    "test_data_fill2 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "id": "e7764523-db87-4736-b96c-cbab7cce7e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                      Non-Null Count  Dtype         \n",
      "---  ------                                      --------------  -----         \n",
      " 0   Equipment_same_num                          40506 non-null  int64         \n",
      " 1   PalletID_Collect_Result_encoded             40506 non-null  float64       \n",
      " 2   Production_Qty_Collect_Result_encoded       40506 non-null  float64       \n",
      " 3   WorkMode Collect Result                     40506 non-null  float64       \n",
      " 4   Receip_n_suffix_3                           40506 non-null  int64         \n",
      " 5   time_gap_All                                40506 non-null  int64         \n",
      " 6   target                                      40506 non-null  object        \n",
      " 7   model_suffix_encoded                        40506 non-null  float64       \n",
      " 8   cleaned_workorder_encoded                   40506 non-null  float64       \n",
      " 9   Collect Date_Fill2                          40506 non-null  datetime64[ns]\n",
      " 10  CURE SPEED Collect Result_Fill2             40506 non-null  int64         \n",
      " 11  Head Purge Position Z Collect Result_Fill2  40506 non-null  float64       \n",
      " 12  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2    40506 non-null  float64       \n",
      " 13  time_ratio_Fill2                            40506 non-null  float64       \n",
      " 14  time_gap_Fill2                              40506 non-null  int64         \n",
      " 15  cure_end_position_XZ_Fill2_encoded          40506 non-null  float64       \n",
      " 16  cure_start_position_XZ_Fill2_encoded        40506 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(10), int64(5), object(1)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_fill2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "id": "7513b854-2823-4494-a669-fa552fdfe5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17361 entries, 0 to 17360\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                      Non-Null Count  Dtype         \n",
      "---  ------                                      --------------  -----         \n",
      " 0   Equipment_same_num                          17361 non-null  int64         \n",
      " 1   PalletID_Collect_Result_encoded             17361 non-null  float64       \n",
      " 2   Production_Qty_Collect_Result_encoded       17361 non-null  float64       \n",
      " 3   WorkMode Collect Result                     17361 non-null  float64       \n",
      " 4   Receip_n_suffix_3                           17361 non-null  int64         \n",
      " 5   time_gap_All                                17361 non-null  int64         \n",
      " 6   Set ID                                      17361 non-null  object        \n",
      " 7   target                                      0 non-null      float64       \n",
      " 8   model_suffix_encoded                        17361 non-null  float64       \n",
      " 9   cleaned_workorder_encoded                   17361 non-null  float64       \n",
      " 10  Collect Date_Fill2                          17361 non-null  datetime64[ns]\n",
      " 11  CURE SPEED Collect Result_Fill2             17361 non-null  int64         \n",
      " 12  Head Purge Position Z Collect Result_Fill2  17361 non-null  float64       \n",
      " 13  HEAD NORMAL DISTANCE_STAGE1_STAGE2_Fill2    17361 non-null  float64       \n",
      " 14  time_ratio_Fill2                            17361 non-null  float64       \n",
      " 15  time_gap_Fill2                              17361 non-null  int64         \n",
      " 16  cure_end_position_XZ_Fill2_encoded          17361 non-null  float64       \n",
      " 17  cure_start_position_XZ_Fill2_encoded        17361 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(11), int64(5), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data_fill2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8510d04-48c5-42c2-9a0c-10532decda68",
   "metadata": {},
   "source": [
    "- autoclave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1258,
   "id": "d43c5ca3-0bff-4ee1-b35d-488abb63f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_suffix_encoded',\n",
       " 'cleaned_workorder_encoded',\n",
       " 'Collect Date_AutoClave',\n",
       " 'Chamber Temp. Collect Result_AutoClave',\n",
       " '1st_pressure_time_AutoClave',\n",
       " '2nd_pressure_time_AutoClave',\n",
       " '3rd_pressure_time_AutoClave',\n",
       " 'avg_pressure_time_AutoClave',\n",
       " 'time_ratio_AutoClave']"
      ]
     },
     "execution_count": 1258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상관관계를 확인할 데이터셋\n",
    "combined_variables = var_all_corr + [var for var in train_data.columns if '_AutoClave' in var]\n",
    "combined_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "id": "a2bc07fd-d3f7-4fae-9794-6851cad4d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['model_suffix_encoded',\n",
    " 'cleaned_workorder_encoded',\n",
    " 'Chamber Temp. Collect Result_AutoClave',\n",
    " '1st_pressure_time_AutoClave',\n",
    " '2nd_pressure_time_AutoClave',\n",
    " '3rd_pressure_time_AutoClave',\n",
    " #'avg_pressure_time_AutoClave',\n",
    " 'time_ratio_AutoClave']\n",
    "\n",
    "# 변수들로만 이루어진 DataFrame 생성\n",
    "filtered_data = train_data[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "id": "54d4680e-9f74-40dc-b876-5c5cd3f275cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Variable 1, Variable 2, Correlation]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 자기자신을 제외하고 상관관계 절댓값이 0.9 이상인 조합 찾기\n",
    "correlation_matrix = filtered_data.corr()\n",
    "strong_correlations = correlation_matrix[(correlation_matrix.abs() >= 0.9) & (correlation_matrix != 1)]\n",
    "\n",
    "# 리스트로 변환\n",
    "strong_correlations_pairs = strong_correlations.stack().reset_index()\n",
    "strong_correlations_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "# 결과 출력\n",
    "print(strong_correlations_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "id": "1bacd287-b6e1-445a-9aee-91a671ff689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드랍할 열 목록\n",
    "columns_to_drop = ['avg_pressure_time_AutoClave']\n",
    "\n",
    "# 열 삭제\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "id": "1eaf9215-077b-48c3-960b-e529c7d4fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_AutoClave'을 포함하는 변수 선택\n",
    "autoclave_variables = [var for var in train_data.columns if '_AutoClave' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_all_train + autoclave_variables\n",
    "train_data_autoclave = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_all_test + autoclave_variables\n",
    "test_data_autoclave = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "id": "1b807fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105 entries, 0 to 104\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   day                                       105 non-null    int64  \n",
      " 1   start_time                                105 non-null    object \n",
      " 2   end_time                                  105 non-null    object \n",
      " 3   Dam Thickness - 1time / day               81 non-null     float64\n",
      " 4   Fill thickness - 1time / day              81 non-null     float64\n",
      " 5   Fill cure energy (mJ) - Every break time  81 non-null     float64\n",
      " 6   Fill cure power (mW) - Every break time   81 non-null     float64\n",
      " 7   Line Temp. - Every break time             81 non-null     float64\n",
      " 8   Line humidity - Every break time          81 non-null     float64\n",
      " 9   OCR gap - Line#1 stage1                   81 non-null     float64\n",
      " 10  OCR gap - Line#1 stage2                   81 non-null     float64\n",
      " 11  OCR gap - Line#1 stage3                   81 non-null     float64\n",
      " 12  OCR gap - Line #2 stage1                  81 non-null     float64\n",
      " 13  OCR gap - Line #2 stage2                  81 non-null     float64\n",
      " 14  OCR gap - Line #2 stage3                  81 non-null     float64\n",
      "dtypes: float64(12), int64(1), object(2)\n",
      "memory usage: 12.4+ KB\n"
     ]
    }
   ],
   "source": [
    "plus_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "id": "8a94c557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      240319\n",
       "1      240319\n",
       "2      240319\n",
       "3      240319\n",
       "4      240319\n",
       "        ...  \n",
       "100    240428\n",
       "101    240428\n",
       "102    240428\n",
       "103    240428\n",
       "104    240428\n",
       "Name: day, Length: 105, dtype: int64"
      ]
     },
     "execution_count": 1264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_data['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "id": "ef3177ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      08:00:00\n",
       "1      10:45:00\n",
       "2      13:00:00\n",
       "3      15:15:00\n",
       "4      18:30:00\n",
       "         ...   \n",
       "100    08:00:00\n",
       "101    10:45:00\n",
       "102    13:00:00\n",
       "103    15:15:00\n",
       "104    18:30:00\n",
       "Name: start_time, Length: 105, dtype: object"
      ]
     },
     "execution_count": 1265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_data['start_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "id": "38c422ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    080000\n",
      "1    104500\n",
      "2    130000\n",
      "3    151500\n",
      "4    183000\n",
      "Name: start_time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'start_time' 열을 datetime 형식으로 변환하고 'HHMMSS' 형식으로 저장\n",
    "plus_data['start_time'] = pd.to_datetime(plus_data['start_time'], format='%H:%M:%S', errors='coerce').dt.strftime('%H%M%S')\n",
    "\n",
    "# 결과 확인\n",
    "print(plus_data['start_time'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "id": "80c48c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    103000\n",
      "1    120000\n",
      "2    150000\n",
      "3    180000\n",
      "4    200000\n",
      "Name: end_time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'start_time' 열을 datetime 형식으로 변환하고 'HHMMSS' 형식으로 저장\n",
    "plus_data['end_time'] = pd.to_datetime(plus_data['end_time'], format='%H:%M:%S', errors='coerce').dt.strftime('%H%M%S')\n",
    "\n",
    "# 결과 확인\n",
    "print(plus_data['end_time'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "id": "4559bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juneh\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\juneh\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Collect Date_Dam     day    time\n",
      "0 2024-04-25 11:10:00  240425  111000\n",
      "1 2023-09-19 14:30:00  230919  143000\n",
      "2 2024-03-05 09:30:00  240305  093000\n",
      "3 2023-09-25 15:40:00  230925  154000\n",
      "4 2023-06-27 13:20:00  230627  132000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juneh\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'Collect Date_Dam' 열을 datetime 형식으로 변환\n",
    "train_data_dam['Collect Date_Dam'] = pd.to_datetime(train_data_dam['Collect Date_Dam'], errors='coerce')\n",
    "\n",
    "# 'Collect Date_Dam' 열에서 날짜와 시간을 분리하여 각각 'day'와 'time' 열로 저장\n",
    "train_data_dam['day'] = train_data_dam['Collect Date_Dam'].dt.strftime('%y%m%d')\n",
    "train_data_dam['time'] = train_data_dam['Collect Date_Dam'].dt.strftime('%H%M%S')\n",
    "\n",
    "# 결과 확인\n",
    "print(train_data_dam[['Collect Date_Dam', 'day', 'time']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "id": "3c8055a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juneh\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data_dam['day'] = train_data_dam['day'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "id": "8da136c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juneh\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data_dam['time'] = train_data_dam['time'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "id": "8372aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_data['start_time'] = plus_data['start_time'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "id": "84cac147",
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_data['end_time'] = plus_data['end_time'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "id": "185ef0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105 entries, 0 to 104\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   day                                       105 non-null    int64  \n",
      " 1   start_time                                105 non-null    int32  \n",
      " 2   end_time                                  105 non-null    int32  \n",
      " 3   Dam Thickness - 1time / day               81 non-null     float64\n",
      " 4   Fill thickness - 1time / day              81 non-null     float64\n",
      " 5   Fill cure energy (mJ) - Every break time  81 non-null     float64\n",
      " 6   Fill cure power (mW) - Every break time   81 non-null     float64\n",
      " 7   Line Temp. - Every break time             81 non-null     float64\n",
      " 8   Line humidity - Every break time          81 non-null     float64\n",
      " 9   OCR gap - Line#1 stage1                   81 non-null     float64\n",
      " 10  OCR gap - Line#1 stage2                   81 non-null     float64\n",
      " 11  OCR gap - Line#1 stage3                   81 non-null     float64\n",
      " 12  OCR gap - Line #2 stage1                  81 non-null     float64\n",
      " 13  OCR gap - Line #2 stage2                  81 non-null     float64\n",
      " 14  OCR gap - Line #2 stage3                  81 non-null     float64\n",
      "dtypes: float64(12), int32(2), int64(1)\n",
      "memory usage: 11.6 KB\n"
     ]
    }
   ],
   "source": [
    "plus_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "id": "a62d20ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       80000\n",
       "1      104500\n",
       "2      130000\n",
       "3      151500\n",
       "4      183000\n",
       "        ...  \n",
       "100     80000\n",
       "101    104500\n",
       "102    130000\n",
       "103    151500\n",
       "104    183000\n",
       "Name: start_time, Length: 105, dtype: int32"
      ]
     },
     "execution_count": 1274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_data['start_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "id": "5a76de67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['day'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22572\\1449663865.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 각 행에 대해 check_and_concat 함수 적용\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_dam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_and_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 결과를 하나의 데이터프레임으로 결합\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8739\u001b[0m         )\n\u001b[1;32m-> 8740\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22572\\1449663865.py\u001b[0m in \u001b[0;36mcheck_and_concat\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      5\u001b[0m                               (plus_data['end_time'] >= row['time'])]\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatching_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   9098\u001b[0m         \"\"\"\n\u001b[0;32m   9099\u001b[0m         return self._join_compat(\n\u001b[1;32m-> 9100\u001b[1;33m             \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9101\u001b[0m         )\n\u001b[0;32m   9102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   9136\u001b[0m                 \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9137\u001b[0m                 \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9138\u001b[1;33m                 \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9139\u001b[0m             )\n\u001b[0;32m   9140\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     )\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m         llabels, rlabels = _items_overlap_with_suffix(\n\u001b[1;32m--> 718\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m         )\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2314\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"columns overlap but no suffix specified: {to_rename}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrenamer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['day'], dtype='object')"
     ]
    }
   ],
   "source": [
    "# train_data_dam의 각 행에 대해 plus_data의 day와 time 구간에 해당하는 값을 확인\n",
    "def check_and_concat(row):\n",
    "    matching_rows = plus_data[(plus_data['day'] == row['day']) & \n",
    "                              (plus_data['start_time'] <= row['time']) & \n",
    "                              (plus_data['end_time'] >= row['time'])]\n",
    "    if not matching_rows.empty:\n",
    "        return pd.concat([row.to_frame().T] * len(matching_rows), axis=1).join(matching_rows.reset_index(drop=True))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# 각 행에 대해 check_and_concat 함수 적용\n",
    "result = train_data_dam.apply(check_and_concat, axis=1)\n",
    "\n",
    "# 결과를 하나의 데이터프레임으로 결합\n",
    "final_result = pd.concat(result.tolist(), ignore_index=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "id": "94067c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240319    5\n",
       "240405    5\n",
       "240425    5\n",
       "240423    5\n",
       "240419    5\n",
       "240417    5\n",
       "240411    5\n",
       "240410    5\n",
       "240409    5\n",
       "240406    5\n",
       "240404    5\n",
       "240320    5\n",
       "240402    5\n",
       "240401    5\n",
       "240330    5\n",
       "240329    5\n",
       "240328    5\n",
       "240327    5\n",
       "240326    5\n",
       "240321    5\n",
       "240428    5\n",
       "Name: day, dtype: int64"
      ]
     },
     "execution_count": 1161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_data['day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "id": "8231ed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231220    407\n",
       "231218    399\n",
       "240106    377\n",
       "231219    368\n",
       "240108    368\n",
       "         ... \n",
       "231026     13\n",
       "240416      9\n",
       "230824      6\n",
       "230504      4\n",
       "230815      3\n",
       "Name: day, Length: 210, dtype: int64"
      ]
     },
     "execution_count": 1162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_dam['day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "56a93ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        240425\n",
       "1        230919\n",
       "2        240305\n",
       "3        230925\n",
       "4        230627\n",
       "          ...  \n",
       "40501    230914\n",
       "40502    240410\n",
       "40503    240222\n",
       "40504    230725\n",
       "40505    230531\n",
       "Name: day, Length: 40506, dtype: int32"
      ]
     },
     "execution_count": 1163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_dam['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "id": "7db70c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plus_data 그룹화 결과:\n",
      "day\n",
      "240319    5\n",
      "240320    5\n",
      "240321    5\n",
      "240326    5\n",
      "240327    5\n",
      "240328    5\n",
      "240329    5\n",
      "240330    5\n",
      "240401    5\n",
      "240402    5\n",
      "240404    5\n",
      "240405    5\n",
      "240406    5\n",
      "240409    5\n",
      "240410    5\n",
      "240411    5\n",
      "240417    5\n",
      "240419    5\n",
      "240423    5\n",
      "240425    5\n",
      "240428    5\n",
      "dtype: int64\n",
      "\n",
      "train_data_dam 그룹화 결과:\n",
      "day\n",
      "230504      4\n",
      "230505    102\n",
      "230506    147\n",
      "230510    140\n",
      "230511    170\n",
      "         ... \n",
      "240422    158\n",
      "240423     59\n",
      "240424     20\n",
      "240425    157\n",
      "240428    281\n",
      "Length: 210, dtype: int64\n",
      "\n",
      "같은 값이 있는 day:\n",
      "{240401, 240402, 240404, 240405, 240406, 240409, 240410, 240411, 240417, 240419, 240423, 240425, 240428, 240319, 240320, 240321, 240326, 240327, 240328, 240329, 240330}\n"
     ]
    }
   ],
   "source": [
    "# plus_data['day']와 train_data_dam['day']로 그룹화하여 출력\n",
    "plus_data_grouped = plus_data.groupby('day').size()\n",
    "train_data_dam_grouped = train_data_dam.groupby('day').size()\n",
    "\n",
    "print(\"plus_data 그룹화 결과:\")\n",
    "print(plus_data_grouped)\n",
    "\n",
    "print(\"\\ntrain_data_dam 그룹화 결과:\")\n",
    "print(train_data_dam_grouped)\n",
    "\n",
    "# 같은 값이 있는지 비교\n",
    "common_days = set(plus_data['day']).intersection(set(train_data_dam['day']))\n",
    "\n",
    "print(\"\\n같은 값이 있는 day:\")\n",
    "print(common_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "eb07fd90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['day'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22572\\3874059593.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 각 행에 대해 check_and_concat 함수 적용\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_dam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_and_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 결과를 하나의 데이터프레임으로 결합\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8739\u001b[0m         )\n\u001b[1;32m-> 8740\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22572\\3874059593.py\u001b[0m in \u001b[0;36mcheck_and_concat\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmatching_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatching_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   9098\u001b[0m         \"\"\"\n\u001b[0;32m   9099\u001b[0m         return self._join_compat(\n\u001b[1;32m-> 9100\u001b[1;33m             \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9101\u001b[0m         )\n\u001b[0;32m   9102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   9136\u001b[0m                 \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9137\u001b[0m                 \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9138\u001b[1;33m                 \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9139\u001b[0m             )\n\u001b[0;32m   9140\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     )\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m         llabels, rlabels = _items_overlap_with_suffix(\n\u001b[1;32m--> 718\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m         )\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2314\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"columns overlap but no suffix specified: {to_rename}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrenamer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['day'], dtype='object')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train_data_dam의 각 행에 대해 plus_data의 day와 time 구간에 해당하는 값을 확인\n",
    "def check_and_concat(row):\n",
    "    matching_rows = plus_data[(plus_data['day'] == row['day']) ]\n",
    "    if not matching_rows.empty:\n",
    "        return pd.concat([row.to_frame().T] * len(matching_rows), axis=1).join(matching_rows.reset_index(drop=True))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# 각 행에 대해 check_and_concat 함수 적용\n",
    "result = train_data_dam.apply(check_and_concat, axis=1)\n",
    "\n",
    "# 결과를 하나의 데이터프레임으로 결합\n",
    "final_result = pd.concat(result.tolist(), ignore_index=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "83ba2a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105 entries, 0 to 104\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                    Non-Null Count  Dtype         \n",
      "---  ------                                    --------------  -----         \n",
      " 0   day                                       105 non-null    datetime64[ns]\n",
      " 1   start_time                                105 non-null    object        \n",
      " 2   end_time                                  105 non-null    object        \n",
      " 3   Dam Thickness - 1time / day               81 non-null     float64       \n",
      " 4   Fill thickness - 1time / day              81 non-null     float64       \n",
      " 5   Fill cure energy (mJ) - Every break time  81 non-null     float64       \n",
      " 6   Fill cure power (mW) - Every break time   81 non-null     float64       \n",
      " 7   Line Temp. - Every break time             81 non-null     float64       \n",
      " 8   Line humidity - Every break time          81 non-null     float64       \n",
      " 9   OCR gap - Line#1 stage1                   81 non-null     float64       \n",
      " 10  OCR gap - Line#1 stage2                   81 non-null     float64       \n",
      " 11  OCR gap - Line#1 stage3                   81 non-null     float64       \n",
      " 12  OCR gap - Line #2 stage1                  81 non-null     float64       \n",
      " 13  OCR gap - Line #2 stage2                  81 non-null     float64       \n",
      " 14  OCR gap - Line #2 stage3                  81 non-null     float64       \n",
      " 15  start_datatime                            105 non-null    datetime64[ns]\n",
      " 16  end_datetime                              105 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](3), float64(12), object(2)\n",
      "memory usage: 14.1+ KB\n"
     ]
    }
   ],
   "source": [
    "plus_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c40fa155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2024-03-19 08:00:00\n",
       "1     2024-03-19 10:45:00\n",
       "2     2024-03-19 13:00:00\n",
       "3     2024-03-19 15:15:00\n",
       "4     2024-03-19 18:30:00\n",
       "              ...        \n",
       "100   2024-04-28 08:00:00\n",
       "101   2024-04-28 10:45:00\n",
       "102   2024-04-28 13:00:00\n",
       "103   2024-04-28 15:15:00\n",
       "104   2024-04-28 18:30:00\n",
       "Name: start_datatime, Length: 105, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_data['start_datatime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4c87f106-0c6f-4ef8-a0d2-66b248bc6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 26 columns):\n",
      " #   Column                                        Non-Null Count  Dtype         \n",
      "---  ------                                        --------------  -----         \n",
      " 0   Equipment_same_num                            40506 non-null  int64         \n",
      " 1   PalletID_Collect_Result_encoded               40506 non-null  float64       \n",
      " 2   Production_Qty_Collect_Result_encoded         40506 non-null  float64       \n",
      " 3   WorkMode Collect Result                       40506 non-null  float64       \n",
      " 4   Receip_n_suffix_3                             40506 non-null  int64         \n",
      " 5   time_gap_All                                  40506 non-null  int64         \n",
      " 6   target                                        40506 non-null  object        \n",
      " 7   model_suffix_encoded                          40506 non-null  float64       \n",
      " 8   cleaned_workorder_encoded                     40506 non-null  float64       \n",
      " 9   Collect Date_Dam                              40506 non-null  datetime64[ns]\n",
      " 10  CURE SPEED Collect Result_Dam                 40506 non-null  int64         \n",
      " 11  DISCHARGED SPEED OF RESIN Collect Result_Dam  40506 non-null  int64         \n",
      " 12  Head Clean Position Z Collect Result_Dam      40506 non-null  float64       \n",
      " 13  Head Purge Position Z Collect Result_Dam      40506 non-null  float64       \n",
      " 14  Head Zero Position Y Collect Result_Dam       40506 non-null  float64       \n",
      " 15  Stage2_Circle_Distance_Speed_Dam              40506 non-null  int64         \n",
      " 16  CURE_DISTANCE_Dam                             40506 non-null  float64       \n",
      " 17  HEAD NORMAL DISTANCE_STAGE1_STAGE3_Dam        40506 non-null  float64       \n",
      " 18  HEAD NORMAL DISTANCE_TRIANGLE_height_Dam      40506 non-null  float64       \n",
      " 19  volume_time_multip_stage1_Dam                 40506 non-null  float64       \n",
      " 20  volume_time_multip_stage2_Dam                 40506 non-null  float64       \n",
      " 21  average_thickness_Dam                         40506 non-null  float64       \n",
      " 22  time_ratio_Dam                                40506 non-null  float64       \n",
      " 23  time_gap_Dam                                  40506 non-null  int64         \n",
      " 24  stage1_line13_distance_speed_Dam_encoded      40506 non-null  float64       \n",
      " 25  stage2_line24_distance_speed_Dam_encoded      40506 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(17), int64(7), object(1)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_dam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "04a3eb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2024-04-25 11:10:00\n",
       "1       2023-09-19 14:30:00\n",
       "2       2024-03-05 09:30:00\n",
       "3       2023-09-25 15:40:00\n",
       "4       2023-06-27 13:20:00\n",
       "                ...        \n",
       "40501   2023-09-14 15:50:00\n",
       "40502   2024-04-10 14:20:00\n",
       "40503   2024-02-22 08:40:00\n",
       "40504   2023-07-25 11:00:00\n",
       "40505   2023-05-31 01:40:00\n",
       "Name: Collect Date_Dam, Length: 40506, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_dam['Collect Date_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95b553a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105 entries, 0 to 104\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                    Non-Null Count  Dtype         \n",
      "---  ------                                    --------------  -----         \n",
      " 0   day                                       105 non-null    datetime64[ns]\n",
      " 1   start_time                                105 non-null    object        \n",
      " 2   end_time                                  105 non-null    object        \n",
      " 3   Dam Thickness - 1time / day               81 non-null     float64       \n",
      " 4   Fill thickness - 1time / day              81 non-null     float64       \n",
      " 5   Fill cure energy (mJ) - Every break time  81 non-null     float64       \n",
      " 6   Fill cure power (mW) - Every break time   81 non-null     float64       \n",
      " 7   Line Temp. - Every break time             81 non-null     float64       \n",
      " 8   Line humidity - Every break time          81 non-null     float64       \n",
      " 9   OCR gap - Line#1 stage1                   81 non-null     float64       \n",
      " 10  OCR gap - Line#1 stage2                   81 non-null     float64       \n",
      " 11  OCR gap - Line#1 stage3                   81 non-null     float64       \n",
      " 12  OCR gap - Line #2 stage1                  81 non-null     float64       \n",
      " 13  OCR gap - Line #2 stage2                  81 non-null     float64       \n",
      " 14  OCR gap - Line #2 stage3                  81 non-null     float64       \n",
      " 15  start_datatime                            105 non-null    datetime64[ns]\n",
      " 16  end_datetime                              105 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](3), float64(12), object(2)\n",
      "memory usage: 14.1+ KB\n"
     ]
    }
   ],
   "source": [
    "plus_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "60de21b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'start_datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'start_datetime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22572\\693040500.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 'start_datetime'과 'end_datetime' 열을 datetime 형식으로 변환 (이미 datetime 형식일 경우에도 안전하게 변환)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_datetime'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'coerce'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end_datetime'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'coerce'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'start_datetime'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'start_datetime'과 'end_datetime' 열을 datetime 형식으로 변환 (이미 datetime 형식일 경우에도 안전하게 변환)\n",
    "plus_data['start_datetime'] = pd.to_datetime(plus_data['start_datetime'], errors='coerce')\n",
    "plus_data['end_datetime'] = pd.to_datetime(plus_data['end_datetime'], errors='coerce')\n",
    "\n",
    "# 'Collect Date_Dam' 열을 datetime 형식으로 변환\n",
    "train_data_dam['Collect Date_Dam'] = pd.to_datetime(train_data_dam['Collect Date_Dam'], errors='coerce')\n",
    "\n",
    "# train_data_dam의 각 행에 대해 plus_data의 start_datetime과 end_datetime 사이에 해당하는 값을 확인\n",
    "def check_and_concat(row):\n",
    "    mask = (plus_data['start_datetime'] <= row['Collect Date_Dam']) & (plus_data['end_datetime'] >= row['Collect Date_Dam'])\n",
    "    matching_rows = plus_data[mask]\n",
    "    if not matching_rows.empty:\n",
    "        return pd.concat([row.to_frame().T] * len(matching_rows), axis=1).join(matching_rows.reset_index(drop=True))\n",
    "    else:\n",
    "        return row.to_frame().T\n",
    "\n",
    "# 각 행에 대해 check_and_concat 함수 적용\n",
    "result = train_data_dam.apply(check_and_concat, axis=1)\n",
    "\n",
    "# 결과를 하나의 데이터프레임으로 결합\n",
    "final_result = pd.concat(result.tolist(), ignore_index=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(final_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ba56836-3c51-4db5-88a6-f40690691927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataFrame을 CSV 파일로 저장\n",
    "# train_data.to_csv('./final_data/final_train_data_ver2.csv', index=False)\n",
    "# test_data.to_csv('./final_data/final_test_data_ver2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d61f88",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7109eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992abeb2",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "263a0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "THRESHOLD = 0.3\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(\"./final_data/final_train_data_ver2.csv\")\n",
    "test_data = pd.read_csv(\"./final_data/final_test_data_ver2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "09519b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dam, fill1, fill2 공통 변수\n",
    "var_dam_fill = [\n",
    "    'Equipment_same_num',\n",
    "    'PalletID_Collect_Result_encoded',\n",
    "    'Production_Qty_Collect_Result_encoded',\n",
    "    'WorkMode Collect Result',\n",
    "    'Receip_n_suffix_3',\n",
    "    'time_gap_All'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1921b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 공통 변수\n",
    "### correlation 확인을 위한 변수 리스트\n",
    "var_all_corr = [\n",
    "    'model_suffix_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]\n",
    "\n",
    "### train\n",
    "var_all_train = [\n",
    "    'target',\n",
    "    'model_suffix_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]\n",
    "\n",
    "### test\n",
    "var_all_test = [\n",
    "    'Set ID',\n",
    "    'target',\n",
    "    'model_suffix_encoded',\n",
    "    'cleaned_workorder_encoded'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "65fce5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Dam'을 포함하는 변수 선택\n",
    "dam_variables = [var for var in train_data.columns if '_Dam' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + dam_variables\n",
    "train_data_dam = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + dam_variables\n",
    "test_data_dam = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "39761840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill1'을 포함하는 변수 선택\n",
    "fill1_variables = [var for var in train_data.columns if '_Fill1' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill1_variables\n",
    "train_data_fill1 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill1_variables\n",
    "test_data_fill1 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67f200d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_Fill2'을 포함하는 변수 선택\n",
    "fill2_variables = [var for var in train_data.columns if '_Fill2' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_dam_fill + var_all_train + fill2_variables\n",
    "train_data_fill2 = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_dam_fill + var_all_test + fill2_variables\n",
    "test_data_fill2 = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a737fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_AutoClave'을 포함하는 변수 선택\n",
    "autoclave_variables = [var for var in train_data.columns if '_AutoClave' in var]\n",
    "\n",
    "# train\n",
    "final_columns_train = var_all_train + autoclave_variables\n",
    "train_data_autoclave = train_data[final_columns_train]\n",
    "\n",
    "# test \n",
    "final_columns_test = var_all_test + autoclave_variables\n",
    "test_data_autoclave = test_data[final_columns_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db275bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----train data-----\n",
      "train_data DataFrame의 칼럼 수: 44\n",
      "train_data_dam DataFrame의 칼럼 수: 25\n",
      "train_data_autoclave DataFrame의 칼럼 수: 8\n",
      "train_data_fill1 DataFrame의 칼럼 수: 16\n",
      "train_data_fill2 DataFrame의 칼럼 수: 16\n",
      "----test data-----\n",
      "test_data DataFrame의 칼럼 수: 45\n",
      "test_data_dam DataFrame의 칼럼 수: 26\n",
      "test_data_autoclave DataFrame의 칼럼 수: 9\n",
      "test_data_fill1 DataFrame의 칼럼 수: 17\n",
      "test_data_fill2 DataFrame의 칼럼 수: 17\n"
     ]
    }
   ],
   "source": [
    "# 각 DataFrame의 칼럼 수 계산\n",
    "num_columns_train_data = train_data.shape[1]\n",
    "num_columns_train_data_dam = train_data_dam.shape[1]\n",
    "num_columns_train_data_autoclave = train_data_autoclave.shape[1]\n",
    "num_columns_train_data_fill1 = train_data_fill1.shape[1]\n",
    "num_columns_train_data_fill2 = train_data_fill2.shape[1]\n",
    "\n",
    "num_columns_test_data = test_data.shape[1]\n",
    "num_columns_test_data_dam = test_data_dam.shape[1]\n",
    "num_columns_test_data_autoclave = test_data_autoclave.shape[1]\n",
    "num_columns_test_data_fill1 = test_data_fill1.shape[1]\n",
    "num_columns_test_data_fill2 = test_data_fill2.shape[1]\n",
    "\n",
    "# 각 DataFrame의 칼럼 수 출력\n",
    "print(\"----train data-----\")\n",
    "print(f\"train_data DataFrame의 칼럼 수: {num_columns_train_data}\")\n",
    "print(f\"train_data_dam DataFrame의 칼럼 수: {num_columns_train_data_dam}\")\n",
    "print(f\"train_data_autoclave DataFrame의 칼럼 수: {num_columns_train_data_autoclave}\")\n",
    "print(f\"train_data_fill1 DataFrame의 칼럼 수: {num_columns_train_data_fill1}\")\n",
    "print(f\"train_data_fill2 DataFrame의 칼럼 수: {num_columns_train_data_fill2}\")\n",
    "print(\"----test data-----\")\n",
    "print(f\"test_data DataFrame의 칼럼 수: {num_columns_test_data}\")\n",
    "print(f\"test_data_dam DataFrame의 칼럼 수: {num_columns_test_data_dam}\")\n",
    "print(f\"test_data_autoclave DataFrame의 칼럼 수: {num_columns_test_data_autoclave}\")\n",
    "print(f\"test_data_fill1 DataFrame의 칼럼 수: {num_columns_test_data_fill1}\")\n",
    "print(f\"test_data_fill2 DataFrame의 칼럼 수: {num_columns_test_data_fill2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ed1ec392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40506 entries, 0 to 40505\n",
      "Data columns (total 8 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   target                                  40506 non-null  object \n",
      " 1   model_suffix_encoded                    40506 non-null  float64\n",
      " 2   cleaned_workorder_encoded               40506 non-null  float64\n",
      " 3   Chamber Temp. Collect Result_AutoClave  40506 non-null  int64  \n",
      " 4   1st_pressure_time_AutoClave             40506 non-null  float64\n",
      " 5   2nd_pressure_time_AutoClave             40506 non-null  float64\n",
      " 6   3rd_pressure_time_AutoClave             40506 non-null  float64\n",
      " 7   time_ratio_AutoClave                    40506 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data_autoclave.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5948dcc9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7bb821",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb07f26",
   "metadata": {},
   "source": [
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "id": "f9553c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 스레드홀드 설정\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "# 모델 설정 및 하이퍼파라미터\n",
    "models = {\n",
    "    'et': ExtraTreesClassifier(),\n",
    "    'rf': RandomForestClassifier(),\n",
    "    'cat': CatBoostClassifier(),\n",
    "    'lgbm': LGBMClassifier(),\n",
    "    'xgb': XGBClassifier(),\n",
    "    'dt': DecisionTreeClassifier(),\n",
    "    'ada': AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "def train_and_evaluate_model(model_name, data, **params):\n",
    "    if model_name not in models:\n",
    "        print(f\"{model_name}은(는) 지원되지 않는 모델입니다.\")\n",
    "        return\n",
    "    \n",
    "    # 데이터셋 분할\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        data.drop(\"target\", axis=1),\n",
    "        data[\"target\"].map({'Normal': 0, 'AbNormal': 1}),\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # 모델 선택\n",
    "    model = models[model_name].__class__()  # 새로운 모델 인스턴스 생성\n",
    "\n",
    "    # 하이퍼파라미터 설정\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # 데이터 이름을 자동으로 추출하기 위한 래퍼 함수\n",
    "    data_name = [name for name in globals() if globals()[name] is data][0]\n",
    "\n",
    "    # 예측\n",
    "    y_val_pred_proba = model.predict_proba(x_val)[:, 1]  # 양성 클래스 확률\n",
    "    y_val_pred = (y_val_pred_proba >= THRESHOLD).astype(int)  # 스레드홀드에 따른 예측\n",
    "\n",
    "    # 평가지표 계산\n",
    "    f1 = f1_score(y_val, y_val_pred, average=\"binary\")\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f'{model_name} 모델이 {data_name} 데이터로 학습한 결과:')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('---')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('---')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print('\\n')\n",
    "\n",
    "    return model  # 학습된 모델 반환\n",
    "\n",
    "def fit_all_train_data_function(model_name, data, **params):\n",
    "    if model_name not in models:\n",
    "        print(f\"{model_name}은(는) 지원되지 않는 모델입니다.\")\n",
    "        return None  # 지원되지 않는 모델일 경우 None 반환\n",
    "    \n",
    "    # 모델 선택\n",
    "    model = models[model_name].__class__()  # 새로운 모델 인스턴스 생성\n",
    "\n",
    "    # 하이퍼파라미터 설정\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(data.drop(\"target\", axis=1), data[\"target\"].map({'Normal': 0, 'AbNormal': 1}))\n",
    "\n",
    "    # 데이터 이름을 자동으로 추출하기 위한 래퍼 함수\n",
    "    data_name = [name for name in globals() if globals()[name] is data][0]\n",
    "\n",
    "    print(f'{model_name} 모델이 {data_name} 데이터로 학습 완료')\n",
    "    return model  # 학습된 모델 반환\n",
    "\n",
    "def voting_function(data, estimators, voting='hard', threshold=0.5):\n",
    "    # 데이터셋 분할 # voting='hard'일 경우 threshold는 사용되지 않음\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        data.drop(\"target\", axis=1),\n",
    "        data[\"target\"].map({'Normal': 0, 'AbNormal': 1}),\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # VotingClassifier 설정\n",
    "    voting_clf = VotingClassifier(estimators=estimators, voting=voting)\n",
    "\n",
    "    # 모델 학습\n",
    "    voting_clf.fit(x_train, y_train)\n",
    "\n",
    "    if voting == 'soft':\n",
    "        # 소프트 보팅의 경우 확률 예측\n",
    "        y_val_pred_proba = voting_clf.predict_proba(x_val)[:, 1]\n",
    "        y_val_pred = (y_val_pred_proba >= threshold).astype(int)\n",
    "    else:\n",
    "        # 하드 보팅의 경우 직접 예측\n",
    "        y_val_pred = voting_clf.predict(x_val)\n",
    "\n",
    "    # 평가지표 계산\n",
    "    f1 = f1_score(y_val, y_val_pred, average=\"binary\")\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f'Voting Classifier로 학습한 결과:')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('---')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('---')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print('\\n')\n",
    "\n",
    "    return voting_clf  # 학습된 VotingClassifier 반환\n",
    "\n",
    "def voting(preds_or_probs, method='soft', threshold=0.3):\n",
    "    \"\"\"\n",
    "    하드 보팅 또는 소프트 보팅을 사용하여 최종 예측을 수행합니다.\n",
    "\n",
    "    Parameters:\n",
    "    preds_or_probs (list of np.array): 각 모델의 예측 배열 리스트 (하드 보팅) 또는 예측 확률 배열 리스트 (소프트 보팅)\n",
    "    method (str): 'soft' 또는 'hard' 보팅 방법 선택\n",
    "    threshold (float): 소프트 보팅 시 예측을 양성으로 간주할 확률 임계값\n",
    "\n",
    "    Returns:\n",
    "    np.array: 최종 예측 결과\n",
    "    \"\"\"\n",
    "    if method == 'soft':\n",
    "        # 소프트 보팅: 각 모델의 확률 평균 계산\n",
    "        soft_voting_probs = np.mean(preds_or_probs, axis=0)\n",
    "        # 최종 예측: 평균 확률에 대해 스레드 홀드 적용\n",
    "        final_predictions = (soft_voting_probs >= threshold).astype(int)\n",
    "    elif method == 'hard':\n",
    "        # 하드 보팅: 각 모델의 예측을 모아서 다수결 원칙 적용\n",
    "        preds = np.array(preds_or_probs)\n",
    "        final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=preds)\n",
    "    else:\n",
    "        raise ValueError(\"method 인자는 'soft' 또는 'hard'여야 합니다.\")\n",
    "    \n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a6b1e",
   "metadata": {},
   "source": [
    "### 공정별 모델 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472385f",
   "metadata": {},
   "source": [
    "### lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "95fd9240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm 모델이 train_data_dam 데이터로 학습 완료\n",
      "lgbm 모델이 train_data_autoclave 데이터로 학습 완료\n",
      "lgbm 모델이 train_data_fill1 데이터로 학습 완료\n",
      "lgbm 모델이 train_data_fill2 데이터로 학습 완료\n",
      "lgbm 모델이 train_data 데이터로 학습 완료\n"
     ]
    }
   ],
   "source": [
    "model_Dam = fit_all_train_data_function(\n",
    "    'lgbm', train_data_dam\n",
    "    , n_estimators=2470\n",
    "    , num_leaves=2454\n",
    "    , max_depth=26\n",
    "    , learning_rate=0.06067228197373452\n",
    "    , min_child_samples=134\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'lgbm', train_data_autoclave\n",
    "    , n_estimators=731\n",
    "    , num_leaves=996\n",
    "    , max_depth=273\n",
    "    , learning_rate=0.0912254393922836\n",
    "    , min_child_samples=195\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'lgbm', train_data_fill1\n",
    "    , n_estimators=821\n",
    "    , num_leaves=1400\n",
    "    , max_depth=52\n",
    "    , learning_rate=0.002743887584386348\n",
    "    , min_child_samples=231\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'lgbm', train_data_fill2\n",
    "    , n_estimators=1005\n",
    "    , num_leaves=2304\n",
    "    , max_depth=293\n",
    "    , learning_rate=0.08460539739469425\n",
    "    , min_child_samples=272\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'lgbm', train_data\n",
    "    , n_estimators=1496\n",
    "    , num_leaves=1611\n",
    "    , max_depth=148\n",
    "    , learning_rate=0.0822880159816304\n",
    "    , min_child_samples=194\n",
    "    , boosting_type='dart'\n",
    "    , random_state=RANDOM_STATE\n",
    "    , verbose=-1\n",
    ")\n",
    "\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "lgbm_probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(lgbm_probs, method='soft', threshold=0.26)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data09xx_lgbm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce343b68",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dam = fit_all_train_data_function(\n",
    "    'xgb', train_data_dam\n",
    "    , n_estimators = 1244\n",
    "    , learning_rate = 0.1258535425769987\n",
    "    , max_depth = 26\n",
    "    , alpha = 2.1820842842359597e-06\n",
    "    , gamma = 0.00010809657684921935\n",
    "    , reg_alpha = 0.5844029076359536\n",
    "    , reg_lambda = 0.4748752246073433\n",
    "    , colsample_bytree = 0.9607659760060685\n",
    "    , subsample = 0.7147741317935203\n",
    "    , objective = 'binary:logistic'\n",
    "    , tree_method = 'exact'\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'xgb', train_data_autoclave,\n",
    "    n_estimators = 1152, \n",
    "    learning_rate = 0.02466611382982541, \n",
    "    max_depth = 29, \n",
    "    alpha = 2.9180083404308157e-05, \n",
    "    gamma = 0.00012667501319666823, \n",
    "    reg_alpha = 0.6903592486292155, \n",
    "    reg_lambda = 0.5638873235014423, \n",
    "    colsample_bytree = 0.9432782030604233, \n",
    "    subsample = 0.19192246128663584,\n",
    "    objective = 'binary:logistic',  # 이진 분류\n",
    "    tree_method = \"exact\", \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'xgb', train_data_fill1,\n",
    "    n_estimators = 1899, \n",
    "    learning_rate = 0.011878583548993711, \n",
    "    max_depth = 12, \n",
    "    alpha = 0.004515243354832891,\n",
    "    gamma = 0.0015693650802180896,\n",
    "    reg_alpha = 0.7484424912256998, \n",
    "    reg_lambda = 0.27164326303977143, \n",
    "    colsample_bytree = 0.7901385059430825,\n",
    "    subsample = 0.9924662032617025,\n",
    "    objective = 'binary:logistic',\n",
    "    tree_method = 'exact',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'xgb', train_data_fill2,\n",
    "    n_estimators = 1162, \n",
    "    learning_rate = 0.014523070494025153, \n",
    "    max_depth = 8, \n",
    "    alpha = 0.00012198482017902725, \n",
    "    gamma = 0.001236902841680112, \n",
    "    reg_alpha = 0.7331637000614692, \n",
    "    reg_lambda = 0.5237223061096699, \n",
    "    colsample_bytree = 0.8250374170841293, \n",
    "    subsample = 0.31906427054137687,\n",
    "    objective = 'binary:logistic',\n",
    "    tree_method = 'exact',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'xgb', train_data,\n",
    "    n_estimators = 2427,\n",
    "    learning_rate = 0.010774204513905965, \n",
    "    max_depth = 17, \n",
    "    alpha = 0.0005233654110538582, \n",
    "    gamma = 5.551445919277608e-05, \n",
    "    reg_alpha = 0.9652805882189326, \n",
    "    reg_lambda = 0.3542856398135083, \n",
    "    colsample_bytree = 0.9094884645797131, \n",
    "    subsample = 0.1733751790853043,\n",
    "    objective = 'binary:logistic',  # 이진 분류\n",
    "    tree_method = \"exact\", \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "xgb_probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf40ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(xgb_probs, method='soft', threshold=0.24)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data09xx_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082008d",
   "metadata": {},
   "source": [
    "### cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dam = fit_all_train_data_function(\n",
    "    'cat', train_data_dam,\n",
    "    iterations = 1478, \n",
    "    learning_rate = 0.009068953796649421, \n",
    "    depth = 11, \n",
    "    min_data_in_leaf = 2,\n",
    "    l2_leaf_reg = 1.187291687951122,\n",
    "    random_strength = 0.43102541391012816, \n",
    "    bagging_temperature = 3.1790702578164853, \n",
    "    border_count = 155, \n",
    "    scale_pos_weight = 1.4418307437388553,\n",
    "    grow_policy = 'Depthwise',\n",
    "\n",
    "    random_state = RANDOM_STATE,\n",
    "    eval_metric = 'F1',\n",
    "    logging_level = 'Silent',\n",
    "    boosting_type = 'Plain'\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'cat', train_data_autoclave,\n",
    "    iterations = 1299, \n",
    "    learning_rate =  0.03808793470493637, \n",
    "    depth = 9, \n",
    "    min_data_in_leaf = 5,\n",
    "    l2_leaf_reg = 4.942829707223811, \n",
    "    random_strength = 3.804933757402697, \n",
    "    bagging_temperature = 1.3151583440997139, \n",
    "    border_count = 286, \n",
    "    scale_pos_weight = 1.9749286362629779,\n",
    "    grow_policy = 'SymmetricTree',\n",
    "\n",
    "    random_state = RANDOM_STATE,\n",
    "    eval_metric = 'F1',\n",
    "    logging_level = 'Silent',\n",
    "    boosting_type = 'Plain'\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'cat', train_data_fill1,\n",
    "    iterations = 2842, \n",
    "    learning_rate = 0.01099464761153367, \n",
    "    depth = 4, \n",
    "    min_data_in_leaf = 3,\n",
    "    l2_leaf_reg = 3.7373183252945945, \n",
    "    random_strength = 9.3675281753561, \n",
    "    bagging_temperature = 4.750112155842117, \n",
    "    border_count = 160, \n",
    "    scale_pos_weight = 2.53860325765727,\n",
    "    grow_policy = 'Lossguide',\n",
    "\n",
    "    random_state = RANDOM_STATE,\n",
    "    eval_metric = 'F1',\n",
    "    logging_level = 'Silent',\n",
    "    boosting_type = 'Plain'\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'cat', train_data_fill2,\n",
    "    iterations = 1458, \n",
    "    learning_rate = 0.004706507801075929, \n",
    "    depth = 13, \n",
    "    min_data_in_leaf = 4,\n",
    "    l2_leaf_reg = 1.909987690181427, \n",
    "    random_strength = 9.047942432889677, \n",
    "    bagging_temperature = 3.545210494821586, \n",
    "    border_count = 300, \n",
    "    scale_pos_weight = 3.4781865667208467,\n",
    "    grow_policy = 'Lossguide',\n",
    "\n",
    "\n",
    "    random_state = RANDOM_STATE,\n",
    "    eval_metric = 'F1',\n",
    "    logging_level = 'Silent',\n",
    "    boosting_type = 'Plain'\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'cat', train_data,\n",
    "    iterations=1349,\n",
    "    learning_rate=0.012526639112437014,\n",
    "    depth=9,\n",
    "    min_data_in_leaf=4,\n",
    "    l2_leaf_reg=2.245006704049574,\n",
    "    random_strength=0.6922797458293842,\n",
    "    bagging_temperature=8.230635636022027,\n",
    "    border_count=211,\n",
    "    scale_pos_weight=2.0709015241138236,\n",
    "    grow_policy='Depthwise',\n",
    "    \n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='F1',\n",
    "    logging_level='Silent',\n",
    "    boosting_type='Plain'\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "cat_probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(cat_probs, method='soft', threshold=0.3)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data09xx_cat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f9017",
   "metadata": {},
   "source": [
    "### et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5dc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dam = fit_all_train_data_function(\n",
    "    'et', train_data_dam\n",
    "    , n_estimators = 2242\n",
    "    , max_depth = 32\n",
    "    , min_samples_split = 2\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'et', train_data_autoclave,\n",
    "    n_estimators = 2708,\n",
    "    max_depth = 41,\n",
    "    min_samples_split = 8,\n",
    "    min_samples_leaf = 1,\n",
    "    criterion = 'entropy',\n",
    "    bootstrap = False,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'et', train_data_fill1,\n",
    "    n_estimators = 1520,\n",
    "    max_depth = 30,\n",
    "    min_samples_split = 2,\n",
    "    min_samples_leaf = 1,\n",
    "    criterion = 'entropy',\n",
    "    bootstrap = False,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'et', train_data_fill2\n",
    "    , n_estimators = 1001\n",
    "    , max_depth = 45\n",
    "    , min_samples_split = 3\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'et', train_data\n",
    "    , n_estimators = 2884\n",
    "    , max_depth = 56\n",
    "    , min_samples_split = 3\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "et_probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(et_probs, method='soft', threshold=0.3)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data09xx_et.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9e9d8",
   "metadata": {},
   "source": [
    "### rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cece5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_Dam = fit_all_train_data_function(\n",
    "    'rf', train_data_dam\n",
    "    , n_estimators = 1330\n",
    "    , max_depth = 36\n",
    "    , min_samples_split = 6\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'rf', train_data_autoclave\n",
    "    , n_estimators = 1103\n",
    "    , max_depth = 36\n",
    "    , min_samples_split = 8\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , bootstrap = False\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'rf', train_data_fill1\n",
    "    , n_estimators = 1861\n",
    "    , max_depth = 91\n",
    "    , min_samples_split = 7\n",
    "    , min_samples_leaf = 5\n",
    "    , criterion = 'entropy'\n",
    "    , class_weight = 'balanced'\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'rf', train_data_fill2\n",
    "    , n_estimators = 2663\n",
    "    , max_depth = 100\n",
    "    , min_samples_split = 6\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , class_weight = 'balanced'\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'rf', train_data\n",
    "    , n_estimators = 1082\n",
    "    , max_depth = 54\n",
    "    , min_samples_split = 6\n",
    "    , min_samples_leaf = 1\n",
    "    , criterion = 'entropy'\n",
    "    , class_weight = 'balanced'\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "rf_probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de36ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(rf_probs, method='soft', threshold=0.3)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data09xx_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24327ce",
   "metadata": {},
   "source": [
    "### ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25236bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=22,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=0.9449544624225188,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Dam = fit_all_train_data_function(\n",
    "    'ada', train_data_dam\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=439\n",
    "    , learning_rate=0.30985993372769294\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=13,\n",
    "    min_samples_split=34,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=0.7473309094470472,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_AutoClave = fit_all_train_data_function(\n",
    "    'ada', train_data_autoclave\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=570\n",
    "    , learning_rate=0.2040105705276999\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=14,\n",
    "    min_samples_split=33,\n",
    "    min_samples_leaf=8,\n",
    "    max_features=0.7113128413756866,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill1 = fit_all_train_data_function(\n",
    "    'ada', train_data_fill1\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=913\n",
    "    , learning_rate=0.055237331816147595\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=7,\n",
    "    min_samples_split=13,\n",
    "    min_samples_leaf=8,\n",
    "    max_features=0.6266118401157937,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_Fill2 = fit_all_train_data_function(\n",
    "    'ada', train_data_fill2\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=293\n",
    "    , learning_rate=0.620377973483163\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=6,\n",
    "    min_samples_split=28,\n",
    "    min_samples_leaf=7,\n",
    "    max_features=0.7331591188366589,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model_All = fit_all_train_data_function(\n",
    "    'ada', train_data\n",
    "    , estimator=base_estimator\n",
    "    , n_estimators=677\n",
    "    , learning_rate=0.6713565955468803\n",
    "    , random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test_dam = test_data_dam.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_autoclave = test_data_autoclave.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill1 = test_data_fill1.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_fill2 = test_data_fill2.drop([\"target\", \"Set ID\"], axis=1)\n",
    "x_test_all = test_data.drop([\"target\", \"Set ID\"], axis=1)\n",
    "\n",
    "# 예측 확률 리스트 (소프트 보팅용)\n",
    "ada_probs = [\n",
    "    model_Dam.predict_proba(x_test_dam)[:, 1]\n",
    "    , model_AutoClave.predict_proba(x_test_autoclave)[:, 1]\n",
    "    , model_Fill1.predict_proba(x_test_fill1)[:, 1]\n",
    "    , model_Fill2.predict_proba(x_test_fill2)[:, 1]\n",
    "    , model_All.predict_proba(x_test_all)[:, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b04f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트 보팅 결과\n",
    "final_predictions = voting(ada_probs, method='soft', threshold=0.3)\n",
    "print(sum(final_predictions))\n",
    "\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
    "df_sub[\"target\"] = final_predictions\n",
    "\n",
    "# df_sub['target'] 값을 문자열 레이블로 변환\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"./data/data09xx_ada.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f124a4a",
   "metadata": {},
   "source": [
    "## Hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def read_submission_files(file_paths):\n",
    "    \"\"\"\n",
    "    제출 파일을 읽어와서 예측 결과를 반환합니다.\n",
    "\n",
    "    Parameters:\n",
    "    file_paths (list of str): 제출 파일 경로 리스트\n",
    "\n",
    "    Returns:\n",
    "    list of np.array: 각 제출 파일의 예측 결과 리스트\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        preds = df['target'].apply(lambda x: 1 if x == 'AbNormal' else 0).values\n",
    "        predictions.append(preds)\n",
    "    return predictions\n",
    "\n",
    "def hard_voting(preds):\n",
    "    \"\"\"\n",
    "    하드 보팅을 사용하여 최종 예측을 수행합니다.\n",
    "\n",
    "    Parameters:\n",
    "    preds (list of np.array): 각 모델의 예측 배열 리스트\n",
    "\n",
    "    Returns:\n",
    "    np.array: 최종 예측 결과\n",
    "    \"\"\"\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    # 각 샘플의 예측 결과를 문자열로 변환하여 리스트에 저장\n",
    "    sample_predictions = [''.join(map(str, x)) for x in preds.T]\n",
    "    \n",
    "    # 각 예측 결과의 빈도수를 계산\n",
    "    prediction_counts = Counter(sample_predictions)\n",
    "    \n",
    "    # 빈도수 출력\n",
    "    for pred, count in prediction_counts.items():\n",
    "        print(f\"Prediction {pred}: {count} times\")\n",
    "    \n",
    "    # 하드 보팅을 통해 최종 예측을 계산\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=preds)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e29233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 공통 경로\n",
    "common_path = \"./data/\"\n",
    "\n",
    "# 제출 파일 이름 리스트\n",
    "file_names = [\n",
    "    \"data09xx_lgbm.csv\"\n",
    "    , \"data09xx_xgb.csv\"\n",
    "    , \"data09xx_cat.csv\"\n",
    "    , \"data09xx_et.csv\"\n",
    "    , \"data09xx_rf.csv\"\n",
    "    , \"data09xx_ada.csv\"   \n",
    "    # 파일 추가 가능  <----- 파일 필요시 추가하세요!!\n",
    "]\n",
    "\n",
    "# 경로를 추가하는 함수\n",
    "def add_common_path(file_names, common_path):\n",
    "    return [common_path + file_name for file_name in file_names]\n",
    "\n",
    "# 경로가 추가된 파일 리스트\n",
    "file_paths = add_common_path(file_names, common_path)\n",
    "\n",
    "# 제출 파일에서 예측 결과 읽어오기\n",
    "predictions = read_submission_files(file_paths)\n",
    "\n",
    "# 하드 보팅 결과\n",
    "final_predictions_hard = hard_voting(predictions)\n",
    "\n",
    "# 결과를 새로운 제출 파일로 저장할 파일 이름\n",
    "output_file_name = \"change_name_submission.csv\" # <----- 파일 이름을 변경하세요!!\n",
    "\n",
    "# 결과를 새로운 제출 파일로 저장\n",
    "df_sub = pd.read_csv(file_paths[0])\n",
    "df_sub[\"target\"] = final_predictions_hard\n",
    "df_sub['target'] = df_sub['target'].apply(lambda x: 'AbNormal' if x == 1 else 'Normal')\n",
    "df_sub.to_csv(output_file_name, index=False)\n",
    "\n",
    "print(f\"최종 제출 파일이 '{output_file_name}'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f14637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c731c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd2eb7",
   "metadata": {},
   "source": [
    "우측 상단의 제출 버튼을 클릭해 결과를 확인하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d80c3b",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
